\documentclass[10pt]{book}
\pagestyle{plain}
%\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{latexsym,amsmath,amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{lmodern}
\usepackage{pifont}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{thmtools}
\usepackage{wrapfig}
\usepackage{extarrows}
\usepackage{breqn}
\usepackage{physics}
\usepackage{afterpage}
\usepackage[inline]{enumitem}
\usepackage{mathrsfs}
\usepackage{scalerel}
\usepackage{stackengine,wasysym}
\usepackage{aligned-overset}
\usepackage{stackengine}
\usepackage{mathtools}
\usepackage{nccmath}
\usepackage{url}
\usepackage{float}
\usepackage{lipsum}
\usepackage[toc]{appendix}
\usepackage{chngcntr}
\usepackage{etoolbox}
\usepackage{framed}
\usepackage{mdframed}
\usepackage{blindtext}
\usepackage{xcolor}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{esint}
\usepackage{aligned-overset}
\usepackage{mathrsfs}
\graphicspath{{images/}}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}


\titleformat{\chapter}[block]{\huge\bfseries\itshape\raggedleft}{\chaptertitlename\  \thechapter.}{0.5ex}{}[]

\titleformat{\section}[block]{\Large\bfseries\itshape}{\thesection\ }{0.5ex}{}[]

\titleformat{\subsection}[block]{\large\bfseries\itshape}{\thesubsection\ }{0.5ex}{}[]

\usepackage{hyperref}
\hypersetup{
    colorlinks = true,
    linkcolor = blue,
    filecolor = blue,      
    urlcolor = blue,
    citecolor = blue,
    pdftitle = {Sharelatex Example},
    bookmarks = true,
    pdfpagemode = FullScreen,
}

\urlstyle{same}

\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=2pt] (char) {#1};}}

\setlength{\oddsidemargin}{1pt}
\setlength{\evensidemargin}{1pt}
\setlength{\marginparwidth}{30pt} % these gain 53pt width
\setlength{\topmargin}{1pt}       % gains 26pt height
\setlength{\headheight}{1pt}      % gains 11pt height
\setlength{\headsep}{1pt}         % gains 24pt height
%\setlength{\footheight}{12 pt} 	  % cannot be changed as number must fit
\setlength{\footskip}{24pt}       % gains 6pt height
\setlength{\textheight}{650pt}    % 528 + 26 + 11 + 24 + 6 + 55 for luck
\setlength{\textwidth}{460pt}     % 360 + 53 + 47 for luck

\title{Sections and Chapters}

\newtheorem{definition}{Definition}[chapter]
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\newtheorem{exercise}{Exercise}[section]
\newtheorem{remark}{Remark}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\numberwithin{equation}{chapter}


\AtBeginEnvironment{subappendices}{%
\Appendix*{Appendix}
\addcontentsline{toc}{chapter}{Appendices}
\counterwithin{figure}{section}
\counterwithin{table}{section}
}

\newmdenv[
  linewidth=1.5pt, 
  topline=false, 
  bottomline=false, 
  rightline=false,
  innerleftmargin=15pt,
  leftmargin=10pt,
  rightmargin=0pt,
  innerrightmargin=0pt, 
]{claim}

\def\MM{\mathfrak{M}}
\def\BB{\mathfrak{B}}
\def\CC{\mathfrak{C}}
\def\leb{{\mathcal L}}
\def\H{{\mathcal H}}
\def\L{{\mathcal L}}
\def\diam{{\operatorname{diam}\,}}
\def\co{{\overline{\operatorname{co}}\,}}
\def\dist{{\operatorname{dist}\,}}

\usepackage{scalerel}
%\usepackage[usestackEOL]{stackengine}
\def\avint{\,\ThisStyle{\ensurestackMath{%
  \stackinset{c}{.2\LMpt}{c}{1\LMpt}{\SavedStyle-}{\SavedStyle\phantom{\int}}}%
  \setbox0=\hbox{$\SavedStyle\int\,$}\kern-\wd0}\int}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE]{{\em \thepage}}
\fancyhead[RE]{{\em \leftmark}}
\fancyhead[LO]{{\em \rightmark}}
\fancyhead[RO]{{\em \thepage}}


\counterwithout{footnote}{chapter}

\def\dsp{\def\baselinestretch{1.35}\large
\normalsize}
%%%%This makes a double spacing. Use this with 11pt style. If you
%%%%want to use this just insert \dsp after the \begin{document}
%%%%The correct baselinestretch for double spacing is 1.37. However
%%%%you can use different parameter.

\newcommand\blankpage{%
    \null
    \thispagestyle{empty}%
    \addtocounter{page}{-1}%
    \newpage}
    
\def\U{{\mathcal U}}

\begin{document}
\frontmatter

\begin{titlepage}
	\begin{center}
	\textbf{\LARGE{}} \\
	\vspace{40mm}
    \textbf{\Huge{Mathematical Analysis}} \\
    \medskip
    \vspace{10mm} %5mm vertical space
    \large{\textsc{Zhen Yao}}\\
    %\large{\textsc{University of Pittsburgh}}
    \end{center}
\end{titlepage}

\tableofcontents{}
\mainmatter

\newpage

\chapter*{Preface}
\addcontentsline{toc}{chapter}{Preface}

The main purpose of this book is to help prepare for the preliminary exam in Mathematical Analysis as a new mathematics PhD student. This book will follow the structure in Rudin's {\it Principles of Mathematical Analysis}\cite{1} and more contents will be added in each chapters, some are from Dr. Piotr Hajłasz's {\it Introduction to Analysis}\cite{2}.

The content of real and complex number system, especially how to constructing the real number system, will not be covered in this book, since it is perfectly discussed in both Rudin's and Dr. Hajłasz's books. Thus, this book starts with the basic topology. 

\null\hfill ZHEN YAO

\newpage

\chapter{Basic Topology}

\section{Functions}

\begin{definition}
Let $X$ and $Y$ be two sets, and suppose with each element $x$ of $X$ there is associated an element of $Y$, denoted by $f(x)$. Then $f$ is said to to a function from $X$ to $Y$ (or a mapping of $X$ into $Y$), and we write it as $f: X \to Y$. Also,
\begin{enumerate}[label=(\alph*)]
    \item the set $X$ is called the domain of $f$, and $f$ is said to be defined on $X$;
    
    \item the set $Y$ is called the target of $f$;
    
    \item the set of all values of $f(X) \coloneqq \{y \in Y \,:\, \exists x \in X, y = f(x)\}$ is called the range of $f$. If $E \subset X$, we call $f(E)$ the image of $E$ under $f$, and clearly $f(E) \subset Y$;
    
    \item the graph of $f$ is a subset of $X \times Y$, defined by
    \begin{align*}
        \operatorname{graph}(f) \coloneqq \left\{(x,f(x)) \in X \times Y \,:\, x \in X \right\}.
    \end{align*}
\end{enumerate}
\end{definition}

\medskip

\begin{definition}
Let $f: X \to Y$. If $f(X) = Y$, we say $f$ is onto or surjective if $f(X) = Y$. Clearly, the term of onto is more specific than into.

If $E \subset Y$, $f^{-1}(E)$ denotes the set of all $x \in X$ such that $f(x) \in E$. We call $f^{-1}(E)$ the inverse image of $E$ under $f$. If for each $y \in Y$, $f^{-1}(y)$ consists of at most one element of $X$, then $f$ is said to be one-to-one or injective, i.e. if for any $x_1 \neq x_2, x_1, x_2 \in X$, then $f(x_1) \neq f(x_2)$.
\end{definition}

\medskip

\begin{definition}
A function $f:X \to Y$ that is one-to-one and onto is called a bijection. In this case, the inverse function is defined on $Y = f(X)$ and the inverse function $f^{-1}: Y \to X$ is also a bijection.
\end{definition}

\begin{definition}
If $f: X \to Y$ and $g: Y \to Z$ are two functions, then the composition of $f$ and $g$ is the function $g \circ f: X \to Z$ defined by $(g \circ f)(x) = g(f(x))$ for $x \in X$.
\end{definition}

\medskip

\begin{definition}
If $f: X \to Y$ and $A \subset X$, then the restriction of $f$ to $A$ is $f|_A: A \to Y$ defined by $f|_A(x) = f(x)$ for all $x \in A$. In this case, we say that $f$ is an extension of $f|_A$.
\end{definition}

\medskip

\begin{proposition}\label{prop_11}
Let $f: X \to Y$ be a function and let $A, B \subset Y$. Then,
\begin{align*}
    f^{-1}(A \cup B) & = f^{-1}(A) \cup f^{-1}(B), \\
    f^{-1}(A \cap B) & = f^{-1}(A) \cap f^{-1}(B), \\
    f^{-1}(Y \setminus A) & = X \setminus f^{-1}(A).
\end{align*}
\end{proposition}

\begin{remark}
Given a set $E \subset X$, $E^c = X \setminus E$ denotes the complement of the set $E$ in $X$. With this notation, the last equality can be expressed as
\begin{align*}
    f^{-1}(A^c) = \left(f^{-1}(A)\right)^c.
\end{align*}
\end{remark}
\begin{proof}
First, if $x \in f^{-1}(A \cup B)$, then $f(x) \in A \cup B$, and hence $f(x) \in A$ or $f(x) \in B$. If $f(x) \in A$, then clearly $x \in f^{-1}(A) \subset f^{-1}(A) \cup f^{-1}(B)$. If $f(x) \in B$, then clearly $x \in f^{-1}(B) \subset f^{-1}(A) \cup f^{-1}(B)$. And hence $f^{-1}(A \cup B) \subset f^{-1}(A) \cup f^{-1}(B)$. On the other hand, if $x \in f^{-1}(A) \cup f^{-1}(B)$, then $x \in f^{-1}(A)$ or $x \in f^{-1}(B)$. If $x \in f^{-1}(A)$, then $f(x) \in A \subset A \cup B$. If $x \in f^{-1}(B)$, then $f(x) \in B \subset A \cup B$. In either case $f(x) \in A \cup B$, and hence $x \in f^{-1}(A \cup B)$. This completes the proof of the first equality.

Second, if $x \in f^{-1}(A \cap B)$, then $f(x) \in A \cap B$, hence $f(x) \in A$ and $f(X) \in B$. In this case, $x \in f^{-1}(A) \cap f^{-1}(B)$. On the other hand, if $x \in f^{-1}(A) \cap f^{-1}(B)$, then $f(x) \in A$ and $f(x) \in B$. Hence, $f(x) \in A \cap B$ and this implies $x \in f^{-1}(A \cap B)$. This completes the proof of the second equality.

Finally, if $x \in f^{-1}(Y \setminus A)$, then $f(x) \in Y \setminus A$, and hence $x \notin f^{-1}(A)$. This implies $x \in X \setminus f^{-1}(A)$. On the other hand, if $x \in X \setminus f^{-1}(A)$, then $f(x) \notin A$, and hence $f(x) \in Y \setminus Y$, and this implies $x \in f^{-1}(Y \setminus A)$. 
\end{proof}

\medskip

\begin{proposition}
Let $f: X \to Y$ be a function and let $A, B \subset X$. Then,
\begin{align*}
    f(A \cup B) = f(A) \cup f(B), \qquad f(A \cap B) \subset f(A) \cap f(B).
\end{align*}
If in addition $f$ is one-to-one then
\begin{align*}
    f(A \cap B) = f(A) \cap f(B).
\end{align*}
\end{proposition}
\begin{proof}
First, if $y \in f(A \cup B)$, then $y = f(x)$ for some $x \in A \cup B$. If $x \in A$, then $y = f(x) \in f(A)$. If $x \in B$, then $y = f(x) \in f(B)$. In either case, we have $y \in f(A) \cup f(B)$. On the other hand, if $y \in f(A) \cup f(B)$, then $y \in f(A)$ or $y \in f(B)$. If $y \in f(A)$, then $y = f(x)$ for some $x \in A$. If $y \in f(B)$, then $y = f(x)$ for some $x \in B$. In either case, we have $y = f(x) \in f(A \cup B)$.

Second, if $y \in f(A \cap B)$, then $y = f(x)$ for some $x \in A \cap B$. Then, $y = f(x) \in f(A)$ and $y = f(x) \in f(B)$, hence $y \in f(A) \cap f(B)$. This implies $f(A \cap B) \subset f(A) \cap f(B)$.

Finally, if $f$ is one-to-one, it remains to show that $f(A) \cap f(B) \subset f(A \cap B)$. If $y \in f(A) \cap f(B)$, then there exist $x_1 \in A$ and $x_2 \in B$ such that $y = f(x_1)$ and $y = f(x_2)$. Since $f$ is one-to-one, then $x_1 = x_2$ and clearly $x_1 = x_2 \in A \cap B$, and hence $y = f(x_1) = f(x_2) \in f(A \cap B)$. This completes the proof of $f(A) \cap f(B) \subset f(A \cap B)$.
\end{proof}

\begin{remark}
Why $f(A) \cap f(B) \subset f(A \cap B)$ fail to hold when $f$ is not one-to-one? If $y \in f(A) \cap f(B)$, then $y = f(x_1)$ for some $x_1 \in A$ and $y = f(x_2)$ for some $x_2 \in B$. However, $x_1$ may not be equal to $x_2$ and hence we cannot claim there is a common $x \in A \cap B$ such that $y = f(x)$. And this proposition leads to the following results.
\end{remark}

\medskip

\begin{proposition}
Let $f: X \to Y$ be a function and $A_1, A_2, A_3, \cdots$ are subsets of $X$, then
\begin{align*}
    f \left(\bigcup^\infty_{i=1} A_i\right) = \bigcup^\infty_{i=1} f(A_i), \qquad f \left(\bigcap^\infty_{i=1} A_i\right) \subset \bigcup^\infty_{i=1} f(A_i).
\end{align*}
If in addition $f$ is one-to-one, then
\begin{align*}
    f \left(\bigcap^\infty_{i=1} A_i\right) = \bigcup^\infty_{i=1} f(A_i).
\end{align*}
\end{proposition}


\medskip

\section{Finite, Countable, and Uncountable Sets}

\begin{definition}
If there is a one-to-one mapping of $X$ onto $Y$, we say that $X$ and $Y$ is one-to-one correspondence, or that $X$ and $Y$ have the same cardinal number, or $X$ and $Y$ are equivalent, and we write $X \sim Y$. Clearly, this relation has the following properties:
\begin{enumerate}[label=(\alph*)]
    \item It is reflexive: $X \sim X$;
    
    \item It is symmetric: If $X \sim Y$, then $Y \sim X$;
    
    \item It is transitive: If $X \sim Y$ and $Y \sim Z$, then $X \sim Z$.
\end{enumerate}
Any relation with these three properties is called an equivalence relation.
\end{definition}

\medskip

\begin{remark}
A bijection $f:X \to Y$ is a one-to-one correspondence between $X$ and $Y$.
\end{remark}

\medskip

\begin{definition}
For any positive integer $n \in \mathbb{N}$, where $\mathbb{N}$ is the set of all positive integers, let $J_n = \{1,2,\cdots,n\}$. For any set $A$, we say:
\begin{enumerate}[label=(\alph*)]
    \item $A$ is finite if $A \sim J_n$ for some $n$ (the empty set $\emptyset$ is also considered to be finite).
    
    \item $A$ is infinite if $A$ is not finite.
    
    \item $A$ is countable if $A \sim \mathbb{N}$ (Countable sets are sometimes called enumerable or denumerable).
    
    \item $A$ is at most countable if $A$ is finite or countable.
\end{enumerate}
\end{definition}

\medskip

\begin{proposition}
The sets $\mathbb{N}$ and $2\mathbb{N} \coloneqq \{2n \,:\, x \in \mathbb{N}\}$ have the same cardinality.
\end{proposition}
\begin{proof}
Indeed, let $f(x) = 2n$ for $n \in \mathbb{N}$, then $f: \mathbb{N} \to 2\mathbb{N}$ is clearly a bijection.
\end{proof}

\medskip

\begin{proposition}
The sets $\mathbb{N}$ and $\mathbb{Z}$ have the same cardinality.
\end{proposition}
\begin{proof}
Indeed, for $n \in \mathbb{N}$, let
\begin{align*}
    f(n) = \begin{cases}
        \frac{n}{2}, & n \,\,\text{even}, \\
        - \frac{n-1}{2}, & n \,\,\text{odd}.
    \end{cases}
\end{align*}
Clearly, $f: \mathbb{N} \to \mathbb{Z}$ is clearly a bijection.
\end{proof}

\medskip

\begin{definition}
A sequence is a function $f$ defined on the set $\mathbb{N}$. If $f(n) = x_n$, for $n \in \mathbb{N}$, denote the sequence $f$ by ${x_n}$, or sometimes by $\{x_1, x_2, x_3, \cdots\}$. The values of $f$, that is, $x_n$, are called the terms of the sequence. If $A$ is a set and if $x_n \in A$ for all $n \in \mathbb{N}$, then $\{x_n\}$ is said to be a sequence in $A$, or a sequence of elements of $A$.

Since every countable set is the range of a one-to-one function defined on $\mathbb{N}$, we may regard every countable set as the range of a sequence of distinct terms.
\end{definition}

\medskip

\begin{theorem}\label{th_11}
Every infinite subset of a countable set $A$ is countable.
\end{theorem}
\begin{proof}
Suppose $E \subset A$, and $E$ is infinite. Arrange the elements of $A$ in a sequence $\{x_n\}$. Now, let $n_{1}$ be the smallest integer such that $x_{n_1} \in E$. After choosing $n_1, \cdots, n_{k-1}$, let $n_k$ be the smallest integer greater than $n_{k-1}$ such that $x_{n_k} \in E$. Let $f(k) = x_{n_k}$ for $k \in \mathbb{N}$, we have a one-to-one correspondence between $E$ and $\mathbb{N}$.
\end{proof}

\medskip

\begin{theorem}\label{th_12}
Let $\{E_n\}, n \in \mathbb{N}$ be a sequence of countable sequences and let 
\begin{align}\label{th_11_equ_1}
    S = \bigcup^\infty_{n=1} E_n.
\end{align}
Then $S$ is countable.
\end{theorem}
\begin{proof}
Let every set $E_n$ be arranged in a sequence $\{x_{nk}\}, k \in \mathbb{N}$, and consider the infinite array:
\begin{align*}
    x_{11}, \quad x_{12}, \quad x_{13}, \quad x_{14}, \quad \cdots \\
    x_{21}, \quad x_{22}, \quad x_{23}, \quad x_{24}, \quad \cdots \\
    x_{31}, \quad x_{32}, \quad x_{33}, \quad x_{34}, \quad \cdots
\end{align*}
where the elements of $E_n$ form the $n$th row. These elements can be arranged into a sequence
\begin{align*}
    x_{11}, x_{21}, x_{12}, x_{31}, x_{22}, x_{13}, x_{41}, x_{32}, x_{23}, x_{14}, \cdots
\end{align*}
If any two of the set $E_n$ have elements in common, then there is a subset $T$ of $\mathbb{N}$ such that $S \sim T$, hence by Theorem \ref{th_11}, $S$ is at most countable. Also, $E_1 \subset S$ is infinite, then $S$ is also infinite, and thus countable.
\end{proof}

\medskip

\begin{corollary}
Suppose $A$ is at most countable, and for every $\alpha \in A$, $E_{\alpha}$ is at most countable. Then,
\begin{align*}
    T = \bigcup_{\alpha \in A} E_{\alpha}
\end{align*}
is at most countable.
\end{corollary}
\begin{proof}
For $T$ is equivalent to a subset of \eqref{th_11_equ_1}.
\end{proof}

\medskip

\begin{theorem}\label{th_13}
Let $A$ be a countable set, and let $E_n$ be the set of all $n$-tuples $(a_1, \cdots, a_n)$, where $a_k \in A, k = 1, \cdots, n$, and the elements $a_1, \cdots, a_n$ need not be distinct. Then $E_n$ is countable.
\end{theorem}
\begin{proof}
$E_1$ is countable since $E_1 = A$. Suppose $E_{n-1}$ is countable, Then the elements of $E_n$ has form $(b,a)$, where $b \in E_{n-1}, a \in A$. For every fixed $b$, the set of pairs $(b,a)$ is equivalent to $A$, hence countable. Thus, $E_n$ is a countable union of countable sets, hence countable by Theorem \ref{th_12}.
\end{proof}

\medskip

\begin{theorem}
The set of rational numbers $\mathbb{Q}$ and $\mathbb{N}$ have the same cardinality.
\end{theorem}
\begin{proof}
Each rational number $q \in \mathbb{Q}$ can be expressed as a quotient $n/m$ where $n \in \mathbb{Z}$ and $m \in \mathbb{N}$ and the greatest common divisor of $\left|n\right|$ and $m$ is $1$. Let $f: \mathbb{Q} \to \mathbb{Z}^2$ be defined by $f(q) = (n,m)$. Then the set of all pairs $(n,m)$ is countable by Theorem \ref{th_13}.
\end{proof}

\medskip

Not all infinite sets are countable. The example of uncountable set is shown below.

\medskip

\begin{theorem}
Let $A$ be the set of all sequences whose elements are the digits $0$ and $1$. The set $A$ is uncountable.
\end{theorem}
\begin{proof}
The elements of $A$ are of form $1,0,1,1,0,\cdots$. Let $E$ be a countable subset of $A$ and let $E = \{s_k\}$. We construct a sequence $s$ as follows, if $n$th digit in $s_n$ is $1$, then let $n$th digit of $s$ be $0$, vice versa. Then the sequence $s$ differs from every element of $E$, hence $s \notin E$. However, $s \in A$, then $E$ is a proper subset of $A$.

Now we have proved that every countable subset of $A$ is a proper subset of $A$, thus $A$ is uncountable. Otherwise, $A$ would be a proper subset of $A$, which is a contradiction. 
\end{proof}

\begin{remark}
This theorem implies that with the binary representation, the set of all real numbers is uncountable.
\end{remark}

\medskip


\section{Metric spaces}

\begin{definition}\label{def_19}
A set $X$, whose elements are called points, is said to be a metric space if with any two points $x$ and $y$ of $X$ there is associated a real number $d(x,y): \mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R}$ called the distance from $x$ to $y$, which is defined as 
\begin{align*}
    d(x,y) = \left\|x - z\right\|,
\end{align*}
which has the following properties:
\begin{enumerate}[label=(\alph*)]
    \item $d(x,y) > 0$ if $x\neq y$;
    \item $d(x,y) = 0$ if $x = y$;
    \item $d(x,y) = d(y,x)$;
    \item $d(x,y) \leq d(x,z) + d(z,y)$.
\end{enumerate}
Any function with these three properties is called a distance function, or a metric. And the pair $(X,d)$ is called the metric space.
\end{definition}

\medskip

\begin{example}
Examples of metric spaces:
\begin{enumerate}[label=(\alph*)]
    \item $(\mathbb{R}^n,\rho_1)$, where $\rho_1(x,y) = \max_i \left|x_i - y_i\right|$.
    
    \item $(\mathbb{R}^n,\rho_2)$, where $\rho_2(x,y) = \sum^n_{i=1} |x_i-y_i|$, this is called taxi metric or New York metric. 
    
    \item $(\mathbb{R}^n,\rho_3)$, where $\rho_3(x,y) = \left\|x - y\right\| = \left(\sum^n_{i=1} (x_i - y_i)^2 \right)^{1/2}$, this is called standard Euclidean space.
    
    
    \item $(X,d)$, where $X$ is arbitrary set and 
    \begin{align*}
        d(x,y) = \begin{cases}
            1, & x \neq y, \\
            0, & x = y.
        \end{cases}
    \end{align*} 
    This is called discrete metric space.
    
    \item For $x = \{x_n\}^\infty_{n=1}$, let $l^1 = \{x \,:\, \sum^\infty_{n=1}\left|x_n\right| < \infty\}$, i.e., $l^1$ is the space of all absolutely convergent sequences. For $x, y \in l^1$, we define 
    \begin{align*}
        d_1(x,y) = \sum^\infty_{n=1} \left|x_n - y_n\right|.
    \end{align*}
    Then $(l^1,d_1)$ is a metric space.
    
    \item For $x = \{x_n\}^\infty_{n=1}$, let $l^2 = \{x  \,:\, \sum^\infty_{n=1}\left|x_n\right|^2 < \infty\}$. For $x, y \in l^1$, we define
    \begin{align*}
        d_2(x,y) = \left(\sum^\infty_{n=1} (x_n - y_n)^2\right)^{1/2}.
    \end{align*}
    Then $(l^2, d_2)$ is a metric space and this space is call Hilbert space.
\end{enumerate}
\end{example} 

\medskip

\begin{definition}
By the segment $(a,b)$ we mean the set if all real numbers $x$ such that $1 < x < b$. By the interval $[a,b]$ we mean the set of all real numbers $x$ such that $1 \leq x \leq b$. 

If $a_i < b_i$ for $i = 1, \cdots, k$, the set of all points $x = (x_1, \cdots, x_k) \in \mathbb{R}^k$ such that $a_i \leq x_i \leq b_i$ for $i = 1, \cdots, k$ is call a $k$-cell. 

We call a set $E \subset \mathbb{R}^k$ convex if $\lambda x + (1 - \lambda)y \in E$, for any $x,y \in E$ and $0 < \lambda < 1$.
\end{definition}

\medskip

\begin{definition}
Let $X$ be a metric space. 
\begin{enumerate}[label=(\alph*)]
    \item A neighborhood or a ball of $x$ is a set $B(x,r)$ consisting of all $y$ such that $d(x,y) < r$, for some $r > 0$. The number $r$ is called the radius of $B(x,r)$.
    
    \item A point $x$ is a limit point of set $E$ if every neighborhood of $x$ contains a point $y \neq x$ such that $y \in E$.
    
    \item If $x\in E$ and $x$ is not a limit point of $E$, then $x$ is called an isolated point of $E$.
    
    \item $E$ is closed if every limit point of $E$ is a point of $E$.
    
    \item A point $x$ is an interior point of $E$ if there is a neighborhood (or ball) $B(x,r)$ of $x$ such that $B(x,r) \in E$.
    
    \item $E$ is open if every point of $E$ is an interior point of $E$.
    
    \item The complement of $E$ (denoted by $E^c$) is the set of all points $x\in X$ such that $x\notin E$.
    
    \item $E$ is perfect if $E$ is closed and if every point of $E$ is a limit point of $E$.
    
    \item $E$ is bounded if there is a real number $M$ and a point $x\in X$ such that $d(x, y) < M$ for all $y\in E$.
    
    \item $E$ is dense in $X$ if every point of $X$ is a limit point of $E$, or a point of $E$(or both).
\end{enumerate}
\end{definition}

\medskip

\begin{theorem}
Every neighborhood is an open set.
\end{theorem}
\begin{proof}
Consider a neighborhood $B(x,r)$, it suffices to show that every point of $B$ is an interior point of $B$. Let $y$ be any point of $B(x,r)$, then there is $h > 0$ such that $d(x,y) = r - h$. For all points $s$ such that $d(y,s) < h$, we have
\begin{align*}
    d(x,s) \leq d(x,y) + d(y,s) \leq r - h + h = r,
\end{align*}
which implies $s \in E$. Thus $y$ is an interior point of $E$.
\end{proof}

\medskip

\begin{theorem}
If $x$ is a limit point of a set $E$, then every neighborhood of $x$ contains infinitely many points of $E$.
\end{theorem}
\begin{proof}
Suppose there is a neighborhood $B$ of $x$ which only contains only a finite number of points of $E$. Let $B \cap E = \{x_1, \cdots, x_n\}$, which are distinct from $x$, and let 
\begin{align*}
    r = \min_{1\leq i\leq n} d(x, x_n).
\end{align*}
Clearly, $r$ exists and $r > 0$. Hence, $B(x,r)$ contains no point of $E$ which is distinct from $x$, and then $x$ is not a limit point, a contradiction. 
\end{proof}

\medskip

\begin{corollary}
A finite point set has no limit point.
\end{corollary}

\medskip

\begin{theorem}\label{th_18}
Let $\{E_{\alpha}\}$ be a (finite or infinite) collection of sets $E_{\alpha}$. Then,
\begin{align*}
    \left(\bigcup_{\alpha} E_{\alpha}\right)^c = \bigcap_{\alpha} (E_{\alpha}^c).
\end{align*}
\end{theorem}
\begin{proof}
If $x \in \left(\bigcup_{\alpha} E_{\alpha}\right)^c$, then $x \notin \bigcup_{\alpha} E_{\alpha}$, hence $x \notin E_{\alpha}$ for every $\alpha$. Hence, $x \in E_{\alpha}^c$ for every $\alpha$, so $x \in \bigcap_{\alpha} (E_{\alpha}^c)$.
On the other hand, if $x \in \bigcap_{\alpha} (E_{\alpha}^c)$, then $x \in E_{\alpha}^c$ for every $\alpha$, hence $x \notin E_{\alpha}$ for every $\alpha$. Hence, $x \notin \bigcup_{\alpha} E_{\alpha}$, and thus, $x \in \left(\bigcup_{\alpha} E_{\alpha}\right)^c$. 
\end{proof}

\medskip

\begin{theorem}\label{th_19}
A set $A$ is open if and only if it complement is closed.
\end{theorem}
\begin{proof}
First, suppose $A^c$ is closed. For $x\in A$, then $x\notin A^c$, and $x$ is not a limit point of $E^c$. Then there exists $r>0$ such that $B(x,r) \cap A^c = \varnothing$. Then, we have $B(x,r) \subset A$. Thus $x$ is an interior point of $A$ and it follows that $A$ is open.

Next, suppose $A$ is open. Let $x$ be a limit point of $A^c$. Then every neighborhood of $x$ contains a point of $A^c$, so $x$ is not a interior point of $A$. Since $A$ is open, then $x\notin A$, which means $x\in A^c$. Since $x$ is a limit point of $A^c$, then $A^c$ is closed.
\end{proof}

\medskip

\begin{corollary}
A set $F$ is closed if and only if its complement is open.
\end{corollary}

\medskip

\begin{theorem}\label{th_110}
~\begin{enumerate}[label=(\alph*)]
    \item For any collection $\{G_{\alpha}\}$ of open sets, $\bigcup_{\alpha} G_{\alpha}$ is open.\label{th_110_a}
    
    \item For any collection $\{F_{\alpha}\}$ of closed sets, $\bigcap_{\alpha} F_{\alpha}$ is closed.\label{th_110_b}
    
    \item For any finite collection $G_1, \cdots, G_n$ of open sets, $\bigcap^n_{i=1} G_i$ is open.\label{th_110_c}
    
    \item For any finite collection $F_1, \cdots, F_n$ of closed sets, $\bigcup^n_{i=1} F_i$ is closed.\label{th_110_d}
\end{enumerate}
\end{theorem}
\begin{proof}
Let $G = \bigcup_{\alpha} G_{\alpha}$. If $x \in G$, then $x \in G_{\alpha}$ for some $\alpha$. Since $G_{\alpha}$ is open, then $x$ is an interior point of $G_{\alpha}$, and of course an interior point of $G$. Hence $G$ is open, and this proves \ref{th_110_a}.

By Theorem \ref{th_18}, 
\begin{align}\label{th_110_equ_1}
    \left(\bigcap_{\alpha} F_{\alpha}\right)^c = \bigcup_{\alpha} \left(F_{\alpha}^c\right),
\end{align}
and $F_{\alpha}^c$ is open by Theorem \ref{th_19}. Also, \ref{th_110_a} implies that \eqref{th_110_equ_1} is open and thus $\bigcap_{\alpha} F_{\alpha}$ is closed.

Now let $H = \bigcap^n_{i=1} G_i$, for any $x \in H$, there is neighborhoods $B(x,r_i)$ of $x$ such that each $B(x,r_i) \subset G_i, i = 1, \cdots, n$. Let $r = \min \{r_1, \cdots, r_n\}$, then clearly $B(x,r) \subset G_i$ for every $i = 1, \cdots, n$, and hence $B(x,r) \subset G$. Thus $G$ is open.

Similarly, \ref{th_110_d} follows from \ref{th_110_c} by
\begin{align*}
    \left(\bigcup^n_{i=1} F_{\alpha}\right)^c = \bigcap^n_{i=1} \left(F_{\alpha}^c\right)
\end{align*}
\end{proof}

\begin{remark}
In \ref{th_110_c} and \ref{th_110_d}, the finiteness of the union and intersection is required. For example, for $n = 1,2,3,\cdots$, let 
\begin{align*}
    G_n = \left(- \frac{1}{n}, \frac{1}{n}\right), 
\end{align*}
then $G = \bigcap^\infty_{n=1} G_n = \{0\}$, which is closed.  

Similarly, for $n = 1,2,3,\cdots$, let  
\begin{align*}
    F_n = \left[\frac{1}{n}, 1 - \frac{1}{n}\right],
\end{align*}
then $F = \bigcup^\infty_{n=1} F_n = (0,1)$, which is not closed.
\end{remark}

\medskip

\begin{definition}
Given $A \subset X$, the interior of the set $A$ is defined as the set of all points $x \in A$ that has a neighborhood contained in $A$, that is
\begin{align*}
    \operatorname{int}(A) = \{x \in A \,:\, \exists \, r > 0, B(x,r) \subset A\}.
\end{align*}
\end{definition}

\medskip

\begin{theorem}
The interior $\operatorname{int}(A)$ is always open and it is the largest open set contained in $A$ in the sense that if $G \subset A$ is open, then $G \subset \operatorname{int}(A)$.
\end{theorem}
\begin{proof}
Clearly, $\operatorname{int}(A)$ is open. Indeed, if $x \in \operatorname{int}(A)$, then there is $r > 0$ such that $B(x,r) \subset A$. Now consider any $y \in B(x,r/2)$, then $d(x,y) < r/2$. Then for any $z \in B(y, r/2)$, we have
\begin{align*}
    d(x,z) \leq d(x,y) + d(y,z) < r,
\end{align*}
which implies that every point in $B(x,r/2)$ is an interior point of $A$. Hence, there is a neighborhood $B(x,r/2)$ of $x$ such that $B(x,r/2) \subset \operatorname{int}(A)$, thus $\operatorname{int}(A)$ is open.

Now, let $G \subset A$ be any open set. Then for any $x \in G$, there is $r > 0$ such that $B(x,r) \subset G \subset A$, hence $x \in \operatorname{int}(A)$. Thus $G \subset \operatorname{int}(A)$.
\end{proof}

\medskip

\begin{definition}
If $X$ is a metric space, if $E \subset X$ and $E'$ denotes the set of all limit points of $E$, then the closure of $E$ is the set $\overline{E} = E \cup E'$.
\end{definition}

\medskip

\begin{theorem}\label{th_112}
If $X$ is a metric space and $E \subset X$, then
\begin{enumerate}[label=(\alph*)]
    \item $\overline{E}$ is closed; \label{th_112_a}
    
    \item $E = \overline{E}$ if and only if $E$ is closed; \label{th_112_b}
    
    \item $\overline{E} \subset F$ for every closed set $F \subset X$ that contains $E$. \label{th_112_c}
\end{enumerate}
By \ref{th_112_a} and \ref{th_112_c}, $\overline{E}$ is the smallest closed subset of $X$ that contains $E$.
\end{theorem}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item If $x \notin \overline{E}$, then $x$ is neither a point of $E$ nor a limit point of $E$. Then there is a neighborhood $B$ of $x$ such that $B \cap E = \emptyset$. Hence, $\overline{E}^c$ is open, and thus $\overline{E}$ is closed; 
    
    \item If $E = \overline{E}$, then \ref{th_112_a} implies $E$ is closed. If $E$ is closed, then $E' \subset E$, hence $\overline{E} = E \cup E' = E$;
    
    \item If $F$ is closed and $E \subset F$, then $F' \subset F$, hence $E' \subset F'$. Thus $\overline{E} \subset F$.
\end{enumerate}
\end{proof}

\medskip

\begin{theorem}\label{th_113}
Let $E$ be a nonempty set of real numbers which is bounded above. Let $y = \sup E$.\footnote{Recall the {\em least upper bound} or the {\em supremum}. Suppose $S$ is an ordered set, $E \subset S$ and $E$ is bounded above. Then the supremum $\alpha \in S$ satisfies the following properties: \begin{enumerate*}
    \item[(i)] $\alpha$ is an upper bound of $E$,
    \item[(ii)] If $\gamma < \alpha$, then $\gamma$ is not an upper bound of $E$.
\end{enumerate*} And we write $\alpha = \sup E$. The {\em greatest lower bound} or the {\em infimum} is defined in a similar way.}Then $y \in \overline{E}$. Hence $y \in E$ if $E$ is closed.
\end{theorem}
\begin{proof}
If $y \in E$, then $y \in \overline{E}$. Assume that $y \notin E$, for every $\varepsilon > 0$, there exists $x \in E$ such that $y - \varepsilon < x < y$, otherwise $y - \varepsilon$ is an upper bound of $E$. Hence, $y$ is a limit point of $E$, and thus $y \in \overline{E}$.
\end{proof}

\begin{remark}
Suppose $E \subset Y \subset X$. We say that $E$ is {\em open relative} to $Y$ if to each $x \in E$, there is $r > 0$ such that $y \in E$ whenever $d(x,y) < r$ and $y \in Y$. We talk about this since a set may be open relative to $Y$ without being an open subset of $X$. For example, let $E = (a,b)$, $a < b$ and $a,b \in \mathbb{R}$, $Y = \mathbb{R}$ and $X = \mathbb{R}^2$, then $(a,b)$ is an open subset of $\mathbb{R}$, but not an open subset of $\mathbb{R}^2$.
\end{remark}

\medskip

\begin{theorem}
Suppose $Y \subset X$. A subset $E$ of $Y$ is open relative to $Y$ if and only if $E = Y \cap G$ for some open subset $G$ of $X$.
\end{theorem}
\begin{proof}
Suppose $E$ is open relative to $Y$. For each $x \in E$, there is $r_x > 0$ such that $y \in E$ if $d(x,y) < r_x$ and $y \in Y$. Let $G_x = \{y \in X \,:\, d(x,y) < r_x\}$, and define
\begin{align*}
    G = \bigcup_{x \in E} G_x.
\end{align*}
Then $G$ is an open subset of $X$ by Theorem \ref{th_110}. Clearly, $E \subset G \cap Y$. Also, $G_x \cap Y \subset E$ for every $x \in E$, so that $G \cap Y \subset E$. Hence, $E = G \cap Y$. 

Conversely, if $G$ is open in $X$ and $E = G \cap Y$, every $x \in E$ has a neighborhood $B_x \subset G$. Then, $B_x \cap Y \subset E$, and thus $E$ is open relative to $Y$.
\end{proof}

\medskip


\section{Compact Sets}

\begin{definition}
An open cover of a set $E$ in a metric space $X$ is a collection $\{G_{\alpha}\}$ of open subsets of $X$ such that $E \subset \bigcup_{\alpha} G_{\alpha}$.
\end{definition}

\medskip

\begin{definition}
A subset $K$ of a metric space $X$ is said to be compact if every open cover of $K$ contains a finite subcover. More explicitly, this requirement is that if $\{G_{\alpha}\}$ is an open cover of $K$, then there are finitely many $\alpha_1, \cdots, \alpha_n$ such that 
\begin{align*}
    K \subset \bigcup^n_{i=1} G_{\alpha_i}.
\end{align*}
\end{definition}

\begin{remark}
With the familiarity of continuity, compactness can also be defined as follows: $K \subset X$ is compact if every sequence in $K$ has subsequence converging to a point in $K$. We will talk more about this later.
\end{remark}

\medskip

\begin{theorem}\label{th_115}
Compact subsets of metric space are closed.
\end{theorem}
\begin{proof}
Let $K$ be a compact subset of a metric space $X$. It suffices to prove that $K^c$ is open.

Suppose that $x \in K$ and $y \in K$, and let $B(x,r_x)$ and $B(y,r_x)$ be neighborhoods of $x$ and $y$, respectively, of radius $r_x$ less than $d(x,y)/2$. Clearly, $\bigcup_x B(x,r_x)$ is an open cover. Since $K$ is compact, then there is a finite subcover $\{B\left(x_i, r_{x_i}\right)\}^n_{i=1}$ such that 
\begin{align*}
    K \subset \bigcup^n_{i=1} B\left(x_i, r_{x_i}\right) = W.
\end{align*}
Now let $r_y = \min \{r_{x_1}, \cdots, r_{x_n}\}$, then $B(y,r_y) \cap W = \emptyset$, and hence $B(y,r_y) \subset K^c$. Hence $y$ is an interior point of $K^c$, and the rest follows.
\end{proof}

\medskip

\begin{theorem}\label{th_116}
Closed subsets of compact sets are compact.
\end{theorem}
\begin{proof}
Suppose $F \subset K \subset X$, $F$ is closed (relative to $X$), and $K$ is compact. Let $\{B_{\alpha}\}$ be an open cover of $F$. If $F^c \cap \bigcup_{\alpha}B_{\alpha} = \emptyset$, then we obtain an open cover $\Omega$ of $K$. Since $K$ is compact, there is a finite subcollection $\Phi$ of $\Omega$ which covers $K$, and hence $F$. If $F^c$ is a member of $\Phi$, then we can remove it from $\Phi$ and still obtain an open cover of $F$. Thus a finite subcollection of $\{B_{\alpha}\}$ covers $F$.
\end{proof}

\medskip

\begin{theorem}
If $F$ is closed and $K$ is compact, then $F \cap K$ is compact.
\end{theorem}
\begin{proof}
Theorems \ref{th_110} \ref{th_110_b} and \ref{th_115} show that $F \cap K$ is closed, since $F \cap K \subset K$, and Theorem \ref{th_116} shows that $F \cap K$ is compact.
\end{proof}

\medskip

\begin{theorem}\label{th_118}
If $\{K_{\alpha}\}$ is a collection of compact subsets of a metric space $X$ such that the intersection of every finite subcollection of $\{K_{\alpha}\}$ is nonempty, then $\cap_{\alpha} K_{\alpha}$ is nonempty.
\end{theorem}
\begin{proof}
Fix $K_1$ and let $G_{\alpha} = K_{\alpha}^c$. Assume no point of $K_1$ belongs to every $K_{\alpha}$. Then the sets $G_{\alpha}$ form an open cover of $K_1$. Since $K_1$ is compact, there are finitely many $\alpha_1, \cdots, \alpha_n$ such that
\begin{align*}
    K_1 \subset \bigcup^n_{i=1} G_{\alpha_i}.
\end{align*}
However, this implies
\begin{align*}
    K_1 \cap \left(\bigcup^n_{i=1} G_{\alpha_i}\right)^c = K_1 \cap \left(\bigcap^n_{i=1} G_{\alpha_i}^c\right) = K_1 \cap \left(\bigcap^n_{i=1} K_{\alpha_i}\right) = \emptyset,
\end{align*}
which is a contradiction.
\end{proof}

\medskip

\begin{corollary}\label{coro_118_1}
If $\{K_n\}$ is a sequence of nonempty compact sets such that $K_n \supset K_{n+1}, n = 1,2,3,\cdots$, then $\bigcap^\infty_{n=1} K_n$ is nonempty.
\end{corollary}

\medskip

\begin{theorem}\label{th_119}
If $E$ is an infinite subset of a compact set $K$, then $E$ has a limit point in $K$.
\end{theorem}
\begin{proof}
If no point in $K$ is a limit point of $E$, then each $x \in K$ would have a neighborhood $B(x,r)$ which contains at most one point of $E$, denoted by $y$, if $y \in E$. Then no finite open cover can cover $E$ and the same is true for $K$, since $E \subset K$, which is a contradiction.
\end{proof}

\medskip

\begin{theorem}\label{th_120}
If $\{I_n\}$ is a sequence of intervals in $\mathbb{R}$ such that $I_n \supset I_{n+1}, n = 1,2,3,\cdots$, then $\bigcap^\infty_{n=1} I_n$ is nonempty.
\end{theorem}
\begin{proof}
If $I_n = [a_n, b_n]$, let $E$ be the set of all $a_n$, then $E$ is nonempty and bounded above by $b_1$. Let $x = \sup E$. If $m$ and $n$ are positive integers, then 
\begin{align*}
    a_n \leq a_{n+m} \leq b_{n+m} \leq b_n,
\end{align*}
and hence $x \leq b_n$ for every $n$. Since $a_n \leq x$ for all $n$, it is clear that $x \in I_n$ for every $n$.
\end{proof}

\medskip

\begin{definition}
Let $a_n, b_n \in \mathbb{R}$ and $a_n < b_n$ for $n = 1,2,3,\cdots$, the set of all points $x = (x_1, \cdots, x_k) \in \mathbb{R}^k$ whose coordinates satisfy the inequalities $a_n \leq x_n \leq b_n, n = 1,2,3,\cdots$ is called a $k$-cell.
\end{definition}

\medskip

\begin{theorem}\label{th_121}
Let $k$ be a positive integer. If $\{I_n\}$ is a sequence of $k$-cells such that $I_n \supset I_{n+1}, n = 1,2,3,\cdots$, then $\bigcap^\infty_{n=1} I_n$ is nonempty.
\end{theorem}
\begin{proof}
For $x = (x_1, \cdots, x_k) \in \mathbb{R}^k$, let $I_n = \{x \,:\, a_{n,j} \leq x_j \leq b_{n,j}\}, 1 \leq j \leq k, n = 1,2,3\cdots$, and let $I_{n,j} = [a_{n,j}, b_{n,j}]$. For each $j$, the sequence $\{I_{n,j}\}$ satisfies the hypotheses of Theorem \ref{th_120}, hence there are real numbers $x_j^*, 1 \leq j \leq k$ such that $a_{n,j} \leq x_j^* \leq b_{n,j}$ for $1 \leq j \leq k, n = 1,2,3\cdots$. Now let $x^* = (x_1^*,\cdots, x_k^*)$, then it is clear that $x^* \in I_n$ for all $n = 1,2,3,\cdots$.
\end{proof}

\medskip

\begin{theorem}\label{th_122}
Every $k$-cell is compact.
\end{theorem}
\begin{proof}
Let $I$ be a $k$-cell, which consists of all points $x = (x_1, \cdots, x_k)$ such that $a_j \leq x_j \leq b_j$ for $1 \leq j \leq k$. Let
\begin{align*}
    \delta = \left(\sum^k_{j=1} (b_j - a_j)^2\right)^{1/2}.
\end{align*}
Then $\left|x - y\right| \leq \delta$ for all $x,y \in I$.

Suppose to the contrary that there exists an open cover $\{G_{\alpha}\}$ of $I$ which contains no finite subcover of $I$. Let $c_j = (a_j + b_j)/2$. The intervals $[a_j, c_j]$ determines $2^k$ $k$-cells $Q_i$ whose union is $I$. By assumption, at least one of these $Q_i$, say $I_1$, cannot be covered by any finite subcover of $\{G_{\alpha}\}$. We now divide $I_1$ and continue this process, and we obtain a sequence $\{I_n\}$ with the following properties:
\begin{enumerate}[label=(\alph*)]
    \item $I \supset I_1 \supset I_2 \supset I_3 \cdots$; \label{th_122_a}
    
    \item $I_n$ is not covered by any finite subcollection of $\{G_{\alpha}\}$; \label{th_122_b}
    
    \item if $x,y \in I_n$, $\left|x - y\right| \leq 2^{-n} \delta$. \label{th_122_c}
\end{enumerate}
By \ref{th_122_a} and Theorem \ref{th_121}, there is $x^*$ which belongs to every $I_n$. For some $\alpha$, $x^* \in G_{\alpha}$. Since $G_{\alpha}$ is open, there is $r > 0$ such that $\left|y - x^*\right| < r$ implies that $y \in G_{\alpha}$. Letting $n$ large enough such  that $2^{-n}\delta < r$, then \ref{th_122_c} implies that $I_n \subset G_{\alpha}$, which is a contradiction to \ref{th_122_b}.
\end{proof}

\medskip

\begin{theorem}[Heine-Borel]\label{th_123}
If a set $E \subset \mathbb{R}^n$ has one of the following three properties, then it has the other two:
\begin{enumerate}[label=(\alph*)]
    \item $E$ is closed and bounded. \label{th_123_a}
    
    \item $E$ is compact. \label{th_123_b}
    
    \item Every infinite subset of $E$ has a limit point in $E$. \label{th_123_c}
\end{enumerate}
The equivalence of \ref{th_123_a} and \ref{th_123_b} is known as the Heine-Borel theorem.
\end{theorem}
\begin{proof}
If \ref{th_123_a} holds, then $E \subset I$ for some $k$-cell $I$, and hence \ref{th_123_b} follows from Theorems \ref{th_122} and \ref{th_116}. Also, \ref{th_123_b} implies \ref{th_123_c} by Theorem \ref{th_119}. It remains to show that \ref{th_123_c} implies \ref{th_123_a}.

Suppose to the contrary that $E$ is not not bounded, then $E$ contains points $x_n$ with $\left|x_n\right| > n, n = 1,2,3,\cdots$. The set consisting of all $x_n$ is infinite and clearly has no limit point in $\mathbb{R}^n$, hence has no limit point in $E$. Hence, \ref{th_123_c} implies that $E$ is bounded.

If $E$ is not closed, then there is a point $x \in \mathbb{R}^n$ which is a limit of $E$ but $x \notin E$. For $n = 1,2,3,\cdots$, there are points $x_n \in E$ such that $\left|x_n - x\right| < 1/n$. Let $S = \{x_n\}$, then $S$ is infinite and $x$ is a limit point of $S$. Also, $x$ is the only limit point of $S$. Indeed, for any $y \in \mathbb{R}^n$, $y \neq x$, then for all but finitely many $n$,
\begin{align*}
    \left|x_n - y\right| & \geq \left|x_n - x\right| - \left|x - y\right| \geq \left|x - y\right| - \frac{1}{n} \geq \frac{1}{2} \left|x - y\right|,
\end{align*}
which implies that $y$ is not a limit point of $S$. This is a contradiction to \ref{th_123_c} since $S \subset E$.
\end{proof}

\begin{remark}
Note that \ref{th_123_b} and \ref{th_123_c} are equivalent in any metric space but in general, \ref{th_123_a} does not implies \ref{th_123_b} and \ref{th_123_c}. More details will be discussed later.
\end{remark}

\medskip

\begin{theorem}[Weierstrass]\label{}
Every bounded infinite subset of $\mathbb{R}^n$ has a limit point in $\mathbb{R}^n$.
\end{theorem}
\begin{proof}
Suppose that $E \subset \mathbb{R}^n$ is bounded, then $E$ is a subset of a $k$-cell $I \subset \mathbb{R}^n$. By Theorem \ref{th_122}, $I$ is compact, and Theorem \ref{th_123} implies that $E$ has a limit point in $I$.
\end{proof}

\medskip

Now we talk about some other properties and results of the compact sets.

\medskip

\begin{definition}
A metric space $X$ is said to be locally bounded if for every $\varepsilon > 0$ there exists a finite covering of $X$ by balls of radius $\varepsilon$.
\end{definition}

\medskip

\begin{lemma}
If a metric space $X$ is compact, then $X$ is totally bounded.
\end{lemma}
\begin{proof}
Suppose to the contrary that there is a $\varepsilon > 0$ such that $X$ cannot be covered by finite collection of balls of radius $\varepsilon$. Let $x_1 \in X$, then $B(x_1,\varepsilon) \neq X$, then there exists $x_2 \in X$ such that $x_2 \notin B(x_1,\varepsilon)$ and $B(x_1,\varepsilon) \cup B(x_2,\varepsilon) \neq X$, or otherwise $X$ can be covered by finite balls. We can continue this process and obtain a sequence $\{x_1, x_2, \cdots\}$ in $X$ such that $d(x_n,x_m) \geq \varepsilon$ if $n \neq m$. Clearly this sequence $\{x_n\}$ has no convergent subsequence, which is a contradiction to the compactness of $X$.
\end{proof}

\medskip

\begin{theorem}\label{th_125}
A metric space $X$ is compact if and only if it is complete and totally bounded.
\end{theorem}
\begin{proof}
Suppose $X$ is compact, by the previous lemma, $X$ is totally bounded. Now let $\{x_n\}$ be a Cauchy sequence in $X$. Since $X$ is compact, by Theorem \ref{th_123}, there exists a subsequence $\{x_{n_k}\}$ such that converges to a point in $X$, say $x_0$. Hence $X$ is complete.

Conversely, we need to prove that every sequence in $X$ has a convergent subsequence in $X$. Since $X$ is totally bounded, then there is a finite covering of balls of radius $1$, that is,
\begin{align*}
    X = \bigcup^{N_1}_{i=1} B\left(x_i^1, 1\right).
\end{align*}
Then infinitely many elements of sequence $\{x_n\}$ belong to at least one of the balls, say $B\left(x_{i_1}^1, 1\right)$, and this ball also has a finite covering by balls of radius $1/2$, that is,
\begin{align*}
    B\left(x_{i_1}^1, 1\right) = \bigcup^{N_2}_{i=1} B\left(x_i^2, \frac{1}{2}\right) \cap B\left(x_{i_1}^1, 1\right).
\end{align*}
Again, there are infinitely many elements of the sequence $\{x_n\}$ belong to at least one of the sets on the right hand side, say $B\left(x_{i_2}^2, 1/2\right) \cap B\left(x_{i_1}^1, 1\right)$, and then this set has a finite covering by ball of radius $1/3$, that is,
\begin{align*}
    B\left(x_{i_2}^2, 1/2\right) \cap B\left(x_{i_1}^1, 1\right) = \bigcup^{N_3}_{i=1} B\left(x_i^3, \frac{1}{3}\right) \cap B\left(x_{i_2}^2, \frac{1}{2}\right) \cap B\left(x_{i_1}^1, 1\right).
\end{align*}
Continue this process and we obtain a subsequence from $\{x_n\}$ such that
\begin{align*}
    x_{n_1} \in B\left(x_{i_1}^1, 1\right), \quad x_{n_2} \in B\left(x_{i_2}^2, \frac{1}{2}\right) \cap B\left(x_{i_1}^1, 1\right), \quad x_{n_3} \in B\left(x_{i_3}^3, \frac{1}{3}\right) \cap B\left(x_{i_2}^2, \frac{1}{2}\right) \cap B\left(x_{i_1}^1, 1\right), \cdots.
\end{align*}
Therefore, for $k,l > N$ we have $x_{n_k}, x_{n_l} \in B\left(x_{i_N}^N, 1/N\right)$ and hence
\begin{align*}
    d(x_{n_k}, x_{n_l}) < \frac{2}{N},
\end{align*}
this implies that $\{x_{n_k}\}$ is a Cauchy sequence. Since $X$ is also complete, then $\{x_{n_k}\}$ converges to a point in $X$, and thus $X$ is compact.
\end{proof}

\medskip


\section{Perfect Sets}

\begin{theorem}\label{th_126}
Let $P$ be a nonempty perfect set in $\mathbb{R}^n$, then $P$ is uncountable.
\end{theorem}
\begin{proof}
Since $P$ has limit points, $P$ is infinite. Suppose $P$ is countable, then $P$ can be written as $\{x_1, x_2, x_3, \cdots\}$. Let $V_1$ be any neighborhood of $x_1$. Suppose $V_n$ has been constructed, so that $V_n \cap P \neq \emptyset$. Since every point of $P$ is a limit of $P$, there is a neighborhood $V_{n+1}$ of $x$ such that \begin{enumerate*}[label=(\roman*)]
    \item $\overline{V}_{n+1} \subset V_n$,
    \item $x_n \notin \overline{V}_{n+1}$,
    \item $V_{n+1} \cap P \neq \emptyset$.\label{th_126_iii}
\end{enumerate*}
By \ref{th_126_iii}, $V_{n+1}$ satisfies our induction hypothesis, and the process can proceed. 

Let $K_n = \overline{V}_{n} \cap P$. Since $\overline{V}_{n}$ is closed and bounded, $\overline{V}_{n}$ is compact. Since $x_n \notin K_{n+1}$, no point of $P$ lies in $\bigcap^\infty_{n=1} K_n$. Since $K_n \subset P$, this implies $\bigcap^\infty_{n=1} K_n = \emptyset$. However, each $K_n \neq \emptyset$ and $K_n \supset K_{n+1}$, and by Corollary \ref{coro_118_1}, $\bigcap^\infty_{n=1} K_n$ is nonempty, thus this is a contradiction.
\end{proof}

\medskip

\begin{corollary}
Every interval $[a,b], a < b$ is uncountable. In particular, the set $\mathbb{R}$ of all real numbers is uncountable.
\end{corollary}

\medskip

\begin{example}[The Cantor set]
The Cantor set shows that there exist perfect sets in $\mathbb{R}$ which contains no segment, and it is constructed as follows:
\begin{enumerate}[label=(\roman*)]
    \item Let $\mathcal{C}_1$ be the interval $[0,1]$;
    
    \item Remove the segment $\left(1/3,2/3\right)$, and let $\mathcal{C}_2$ be the union of the interval of $\left[0,1/3\right],\left[1/3,1\right]$;
    
    \item Remove the middle thirds of these two intervals and let $\mathcal{C}_3$ be the union of $\left[0,1/9\right]$, $\left[2/9,1/3\right]$, $\left[6/9,7/9\right]$ and $\left[8/9,1\right]$;
    
    \item Continue this way and we can get a sequence of compact sets $\mathcal{C}_n$, such that $\mathcal{C}_1\supset \mathcal{C}_2\supset \mathcal{C}_3\supset\cdots$ and $\mathcal{C}_n$ is the union of $2^n$ intervals with length $3^{-n}$.
\end{enumerate}
Then the {\em Cantor set} is defined as 
\begin{align*}
    \mathcal{C} = \bigcap^\infty_{i=1}\mathcal{C}_i.
\end{align*}
$\mathcal{C}$ is clearly compact and by Theorem \ref{th_118}, $\mathcal{C}$ is nonempty.

No segment of the form
\begin{align}\label{exam_12_equ1}
    \left(\frac{3k+1}{3^n}, \frac{3k+2}{3^n}\right),
\end{align}
where $k,n$ are positive integers, has a point in common with $\mathcal{C}$. Since every segment $(\alpha, \beta)$ contains a segment of the form \eqref{exam_12_equ1}, if 
\begin{align*}
    3^{-m} < \frac{\beta - \alpha}{6},
\end{align*}
$\mathcal{C}$ contains no segment.

To show that $\mathcal{C}$ is perfect, it is enough to show that $\mathcal{C}$ contains no isolated point. Let $x \in \mathcal{C}$, and let $S$ be any segment containing $x$. Let $I_n$ be the interval of $\mathcal{C}_n$ which contains $x$. Let $n$ large enough such that $I_n \subset S$. Let $x_n$ be an endpoint of $I_n$ such that $x_n \neq x$. It is clear that $x_n \in \mathcal{C}$ and hence $x$ is a limit point of $\mathbb{C}$. Thus $\mathbb{C}$ is perfect, and hence uncountable. 

One of the most interesting facts about the Cantor set is that it provides an example as an uncountable set of measure zero.
\end{example}

\medskip

Now we provide another approach to proving that the Cantor set is uncountable.

\medskip

\begin{proof} We use Cantor's first proof of uncountability. Let $\mathcal{C}$ be the cantor set, and let $E = \{u_1,u_2,\cdots\}$ be a countable set. We construct a point of $\mathcal{C}$ but not in $E$. First, $\mathcal{C} \subseteq [0,1/3] \cup [2/3,1]$, and point $u_1$ does not belong to both of those intervals, so there is an interval $I_1$of length $1/3$ (one of the ones in the first stage of the construction of the Cantor set) with $u_1 \notin I_1$. Now when we remove the middle third of $I_1$ we get two intervals of length $1/3^2$. As before, $u_2$ does not belong to both of these intervals, so there is an interval $I_2 \subset I_1$ of length $1/3^2$ (one of the ones in the second stage of the construction of the Cantor set) with $u_2 \notin I_2$. Continue in this way to get $I_1 \supset I_2 \supset I_3 \supset \cdots \supset I_k \supset \cdots$ where $I_k$ has length $1/3^k$ (one of the intervals in the $k$th stage of the construction of the Cantor set) and $u_k \notin I_k$. Finally, we get a point $x \in \bigcap^\infty_{k=1} I_k$ where $x \in \mathcal{C}$ but $x \neq u_k$ for all $k$.
\end{proof}

\medskip



\section{Connected Sets}

\begin{definition}
Let $X$ be a metric space, and $A \subset X$. $A$ is said to be disconnected if there exist open sets $U$ and $V$ in $X$ which satisfy the following properties:
\begin{enumerate}[label=(\alph*)]
    \item $A \subset U \cup U$;
    
    \item $A \cap U \neq \emptyset$ and $A \cap V \neq \emptyset$;
    
    \item $A \cap (U \cap V) = \emptyset$.
\end{enumerate}
Moreover, $A$ is called connected if it is not disconnected.
\end{definition}

\medskip

\begin{proposition}
Prove that the space $X$ is connected if and only if the only subsets of $X$, which are open and closed at the same time, are $\emptyset$ and $X$ itself.
\end{proposition}
\begin{proof}
If $X$ is connected, we need to show that if $E \subset X$ is open and closed, then $E = \emptyset$ or $E = X$. Suppose to the contrary that there is a set $E \subset X$ such that $E \neq \emptyset, E \neq X$ and $E$ is open and closed at the same time. Since $E$ is closed, then $E^c$ is open, and hence $X \subset (E \cup E^c)$. Also, $X \cap E \neq \emptyset$, $X \cap E^c \neq \emptyset$, and $X \cap (E \cap E^c) = \emptyset$. Hence $X$ is disconnected, which is a contradiction.

Conversely, suppose that $X$ is disconnected, then there are two open sets $U$ and $V$ in $X$ such that $X = U \cup V$, $X \cap U \neq \emptyset$, $X \cap V \neq \emptyset$ and $X \cap (U \cap V) = \emptyset$. Hence, $U \neq \emptyset$. Also, $U$ is also closed since $V = U^c$ is open. Thus $U$ is open and closed at the same time, which is a contradiction.
\end{proof}

\medskip

Next, we talk about another definition of connected sets.

\medskip

\begin{definition}
Two subsets $A$ and $B$ of a metric space $X$ are said to be separated if $A \cap \overline{B} = \emptyset$ and $\overline{A} \cap B = \emptyset$. A set $E \subset X$ is said to be connected if $E$ is not a union of two nonempty separated sets (this definition is equivalent to the previous one).
\end{definition}

\begin{remark}
Separated sets are of course disjoint, but disjoint sets need not be separated. For example, the intervals $[0,1]$ and $(1,2)$ are not separated, since $1$ is a limit point of $(1,2)$. However, $(0,1)$ and $(1,2)$ are separated.
\end{remark}

\medskip

\begin{theorem}\label{th_126}
A subset $E \subset \mathbb{R}$ is connected if and only if it has the following property: If $x,y \in E$ and $x < z < y$, then $z \in E$.
\end{theorem}
\begin{proof}
Suppose there exist $x, y \in E$ and $z \in (x,y)$, but $z \notin E$. Then $E = U \cup V$, where 
\begin{align*}
    U = (-\infty, z) \cap E,\qquad V = (z, \infty) \cap E.
\end{align*}
Since $x \in U$ and $y \in V$, $U$ and $V$ are nonempty. Also, since $U \cap V = \emptyset$, $E \cap (U \cap V) = \emptyset$, and hence $E$ is disconnected, which is a contradiction.

Conversely, suppose that $E$ is disconnected. Then there are nonempty separated sets $U$ and $V$ such that $E = U \cup V$. Let $x \in U$ and $y \in V$, and without loss of generality we assume that $x < y$. Let
\begin{align*}
    z = \sup (U \cap [x,y]).
\end{align*}
By Theorem \ref{th_113}, $z \in \overline{U}$, hence $z \notin V$. In particular, $x \leq z < y$. If $z \notin U$, it follows that $x < z < y$ and $z \notin E$. If $z \in U$, then $z \notin \overline{V}$, hence there exists $z_1$ such that $z < z_1 < y$ and $z_1 \notin E$, which is a contradiction.
\end{proof}










\newpage

\chapter{Numerical Sequences and Series}

\section{Convergent Sequences}

\begin{definition}
A sequence $\{x_n\}$ in a metric space $X$ is said to converge if there is a point $x \in X$ with the following property: For every $\varepsilon > 0$, there is an integer $N$ such that for all $n \geq N$, $d(x_n,x) < \varepsilon$. (Here $d$ denotes the distance in $X$.)

In this case, we also say that $\{x_n\}$ converges to $x$, or $x$ is the limit of $\{x_n\}$, and we write $x_n \to x$, or
\begin{align*}
    \lim_{n\to\infty} x_n = x.
\end{align*}

If $\{x_n\}$ does not converge, we say it diverge\footnote{The formal definition of being divergent can be expressed as: If for every $x \in X$, there exists $\varepsilon > 0$ such that for all $N \in \mathbb{N}$, there is $n \geq N$, $d(x_n,x) > \varepsilon$. Also, we say a sequence $\{x_n\}$ diverges to $+\infty$ if for all $M > 0$, there is an integer $N > 0$ such that for all $n \geq N$, $x_n > M$. Then we write $\lim_{n\to\infty} x_n = + \infty$. Similarly, we could define a sequence that diverges to $-\infty$.}.
\end{definition}

\medskip

\begin{theorem}\label{th_21}
Let $\{x_n\}$ be a sequence in a metric space $X$.
\begin{enumerate}[label=(\alph*)]
    \item $\{x_n\}$ converges to $x \in X$ if and only if every neighborhood of $x$ contains $x_n$ for all but finitely many $n$. \label{th_21_a}
    
    \item If $x \in X$, $x' \in X$ and if $\{x_n\}$ converges to $x$ and to $x'$, then $x = x'$. \label{th_21_b}
    
    \item If $\{x_n\}$ converges, then $\{x_n\}$ is bounded. \label{th_21_c}
    
    \item If $E \subset X$ and $x$ is a limit point of $E$, then there is a sequence $\{x_n\}$ in $E$ such that $\lim_{n\to\infty} x_n = x$. \label{th_21_d}
\end{enumerate}
\end{theorem}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item Suppose $x_n \to x$ and let $V$ be a neighborhood of $x$. For some $\varepsilon > 0$, $d(x,y) < \varepsilon, y \in X$ implies that $y \in V$. Corresponding to this $\varepsilon$, there is $N > 0$ such that for all $n \geq N$, $d(x_n,x) < \varepsilon$. Hence for all $n \geq N$, $x_n \in V$.
    
    Conversely, suppose every neighborhood of $x$ contains all but finitely many $x_n$. Fix $\varepsilon > 0$, and let $V$ be the set of all $y \in X$ such that $d(x,y) < \varepsilon$. By assumption, there is $N > 0$ which depends on $V$, such that for $n \geq N$, $x_n \in V$. Hence $d(x_n, x) < \varepsilon$ for $n \geq N$, which implies that $x_n \to x$.
    
    \item Let $\varepsilon > 0$ be given, then there are integers $N, N'$ such that for all $n \geq N$, $d(x_n,x) < \varepsilon/2$ and for all $n \geq N'$, $d(x_n,x') < \varepsilon/2$. Hence if $n \geq \max \{N, N'\}$, we have
    \begin{align*}
        d(x,x') \leq d(x,x_n) + d(x_n,x') < \varepsilon.
    \end{align*}
    Since $\varepsilon$ is arbitrary, we conclude that $d(x,x') = 0$.
    
    \item Let $\varepsilon = 1$, then there is an integer $N$ such that for all $n \geq N$, $d(x_n, x) < 1$. Let \begin{align*}
        r = \max\{1, d(x_1,x), \cdots, d(x_N,x)\},
    \end{align*}
    then $d(x_n,x) \leq r$ for all $n = 1,2,3,\cdots$.
    
    \item Since $x$ is a limit point of $E$, then for neighborhood of $x$ with radius $1/n, n = 1,2,3,\cdots$, there is a point $x_n \in E$ such that $d(x_n,x) < 1/n$. For any $\varepsilon > 0$, let $N > 1/\varepsilon$, then for all $n \geq N$, $d(x_n, x) < \varepsilon$, which implies $x_n \to x$.
\end{enumerate}
\end{proof}

\medskip

Next, we talk about some important properties about convergent sequences.

\medskip

\begin{theorem}\label{th_22}
Suppose $\{a_n\}$ and $\{b_n\}$ are two complex sequences, and $\lim_{n\to\infty} a_n = a$ and $\lim_{n\to\infty} b_n = b$. Then,
\begin{enumerate}[label=(\alph*)]
    \item $\lim_{n\to\infty} (a_n + b_n) = a + b$; \label{th_22_a}
    
    \item $\lim_{n\to\infty} c a_n = ca$, $\lim_{n\to\infty} (c + a_n) = c + a_n$, for any $c \in \mathbb{R}$; \label{th_22_b}
    
    \item $\lim_{n\to\infty} a_n b_n = ab$; \label{th_22_c}
    
    \item $\lim_{n\to\infty} a_n/b_n = a/b$, provided $b_n \neq 0$ for all $n$ and $b \neq 0$. \label{th_22_d}
\end{enumerate}
\end{theorem}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item and \ref{th_22_b} are trivial.
    
    \setcounter{enumi}{2}
    \item For every $\varepsilon > 0$, there are integers $N_1$ and $N_2$ such that for all $n \geq N_1$, $\left|a_n - a\right| < \sqrt{\varepsilon}$ and for all $n \geq N_2$, $\left|b_n - b\right| < \sqrt{\varepsilon}$. Let $N = \max \{N_1, N_2\}$, for all $n \geq N$, we have
    \begin{align*}
        \left|(a_n - a)(b_n - b)\right| < \varepsilon,
    \end{align*}
    which implies that $\lim_{n\to\infty} (a_n - a)(b_n - b) = 0$. Also, by \ref{th_22_a} and \ref{th_22_b},
    \begin{align*}
        \lim_{n\to\infty} (a_nb_n - ab) = \lim_{n\to\infty} \left[(a_n - a)(b_n - b) + a(b_n - b) + b(a_n - a)\right] = 0.
    \end{align*}
    
    \item For $\varepsilon = \left|b\right|/2$, there is an integer $N_1$ such that for all $n \geq N_1$, $ \left|b_n - b\right| < \left|b\right|/2$, and hence $\left|b_n\right| > \left|b\right|/2$ for $n \geq N_1$. 
    
    Now for any $\varepsilon > 0$, there is an integer $N_2$ such that for all $n \geq N_2$, $\left|b_n - b\right| < \left|b\right|^2\varepsilon/2$. Let $N = \max \{N_1, N_2\}$, then for all $n \geq N$, 
    \begin{align*}
        \left|\frac{1}{b_n} - \frac{1}{b}\right| = \left|\frac{b_n - b}{b_n b}\right| < \frac{2}{\left|b\right|^2} \left|b_n - b\right| < \varepsilon.
    \end{align*}
    Hence, $\lim_{n\to\infty} 1/b_n = 1/b$, and by \ref{th_22_c}, $\lim_{n\to\infty} a_n/b_n = a/b$.
\end{enumerate}
\end{proof}

\medskip

Now we can discuss the sequences in $\mathbb{R}^k$.

\medskip

\begin{theorem}
~\begin{enumerate}[label=(\alph*)]
    \item Suppose $x_n \in \mathbb{R}^k, n = 1,2,3,\cdots$ and $x_k = (x_{1,n}, \cdots, x_{k,n})$. Then $\{x_n\}$ converges to $x = (x_1, \cdots, x_k)$ if and only if 
    \begin{align*}
        \lim_{n\to\infty} x_{j,n} = x_j, \quad 1 \leq j \leq k.
    \end{align*}
    
    \item Suppose $\{x_n\}, \{y_n\}$ are sequences in $\mathbb{R}^k$, $\{\beta_n\}$ is a sequence of real numbers, and $x_n \to x, y_n \to y, \beta_n \to \beta$. Then,
    \begin{align*}
        \lim_{n\to\infty} (x_n + y_n) = x + y, \quad \lim_{n\to\infty} x_n \cdot y_n = x \cdot y, \quad \lim_{n\to\infty} \beta_n x_n = \beta x.
    \end{align*}
\end{enumerate}
\end{theorem}
\begin{proof}
The proof is trivial and we skip it here.
\end{proof}

\medskip

Now we talk some important results and applications about convergent sequences.

\medskip

\begin{theorem}
If $a_n \leq b_n \leq c_n$ and $\lim_{n\to\infty} a_n = \lim_{n\to\infty} c_n = g \in \mathbb{R}$, then $\lim_{n\to\infty} b_n = g$.
\end{theorem}

\medskip

We now talk about some special sequences which will be used frequently.

\medskip

\begin{theorem}\label{th_25}
~\begin{enumerate}[label=(\alph*)]
    \item If $a > 0$, then $ \lim_{n\to\infty} \displaystyle \frac{1}{n^a} = 0$.\label{th_25_a}
    
    \item If $a > 0$, $\lim_{n\to\infty} \displaystyle \sqrt[n]{a} = 1$.\label{th_25_b}
    
    \item For $n \in \mathbb{N}$, $ \lim_{n\to\infty} \displaystyle \sqrt[n]{n} = 1$.\label{th_25_c}
    
    \item If $a > 0$ and $k$ is real, then $ \lim_{n\to\infty} \displaystyle \frac{n^k}{(1 + a)^n} = 0$.\label{th_25_d}
    
    \item If $a > 1$, then $\lim_{n\to\infty} a^{-n} = 0$. If $\left|a\right| < 1$, then $\lim_{n\to\infty} a^n = 0$.\label{th_25_e}
\end{enumerate}
\end{theorem}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item For every $\varepsilon > 0$, let $N > (1/\varepsilon)^{1/a}$, then for all $n \geq N$, 
    \begin{align*}
        \frac{1}{n^a} < \frac{1}{\left[(1/\varepsilon)^{1/a}\right]^a} = \varepsilon.
    \end{align*}
    
    \item If $a > 1$, let $x_n = \sqrt[n]{a} - 1$. By the Binomial formula,\footnote{For $a, b \in \mathbb{R}$ and $n \in \mathbb{N}$, \begin{align*}
    (a + b)^n = \sum^n_{k=1} \binom{n}{k} a^{n-k}b^{k} = \sum^n_{k=1} \frac{n!}{k!(n-k)!} a^{n-k}b^{k}.
    \end{align*}}we have $a = (1 + x_n)^n \geq 1 + nx_x$, and hence
    \begin{align*}
        0 < x_n < \frac{a - 1}{n},
    \end{align*}
    since the right hand side converges to $0$ as $n \to \infty$, we could conclude that $\sqrt[n]{a} - 1 \to 0$, which implies $\lim_{n\to\infty} \sqrt[n]{a} = 1$.

    If $a = 1$, then $\sqrt[n]{a} = 1 \to 1$. If $0 < a < 1$, then $1/a > 1$ and hence $\sqrt[n]{1/a} \to 1$. Thus, by Theorem \ref{th_22} \ref{th_22_d},
    \begin{align*}
        \lim_{n\to\infty} \sqrt[n]{a} = \lim_{n\to\infty} \frac{1}{\sqrt[n]{1/a}} = 1.
    \end{align*}
    
    \item Let $x_n = \sqrt[n]{n} - 1$, then $x_n > 0$ and applying the Binomial formula implies that 
    \begin{align*}
        n = (1 + x_n)^n \geq \binom{n}{2} 1^{n-1} x_n^2 = \frac{n(n-1)}{2} x_n^2,
    \end{align*}
    and hence
    \begin{align*}
        0 \leq x_n \leq \sqrt{\frac{2}{n-1}}.
    \end{align*}
    Since both the left and right hand sides converges to $0$, we have $\sqrt[n]{n} - 1 \to 0$.
    
    \item Let $m$ be an integer such that $m > k$. For $n > 2m$, we have
    \begin{align*}
        (1 + a)^n > \binom{n}{m}a^m = \frac{n!}{m!(n-m)!} a^m > \frac{n^m a^m}{2^m m!}.
    \end{align*}
    Hence, 
    \begin{align*}
        0 < \frac{n^k}{(1 + a)^n} < \frac{2^m m!}{a^m} n^{k - m} \xrightarrow[]{n\to\infty} 0.
    \end{align*}
    
    \item If $a > 1$, the Binomial formula implies 
    \begin{align*}
        a^n = (1 + (a - 1))^n \geq 1 + n(a - 1),
    \end{align*}
    and hence
    \begin{align*}
        0 < a^{-n} \frac{1}{1 + n(a - 1)} \xrightarrow[]{n\to\infty} 0.
    \end{align*}
    
    If $a = 0$, then it is clear that $a^n = 0$ and if $0 < \left|a\right| < 1$, then $1/\left|a\right| > 1$. Hence,
    \begin{align*}
        \left|a^n - 0\right| = \left(\frac{1}{\left|a\right|}\right)^{-n} \xrightarrow[]{n\to\infty} 0.
    \end{align*}
\end{enumerate}
\end{proof}

\medskip

\begin{definition}\label{def_22}
The extended real line $\overline{\mathbb{R}}$ is defined as $\overline{\mathbb{R}} = \mathbb{R} \cup \{-\infty, \infty\}$.
\end{definition}

\medskip

Some important results about convergent sequences will be shown below.

\medskip

\begin{theorem}\label{th_26}
If $\lim_{n\to\infty} a_n = a \in \overline{\mathbb{R}}$, then
\begin{align*}
    \lim_{n\to\infty} \frac{a_1 + a_2 + \cdots + a_n}{n} = a.
\end{align*}
\end{theorem}
\begin{proof}
Since $a_n \to a$, then for every $\varepsilon > 0$, there is an integer $N_1$ such that for all $n \geq N_1$, $\left|a_n - a\right| < \varepsilon$. Also, since $\{a_n\}$ is convergent, there is $M > 0$ such that $\left|a_n\right| \leq M$ for all $n$ and $\left|a\right| < M$. Now, let $N = \max \left\{2N_1 M/\varepsilon, N_1\right\}$, then for all $n \geq N$, we have
\begin{align*}
    \left|\frac{a_1 + \cdots + a_n}{n} - a\right| & = \left|\frac{(a_1-a) + \cdots + (a_n-a)}{n}\right| \\
    & \leq \left|\frac{(a_1-a) + \cdots + (a_{N_1}-a)}{n}\right| + \left|\frac{(a_{N_1+1}-a) + \cdots + (a_n-a)}{n}\right| \\
    & \leq \frac{2N_1M}{n} + \frac{n - N_1}{n} \varepsilon \leq 2 \varepsilon.
\end{align*}
\end{proof}

\medskip

\begin{theorem}\label{th_27}
If $\lim_{n\to\infty} a_n = a \in \overline{\mathbb{R}}$, and $a_n > 0$ for all $n$, then
\begin{align*}
    \lim_{n\to\infty} \sqrt[n]{a_1 a_2 \cdots a_n} = a.
\end{align*}
\end{theorem}
\begin{proof}
Since $a_n \to a$, then for every $\varepsilon > 0$, there is an integer $N$ such that for all $n \geq N$, $\left|a_n - a\right| < \varepsilon$. Hence, for any $n > N$, since $a_1 a_2 \cdots a_N (g+\varepsilon)^{-N} > 0$, by Theorem \ref{th_25} \ref{th_25_b}, we have
\begin{align*}
    \limsup_{n\to\infty} \sqrt[n]{a_1 a_2 \cdots a_n} & = \limsup_{n\to\infty} \sqrt[n]{a_1 a_2 \cdots a_N} \sqrt[n]{a_{N+1}\cdots a_n} \\
    & \leq \limsup_{n\to\infty} \sqrt[n]{a_1 a_2 \cdots a_N} \sqrt[n]{(g+\varepsilon)^{n-N}} \\
    & = \limsup_{n\to\infty} \sqrt[n]{a_1 a_2 \cdots a_N (g+\varepsilon)^{-N}} \sqrt[n]{(g+\varepsilon)^n} \\
    & = (g+\varepsilon) \cdot \limsup_{n\to\infty} \sqrt[n]{a_1 a_2 \cdots a_N (g+\varepsilon)^{-N}} = g+\varepsilon.
\end{align*}
Similarly, we can prove that $g - \varepsilon \leq \liminf_{n\to\infty} \sqrt[n]{a_1 a_2 \cdots a_n}$, and the result follows.
\end{proof}

\begin{remark}
We could only write $\limsup$ and $\liminf$ in the above theorem, since we do not know if the limit exists before we prove it. 
\end{remark}

\medskip

\begin{theorem}
If $a_n > 0$ for all $n$ and $\lim_{n\to\infty} a_{n+1}/a_n = a \in \overline{\mathbb{R}}$, then $\lim_{n\to\infty} \sqrt[n]{a_n} = a$.
\end{theorem}
\begin{proof}
Since $\lim_{n\to\infty} a_{n+1}/a_n = a$, then the following sequence also converges to $a$:
\begin{align*}
    a_1, \frac{a_2}{a_1}, \cdots, \frac{a_n}{a_{n-1}}, \cdots.
\end{align*}
Hence, Theorem \ref{th_27} implies
\begin{align*}
    \lim_{n\to\infty} \sqrt[n]{a_n} & = \lim_{n\to\infty} \sqrt[n]{a_1 \cdot \frac{a_2}{a_1} \cdots \frac{a_n}{a_{n-1}}} = a.
\end{align*}
\end{proof}

\medskip

\begin{theorem}[Stolz]\label{th_29}
Suppose that $\{x_n\}$ and $\{y_n\}$ are two sequences of real numbers that satisfy the following properties:
\begin{enumerate}[label=(\alph*)]
    \item there is $N$ such that $0 < y_n < y_{n+1}$ for all $n \geq N$;  \label{th_29_a}
    
    \item $\lim_{n\to\infty} y_n = \infty$. \label{th_29_b}
\end{enumerate}
If
\begin{align*}
    \lim_{n\to\infty} \frac{x_n - x_{n-1}}{y_n - y_{n-1}} = g \in \overline{\mathbb{R}},
\end{align*}
then 
\begin{align*}
    \lim_{n\to\infty} \frac{x_n}{y_n} = g.
\end{align*}
\end{theorem}
\begin{proof}
First we assume that $g$ is finite, that is $g \in \mathbb{R}$. For every $\varepsilon > 0$, there is an integer $N > 0$ such that for all $n \geq N$, 
\begin{align}\label{th_29_equ1}
    \left|\frac{x_{n+1} - x_n}{y_{n+1} - y_n} - g\right| < \frac{\varepsilon}{2}.
\end{align}
Note that if $b,d > 0$ and $a,b < c < d$, then
\begin{align}\label{th_29_equ2}
    \frac{a}{b} < \frac{a+c}{b+d} < \frac{c}{d}.
\end{align}

Applying \eqref{th_29_equ2} to \eqref{th_29_equ1} with induction implies that for all $n > N$ we have
\begin{align*}
    \frac{x_n - x_N}{y_n - y_N} = \frac{\sum^{n-1}_{i=N} (x_{i+1} - x_i)}{\sum^{n-1}_{i=N} (y_{i+1} - y_i)} \in \left(g - \frac{\varepsilon}{2}, g + \frac{\varepsilon}{2}\right),
\end{align*}
which is equivalent to that for all $n > N$,
\begin{align}
    \left|\frac{x_n - x_N}{y_n - y_N} - g\right| < \frac{\varepsilon}{2}.
\end{align}
Also, 
\begin{align*}
    \frac{x_n}{y_n} - g = \frac{x_N - gy_N}{y_n} + \left(1 - \frac{y_N}{y_n}\right) \left(\frac{x_n - x_N}{y_n - y_N} - g\right).
\end{align*}
Since $\left|1 - y_n/y_n\right| < 1$ for $n > N$, we have
\begin{align}
    \left|\frac{x_n}{y_n} - g\right|  \leq \left|\frac{x_N - gy_N}{y_n}\right| + \left|\frac{x_n - x_N}{y_n - y_N} - g\right|.
\end{align}

Let $N_1 > N$ be such that for all $n > N_1$,
\begin{align}
    \left|\frac{x_N - gy_N}{y_n}\right| < \frac{\varepsilon}{2},
\end{align}
and the existence of $N_1$ follows from \ref{th_29_b}. Hence for all $n > N_1$, we have
\begin{align*}
    \left|\frac{x_n}{y_n} - g\right| \leq \frac{\varepsilon}{2} + \frac{\varepsilon}{2} = \varepsilon.
\end{align*}

Now we consider the case $g = \infty$, then there is an integer $N_2 > 0$ such that for $n \geq N_2$,
\begin{align*}
    x_n - x_{n-1} > y_n - y_{n-1} > 0,
\end{align*}
and hence $\lim_{n\to\infty} x_n = \infty$. Since
\begin{align*}
    \lim_{n\to\infty} \frac{x_n - x_{n-1}}{y_n - y_{n-1}} = \infty,
\end{align*}
we have
\begin{align*}
    \lim_{n\to\infty} \frac{y_n - y_{n-1}}{x_n - x_{n-1}} = 0.
\end{align*}
And the assumptions of Stolz's theorem are satisfied and hence
\begin{align*}
    \lim_{n\to\infty} \frac{y_n}{x_n} = 0,
\end{align*}
and thus 
\begin{align*}
    \lim_{n\to\infty} \frac{x_n}{y_n} = \infty.
\end{align*}
\end{proof}

\medskip

The Stolz's theorem can be useful, and as an example, we use it as a second method to prove Theorem \ref{th_26}.

\medskip

\begin{proof}[Second Proof of Theorem \ref{th_26}]
Let $x_n = a_1 + a_2 + \cdots + a_n$, and $y_n = n$, then
\begin{align*}
    \lim_{n\to\infty} \frac{a_1 + a_2 + \cdots + a_n}{n} = \lim_{n\to\infty} \frac{x_n - x_{n-1}}{y_n - y_{n-1}} = \lim_{n\to\infty} a_n = a.
\end{align*}
\end{proof}

\medskip


\section{Subsequences}

\begin{definition}
Given a sequence $\{x_n\}$, consider a sequence $\{n_k\}$ of positive integers such that $n_1 < n_2 < n_3 < \cdots$, then the sequence $\{x_{n_k}\}$ is called a subsequence of $\{x_n\}$. If $\{x_{n_k}\}$ converges, its limit is called a subsequential limit of $\{x_n\}$.
\end{definition}


\medskip

\begin{theorem}\label{th_210}
~\begin{enumerate}[label=(\alph*)]
    \item If $\{x_n\}$ is a sequence in a compact metric space $X$, then some subsequence of $\{x_n\}$ converges to a point of $X$. \label{th_210_a}
    
    \item Every bounded sequence in $\mathbb{R}^n$ contains a convergent subsequence. \label{th_210_b}
\end{enumerate}
\end{theorem}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item Let $E$ be the range of $\{x_n\}$. If $E$ is finite then there is a $x \in E$ and a sequence $\{n_k\}$ with $n_1 < n_2 < n_3 < \cdots$, such that $x_{n_1} = x_{n_2} = \cdots = x$. The subsequence $\{x_{n_k}\}$ obtained converges to $x$. If $E$ is infinite, Theorem \ref{th_119} implies that $E$ has a limit point $x \in X$. Choose $n_1$ such that $d(x,x_{n_1}) < 1$, and let $n_2 > n_1$ be such that $d(x,x_{n_2}) < 1/2$. Continue this process and we have $\{x_{n_k}\}$ such that $d(x,x_{n_k}) < 1/k$, and clearly $x_{n_k} \to x$.
    
    \item This follows from \ref{th_210_a}, since by Theorem \ref{th_123}, every bounded subset of $\mathbb{R}^n$ belongs to a compact subset of $\mathbb{R}^n$.
\end{enumerate}
\end{proof}

\begin{remark}
If the bounded sequence belongs to $\mathbb{R}$ in \ref{th_210_b}, then the result is well known as Bolzano-Weierstrass theorem.
\end{remark}

\medskip

\begin{theorem}[Bolzano-Weierstrass]
Every bounded sequence of real numbers has a convergent subsequence.
\end{theorem}

\medskip

\begin{theorem}\label{th_212}
The subsequential limits of a sequence $\{x_n\}$ in a metric space $X$ forms a closed subset of $X$.
\end{theorem}
\begin{proof}
Let $E$ be the set of all subsequential limits of $\{x_n\}$ and suppose $x$ is a limit point in $E$. We need to show that $x \in E$.

Choose $n_1$ such that $x_{n_1} \neq X$. If no such point exists, then $E$ has only one point, hence closed. Let $\delta = d(x, x_{n_1})$. Suppose $n_1, \cdots, n_m$ are chosen. Since $x$ is a limit point in $E$, then there is a point $\widetilde{x} \in E$ such that $d(x, \widetilde{x}) < \delta/2^m$. Since $\widetilde{x} \in E$, there exists $n_{m+1} > n_m$ such that $d(\widetilde{x},x_{n_{m+1}}) < \delta/2^m$. Hence,
\begin{align*}
    d(x,x_{n_{m+1}}) \leq \frac{\delta}{2^{m-1}},
\end{align*}
for $m = 1,2,\cdots$, which implies $\{x_{n_k}\}$ converges to $x$. Hence $x \in E$.
\end{proof}


\medskip


\section{Cauchy Sequences}

\begin{definition}
A sequence $\{x_n\}$ in a metric space $X$ is said to be a Cauchy sequence if for every $\varepsilon > 0$, there is an integer $N > 0$ such that $d(x_n, x_m) < \varepsilon$ for all $n, m \geq N$.
\end{definition}

\medskip

\begin{definition}
Let $E$ be a nonempty subset of a metric space $X$, and let $S$ be the set of all real numbers of the the form $d(x,y)$ for $x,y \in E$. Then $\sup S$ is called the diameter of $E$, denoted by $\diam E$.
\end{definition}

\begin{remark}
If $\{x_n\}$ is a sequence in $X$ and if $E_n$ consists of the points $x_n, x_{n+1}, x_{n+2}, \cdots$, it is clear that $\{x_n\}$ is a Cauchy sequence if and only if $\lim_{n\to\infty} \diam E_n = 0$.
\end{remark}

\medskip

\begin{theorem}\label{th_213}
~\begin{enumerate}[label=(\alph*)]
    \item If $\overline{E}$ is the closure of a set $E \subset X$, then $\diam \overline{E} = \diam E$. \label{th_213_a}
    
    \item If $K_n$ is a sequence of compact sets in $X$ such that $K_n \supset K_{n+1}$ for $n = 1,2,3,\cdots$ and if
    \begin{align*}
        \lim_{n\to\infty} \diam K_n = 0,
    \end{align*}
    then $\bigcap^\infty_{n=1} K_n$ consists of exactly one point. \label{th_213_b}
\end{enumerate}
\end{theorem}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item Since $E \subset \overline{E}$, then $\diam E \leq \diam \overline{E}$. For every $\varepsilon > 0$, let $x,y \in \overline{E}$, and there are $x',y' \in E$ such that $d(x,x') < \varepsilon$, $d(y,y') < \varepsilon$. Hence, 
    \begin{align*}
        d(x,y) \leq d(x,x') + d(x',y') + d(y',y) \leq d(x',y') + 2 \varepsilon,
    \end{align*}
    and it follows that $\diam \overline{E} \leq \diam E + 2 \varepsilon$. Since $\varepsilon$ is arbitrary, $\diam \overline{E} \leq \diam E$. Thus $\diam \overline{E} = \diam E$.
    
    \item Let $K = \bigcap^\infty_{n=1} K_n$ and by Theorem \ref{th_118}, $K$ is not empty. Suppose that $K$ contains more than one point, then $\diam K > 0$. However, for each $n$, $K_n \supset K$ so that $\diam K_n \geq \diam K$, which is a contradiction to $\lim_{n\to\infty} \diam K_n = 0$. 
\end{enumerate}
\end{proof}

\medskip

\begin{theorem}\label{th_214}
~\begin{enumerate}[label=(\alph*)]
    \item In any metric space $X$, every convergent sequence is a Cauchy sequence. \label{th_214_a}
    
    \item If $X$ is a compact metric space and if $\{x_n\}$ is a Cauchy sequence in $X$, then $\{x_n\}$ converges to some point of $X$. \label{th_214_b}
    
    \item In $\mathbb{R}^n$, every Cauchy sequence converges. \label{th_214_c}
\end{enumerate}
\end{theorem}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item If $x_n \to x$ and for every $\varepsilon > 0$, there is an integer $N > 0$ such that $d(x,x_n) < \varepsilon$ for all $n \geq N$. Hence, 
    \begin{align*}
        d(x_n,x_m) \leq d(x_n,x) + d(x_m,x) < 2 \varepsilon,
    \end{align*}
    for $n,m \geq N$. Hence $\{x_n\}$ is a Cauchy sequence.
    
    \item Let $\{x_n\}$ be a Cauchy sequence in $X$. Let $E_n = \{x_n, x_{n+1}, \cdots\}$ for $n = 1,2,3,\cdots$. By Theorem \ref{th_213} \ref{th_213_b}, \begin{align}\label{th_214_equ1}
        \lim_{n\to\infty} \diam \overline{E}_n = 0.
    \end{align}
    Note that each $\overline{E}_n$ is compact by Theorem \ref{th_116} and also $\overline{E}_n \supset \overline{E}_{n+1}$. Also, Theorem \ref{th_213} \ref{th_213_b} implies there is a unique point $x \in X$ such that $x = \bigcap^\infty_{n=1} E_n$. 
    
    For every $\varepsilon > 0$, \eqref{th_214_equ1} implies that there is an integer $N > 0$ such that $\diam \overline{E}_n < \varepsilon$ for all $n \geq N$. Since $x \in \overline{E}_n$, it follows that $d(x,y) < \varepsilon$ for every $y \in \overline{E}_n$. Hence $d(x,x_n) < \varepsilon$ for all $n \geq N$, and this is precisely that $x_n \to x$. 
    
    \item Let $\{x_n\}$ be a Cauchy sequence in $\mathbb{R}^n$. Define $E_n$ as above. Then for $\varepsilon = 1$, there is an integer $N > 0$ such that $\diam \overline{E}_N < 1$. The range of $\{x_n\}$ is the union of $E_N$ and the finite set $\{x_1, \cdots, x_{N-1}\}$, and hence $\{x_n\}$ is bounded. Also, then range of $\{x_n\}$ is closed and hence compact, then \ref{th_214_c} follows from  \ref{th_214_b}.
\end{enumerate}
\end{proof}

\medskip

\begin{definition}
A metric space $X$ in which every Cauchy sequence converges is said to be complete.
\end{definition}

\medskip

\begin{remark}
Being complete means every Cauchy sequence in $X$ also converges in $X$. Hence, Theorem \ref{th_214} implies that all compact metric and all Euclidean spaces (including $\mathbb{R}^n$) are complete. 
\end{remark}

\medskip

\begin{definition}
A sequence $\{x_n\}$ of real numbers is said to be
\begin{enumerate}[label=(\alph*)]
    \item monotonically increasing if $x_n \leq x_{n+1}$ for $n = 1,2,3,\cdots$;
    
    \item monotonically decreasing if $x_n \geq x_{n+1}$ for $n = 1,2,3,\cdots$.
\end{enumerate}
\end{definition}

\medskip

\begin{theorem}\label{th_215}
Suppose $\{x_n\}$ is monotonic. Then $\{x_n\}$ converges if and only if it is bounded.
\end{theorem}
\begin{proof}
Suppose $x_n \leq x_{n+1}$. Let $E$ be the range of $\{x_n\}$. If $\{x_n\}$ is bounded, let $s = \sup E$, then $s_n \leq s$ for all $n = 1,2,3,\cdots$. For every $\varepsilon > 0$, there is an integer $N > 0$ such that $s - \varepsilon < x_N < s$, for otherwise $s - \varepsilon$ would be the least upper bound. Since $\{x_n\}$ increases, for all $n \geq N$, $s - \varepsilon < x_n < s$, which shows that $\{x_n\}$ converges.

The converse follows from Theorem \ref{th_21} \ref{th_21_c}.
\end{proof}


\medskip



\section{Upper and Lower Limits}

\begin{definition}\label{def_28}
Let $\{x_n\}$ be a sequence of real numbers. Let $E$ be the set of all $x \in \overline{\mathbb{R}}$ such that $x_{n_k} \to x$ for some subsequence $\{x_{n_k}\}$. The set contains all subsequential limits, plus possibly the numbers $+\infty$, $-\infty$. Let
\begin{align*}
    x^* = \sup E, \qquad x_* = \inf E.
\end{align*}
The numbers $x^*$ and $x_*$ are called the upper and lower limits of $\{x_n\}$, we use the notation
\begin{align*}
    \limsup_{n\to\infty} x_n = x^*, \qquad \liminf_{n\to\infty} x_n = x_*.
\end{align*}
\end{definition}

\begin{remark}
The upper limit $\limsup_{n\to\infty} x_n$ is the largest possible limit (including $+\infty$) of any subsequence of $\{x_n\}$, and the lower limit $\liminf_{n\to\infty} x_n$ is the smallest possible limit (including $-\infty$). Hence we could also define $\limsup$ and $\liminf$ as below:
\begin{enumerate}[label=(\alph*)]
    \item If $\{x_n\}$ is bounded from above, then \begin{align*}
        \limsup_{n\to\infty} x_n \coloneqq \inf_{n \in \mathbb{N}} \sup_{k \geq n} x_k,
    \end{align*}
    and this number exists since $\{x_n\}$ is bounded from above. If $\{x_n\}$ is not bounded above, then $\limsup x_n \coloneqq +\infty$.
    
    \item If $\{x_n\}$ is bounded from below, then \begin{align*}
        \liminf_{n\to\infty} x_n \coloneqq \sup_{n \in \mathbb{N}} \inf_{k \geq n} x_k.
    \end{align*}
    If $\{x_n\}$ is not bounded from below, then $\liminf x_n \coloneqq -\infty$.
\end{enumerate}
\end{remark}

\medskip

\begin{lemma}
Let $\{x_n\} \subset \mathbb{R}$ be a sequence.
\begin{enumerate}[label=(\alph*)]
    \item If $\sup_{k \geq n} x_k$ exists, then
    \begin{align*}
        \limsup_{n\to\infty} x_n = \lim_{n\to\infty} \sup_{k \geq n} x_k.
    \end{align*}
    
    \item If $\inf_{k \geq n} x_k$ exists, then
    \begin{align*}
        \liminf_{n\to\infty} x_n = \lim_{n\to\infty} \inf_{k \geq n} x_k.
    \end{align*}
\end{enumerate}
\end{lemma}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item Let $a_n = \sup_{k\geq n}x_k$. If $\{a_n\}$ is not bounded from above, then $\{x_n\}$ is not bounded above, hence $\lim a_n = \limsup x_n = + \infty$. 
    
    If $\{a_n\}$ is bounded from above, then it is a bounded and monotonically decreasing sequence. By Theorem \ref{th_215}, $\{a_n\}$ is convergent and
    \begin{align*}
        \lim_{n\to\infty} a_n = \inf_{n \in \mathbb{N}} \sup_{k \geq n} x_k = \limsup_{n\to\infty} x_n.
    \end{align*}
    
    \item By the similar argument as above.
\end{enumerate}
\end{proof}

\medskip

\begin{theorem}\label{th_216}
Let $\{x_n\} \subset \mathbb{R}$ be a sequence. Let $E$ and $x^*$ be the same as in Definition \ref{def_28}. Then $X^*$ has the following properties:
\begin{enumerate}[label=(\alph*)]
    \item $x^* \in E$. \label{th_216_a}
    
    \item If $x > x^*$, there is an integer $N$ such that $x_n < x$ for $n \geq N$. \label{th_216_b}
\end{enumerate}
Moreover, $x^*$ is the only number which satisfies \ref{th_216_a} and \ref{th_216_b}. An analogous result is true for $x_*$.
\end{theorem}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item If $x^* = + \infty$, then $E$ is not bounded and hence $\{x_n\}$ is not bounded and there is a subsequence $\{x_{n_k}\}$ such that $x_{n_k} \to + \infty$. If $x^*$ is finite, then $E$ is bounded above, and hence at least one subsequential limit exists. \ref{th_216_a} follows from Theorem \ref{th_212} and \ref{th_113}. If $x^* = - \infty$, then $E$ only contains one element $- \infty$, and there is no subsequential limit. Hence for any $M$, $x_n > M$ for at most a finite number of values of $n$, so $x_n \to - \infty$.
    
    \item Suppose there is a number $x > x^*$ such that $x_n \geq x$ for infinitely many $n$, then there is $y \in E$ such that $y \geq x > x^*$, contradiction.
\end{enumerate}
To show the uniqueness, suppose there are two numbers $x_1$ and $x_2$ which satisfy \ref{th_216_a} and \ref{th_216_b}, and suppose $x_1 < x_2$. Choose $x$ such that $x_1 < x < x_2$. Since $x_1$ satisfies \ref{th_216_b}, we have $x_n < x$ for all $n \geq N$, hence $x_2$ cannot satisfy \ref{th_216_a}.
\end{proof}

\medskip

We will also present another proof of part \ref{th_216_a} when $\{x_n\}$ is bounded in the above theorem.

\medskip

\begin{proof}[Second Proof of \ref{th_216_a}]
Let $x^* = \sup E$. It suffices to show that there is a subsequence converging to $x^*$. 

Let $k \in \mathbb{N}$. Since $x - 1/k < x^*$, there is a subsequence with limit larger than $x^* - 1/k$, in particular, infinitely many $x_n$ are larger than $x^* - 1/k$. Hence there is $n_1$ such that $x_{n_1} > x^* - 1$, and there is $n_2 > n_1$ such that $x_{n_2} > x^* - 1/2$. Continue this process and we can have a subsequence $\{x_{n_k}\}$ such that $x_{n_k} > x^* - 1/k$ for $k = 1,2,3,\cdots$. Also, $\{x_{n_k}\}$ is bounded, os it has a convergent subsequence $\{x_{n_{k_l}}\}$. Clearly, $\lim_{l\to\infty} x_{n_{k_l}} \leq x^*$. 

Note that $k_l \geq l$, so for $l \geq i$
\begin{align*}
    x_{n_{k_l}} \geq x^* - \frac{1}{k_l} \geq x^* - \frac{1}{l} \geq x^* - \frac{1}{i},
\end{align*}
and hence
\begin{align*}
    x^* \geq \lim_{l\to\infty} x_{n_{k_l}} \geq x^* - \frac{1}{i},
\end{align*}
letting $i \to \infty$ implies $\lim_{l\to\infty} x_{n_{k_l}} = x^*$.
\end{proof}



\medskip


\section{Series}

\begin{definition}
Given a sequence $\{x_n\}$, we use the notation
\begin{align*}
    \sum^m_{n=k} x_n
\end{align*}
to denote the sum $x_k + a_{k+1} + \cdots + a_m$. With $\{x_n\}$ we associate a sequence $\{s_n\}$, which is defined as
\begin{align*}
    s_n = \sum^n_{k=1} x_k.
\end{align*}
For $\{s_n\}$ we also use the symbolic expression 
\begin{align}\label{sec_25_equ1}
    \sum^\infty_{n=1} x_n
\end{align}
to denote $x_1 + x_2 + x_3 + \cdots$.
\end{definition}

The symbol \eqref{sec_25_equ1} is called an {\em infinite series}, or just a {\em series}. $s_n$ are called the {\em partial sums} of the series. If $\{s_n\}$ converges to $x$, then we say the series converges, and write
\begin{align*}
    \sum^\infty_{n=1} x_n = x.
\end{align*}

\medskip

\begin{theorem}
Given a sequence $\{x_n\}$, $\sum x_n$ converges if and only if for every $\varepsilon > 0$ there is an integer $N > 0$ such that if $m \geq n \geq N$,
\begin{align*}
    \left|\sum^m_{k=n} x_k\right| \leq \varepsilon.
\end{align*}
This is called the Cauchy criterion. In particular, letting $m = n$ implies that for $n \geq N$, $\left|x_n\right| \leq \varepsilon$. In other words:
\end{theorem}

\medskip

\begin{theorem}\label{th_218}
If $\sum x_n$ converges, then $\lim_{n\to\infty} x_n = 0$.
\end{theorem}

\medskip

\begin{remark}
In general, the condition $\lim_{n\to\infty} x_n = 0$ is not sufficient to ensure the convergence of $\sum x_n$. For example, the series
\begin{align*}
    \sum^\infty_{n=1} \frac{1}{n}
\end{align*}
diverges, which we will prove it later. 
\end{remark}

\medskip

\begin{theorem}\label{th_219}
If $x_n \geq 0, n = 1,2,3,\cdots$, then $\sum x_n$ converges if and only if the sequence of partial sums is bounded. 
\end{theorem}
\begin{proof}
$x_n \geq 0$ implies that $s_n = \sum^n_{k=1} x_k$ is an increasing sequence, and hence $\{s_n\}$ converges if and only if it is bounded.
\end{proof}

\medskip

\begin{theorem}[Comparison Test]\label{th_220}
~\begin{enumerate}[label=(\alph*)]
    \item If $\left|x_n\right| \leq c_n$ for all $n > N$, where $N$ is some fixed integer, and if $\sum c_n$ converges, then $\sum x_n$ converges. \label{th_220_a}
    
    \item If $x_n \geq d_n \geq 0$ for all $n \geq N$, and if $\sum d_n$ diverges, then $\sum x_n$ diverges. \label{th_220_b}
\end{enumerate}
\end{theorem}
\begin{proof}
Part \ref{th_220_b} is obvious, and we only prove part \ref{th_220_a}. Given $\varepsilon > 0$, there is an integer $\widetilde{N} \geq N$ such that for all $m \geq n \geq \widetilde{N}$, 
\begin{align*}
    \sum^m_{k=n} c_k < \varepsilon,
\end{align*}
hence
\begin{align*}
    \left|\sum^m_{k=n} x_k\right| \leq \sum^m_{k=n} \left|x_k\right| \leq \sum^m_{k=n} c_k < \varepsilon,
\end{align*}
which implies that $\{x_n\}$ converges.
\end{proof}

\medskip



\section{Series of Nonnegative Terms}

\begin{theorem}[Geometric series]\label{th_221}
If $0 \leq x < 1$, then
\begin{align*}
    \sum^\infty_{n=0} x^n = \frac{1}{1 - x}.
\end{align*}
If $x > 1$, the series diverges.
\end{theorem}
\begin{proof}
If $0 \leq x < 1$, we have
\begin{align*}
    s_n \coloneqq \sum^n_{k=1} x^k = \frac{1 - x^{n+1}}{1 - x},
\end{align*}
and hence letting $n \to \infty$ implies the result. For $x = 1$, we have $\sum^\infty_{n=0} x^n = 1 + 1 + 1 + \cdots$, which diverges. 
\end{proof}

\medskip

\begin{theorem}[Cauchy Condensation Test]\label{th_222}
Suppose $a_1 \geq a_2 \geq a_3 \geq \cdots \geq 0$. Then the series $\sum^\infty_{n=0} a_n$ converges if and only if the series $\sum^\infty_{n=0} 2^n a_{2^n}$ converges.
\end{theorem}
\begin{proof}
By Theorem \ref{th_219}, it suffices to show the boundness of the partial sums. Let
\begin{align*}
    s_n & = a_1 + a_2 + \cdots + a_n, \\
    t_k & = a_1 + 2a_2 + \cdots + 2^k a_{2^k}.
\end{align*}
For $n < 2^k$, we have
\begin{align}\label{th_222_equ1}
    s_n & \leq a_1 + (a_2 + a_3) + \cdots + (a_{2^k} + \cdots + a_{2^{k+1}-1}) \\
    & \leq a_1 + 2a_2 + \cdots + 2^k a_{2^k} = t_k,
\end{align}
and hence $s_n \leq t_k$. 

On the other hand, if $n > 2^k$,
\begin{align}\label{th_222_equ2}
    s_n & \geq a_1 + a_2 + (a_3 + a_4) + \cdots + (a_{2^{k-1}+1} + \cdots a_{2^k}) \\
    & \geq \frac{1}{2}a_1 + a_2 + 2 a_4 + \cdots + 2^{k-1} a_{2^k} = \frac{1}{2} t_k.
\end{align}
Hence \eqref{th_222_equ1} and \eqref{th_222_equ2} implies that $\{s_n\}$ and $\{t_k\}$ are either both bounded or both unbounded. This completes the proof.
\end{proof}

\medskip

\begin{theorem}[$p$-Series Test]\label{th_223}
$\displaystyle \sum \frac{1}{n^p}$ converges if $p > 1$ and diverges if $p \leq 1$.
\end{theorem}
\begin{proof}
If $p \leq 1$, then $1/n^p \geq 1/n$, and since $\sum 1/n$ diverges, $\sum 1/n^p$ also diverges. For $p > 1$, we apply the Cauchy Condensation Test, and let $a_n = 1/n^p$, then
\begin{align*}
    \sum^\infty_{k=0} 2^k \frac{1}{2^{kp}} = \sum^\infty_{k=0} 2^{(1-p)k}.
\end{align*}
Now $2^{1-p} < 1$ if and only if $1 - p < 0$, and the result follows by comparison with the geometric series (take $x = 2^{1-p}$ in Theorem \ref{th_221}).
\end{proof}

\medskip

\begin{theorem}
If $p > 1$, the series
\begin{align*}
    \sum^\infty_{n=2} \frac{1}{n \left(\log n\right)^p} 
\end{align*}
converges and if $p \leq 1$, the series diverges.
\end{theorem}
\begin{proof}
Let $a_n = 1/\left(n \left(\log n\right)^p\right)$, then
\begin{align}\label{th_224_equ1}
    \sum^\infty_{n=2} 2^n a_{2^n} = \left(\frac{1}{\log 2}\right)^p \sum^\infty_{n=2} \frac{1}{n^p},
\end{align}
and by Theorem \ref{th_223}, \eqref{th_224_equ1} converges if and only if $p > 1$ and then the theorem follows from the Cauchy Condensation Test in Theorem \ref{th_222}.
\end{proof}

\medskip



\section{Natural Number $e$ and Natural Logarithm}

\begin{theorem}
The sequence $a_n = \left(1 + 1/n\right)^n$ is strictly increasing and the sequence $b_n = \left(1 + 1/n\right)^{n+1}$ is strictly decreasing, and both sequences converges to the same limit. We denote this limit by
\begin{align*}
    e = \lim_{n\to\infty} \left(1 + \frac{1}{n}\right)^n = \lim_{n\to\infty} \left(1 + \frac{1}{n}\right)^{n+1}.
\end{align*}
\end{theorem}
\begin{proof}
First we prove that $a_n$ is strictly increasing, and by the Bernoulli formula, we have
\begin{align*}
    \frac{a_{n+1}}{a_n} & = \frac{\left(\frac{n + 2}{n + 1}\right)^{n + 1}}{\left(\frac{n + 1}{n}\right)^{n}}  = \left(\frac{n^2 + 2n}{n^2 + 2n + 1}\right)^n \frac{n + 2}{n + 1} \\
    & = \left(1 - \frac{1}{n^2 + 2n + 1}\right)^n \frac{n + 2}{n + 1} \\
    & \geq \left(1 - \frac{n}{n^2 + 2n + 1}\right) \frac{n + 2}{n + 1} \\
    & = \frac{n^3 + 3n^2 + 3n + 2}{n^3 + 3n^2 + 3n + 1} > 1.
\end{align*}
And similarly, we can show $b_n$ is decreasing. Since $a_n \leq b_n$ for all $n$, we have
\begin{align*}
    2 = a_1 < a_2 < \cdots < a_n < \cdots < b_n < \cdots < b_2 < b_1 = 4.
\end{align*}
Hence $\{a_n\}$ is increasing and bounded from above, so convergent, and also similar for $\{b_n\}$. Thus, 
\begin{align*}
    \frac{\lim_{n\to\infty} b_n}{\lim_{n\to\infty} a_n} = \lim_{n\to\infty} \frac{b_n}{a_n} = \lim_{n\to\infty} 1 + \frac{1}{n} = 1,
\end{align*}
which implies
\begin{align*}
    \lim_{n\to\infty} a_n = \lim_{n\to\infty} b_n.
\end{align*}
\end{proof}

\begin{remark}
Since $e$ is the common limit of $\{a_n\}$ and $\{b_n\}$, we have
\begin{align}\label{remark_27_equ1}
    \left(1 + \frac{1}{n}\right)^n < e < \left(1 + \frac{1}{n}\right)^{n+1}.
\end{align}
\end{remark}

\medskip

\begin{theorem}
$\displaystyle e = \sum^\infty_{n=0} \frac{1}{n!}$.
\end{theorem}
\begin{proof}
With $0! = 1$, let
\begin{align*}
    x_n = \left(1 + \frac{1}{n}\right)^n, \quad y_n = \sum^n_{k=0} \frac{1}{k!}.
\end{align*}
And the binomial formula yields
\begin{align*}
    x_n & = 1^n + \binom{n}{1} \frac{1}{n} + \binom{n}{2} \frac{1}{n^2} + \cdots + \binom{n}{n-1} \frac{1}{n^{n-1}} + \frac{1}{n^n} \\
    & = 1 + 1 + \frac{n(n-1)}{2!} \frac{1}{n^2} + \cdots + \frac{n!}{k!(n-k)!} \frac{1}{n^k} + \cdots + \frac{1}{n^n} \\
    & = 1 + 1 + \frac{n-1}{n} \frac{1}{2!} + \frac{(n-1)(n-2)}{n^2} \frac{1}{3!} + \cdots + \frac{(n-1)!}{n^{n-1}} \frac{1}{n!} \leq y_n.
\end{align*}
On the other hand, for $n \geq k$ we have
\begin{align*}
    x_n \geq 1 + 1 + \frac{n-1}{n} \frac{1}{2!} + \cdots + \frac{(n-1)(n-2)\cdots(n-k+1)}{n^k} \frac{1}{k!},
\end{align*}
with this fixed $k$, letting $n\to\infty$ on both sides yields
\begin{align*}
    e = \lim_{n\to\infty} x_n \geq 1 + 1 + \frac{1}{2!} + \frac{1}{3!} + \cdots + \frac{1}{k!} = y_k.
\end{align*}
Hence $e \leftarrow x_n \leq y_n \leq e$, which implies
\begin{align*}
    \sum^\infty_{n=0} \frac{1}{n!} = \lim_{n\to\infty} y_n = e.
\end{align*}
\end{proof}

\medskip

\begin{definition}
The natural logarithm is defined by
\begin{align*}
    \ln x = \log x = \log_e x.
\end{align*}
\end{definition}

\medskip

\begin{lemma}
$\displaystyle \frac{1}{n + 1} < \ln \left(1 + \frac{1}{n}\right) < \frac{1}{n}$ for $n = 1,2,3,\cdots$.
\end{lemma}
\begin{proof}
Taking $\ln$ function on both sides of the inequality \eqref{remark_27_equ1} implies 
\begin{align*}
    n \ln\left(1 + \frac{1}{n}\right) < 1 < (n+1) \ln \left(1 + \frac{1}{n}\right),
\end{align*}
and the lemma follows.
\end{proof}

\medskip



\section{The Root and Ratio Test}

\begin{definition}
We say a series $\sum^\infty_{n=1} a_n$ is absolutely convergent if $\sum^\infty_{n=1} \left|a_n\right|$ converges.
\end{definition}

\medskip

\begin{theorem}
If $\sum^\infty_{n=1} a_n$ is absolutely convergent, then it is convergent.
\end{theorem}
\begin{proof}
Clearly $\left|a_n\right| \leq \left|a_n\right|$ and by the Comparison Test (Theorem \ref{th_220}), the theorem follows.
\end{proof}

\medskip

Now we talk about two useful theorems that can be used to determine if the series converges, {\em Root Test} (or {\em Cauchy Test}) and {\em Ratio Test} (or {\em d'Alembert Test}).

\medskip

\begin{theorem}[Root Test]
Given $\sum^\infty_{n=1} a_n$.
\begin{enumerate}[label=(\alph*)]
    \item If $\limsup_{n\to\infty}\displaystyle  \sqrt[n]{\left|a_n\right|} < 1$, then $\sum^\infty_{n=1} a_n$ converges absolutely.
    
    \item If $\limsup_{n\to\infty}\displaystyle  \sqrt[n]{\left|a_n\right|} > 1$, then $\sum^\infty_{n=1} a_n$ diverges.
\end{enumerate}
\end{theorem}
\begin{remark}
If $\limsup_{n\to\infty}\displaystyle  \sqrt[n]{\left|a_n\right|} = 1$, the test gives no information. For example, consider the series
\begin{align}\label{remark_28_equ1}
    \sum^\infty_{n=1} \frac{1}{n}, \quad \sum^\infty_{n=1} \frac{1}{n^2}.
\end{align}
For each of these sequences $ \limsup_{n\to\infty} \displaystyle\sqrt[n]{\left|a_n\right|} = 1$, but the first diverges and the second converges.
\end{remark}
\begin{proof}
Let $\alpha = \limsup_{n\to\infty}\displaystyle  \sqrt[n]{\left|a_n\right|}$. If $\alpha < 1$, then there is $\beta$ such that $\alpha < \beta < 1$. Then by Theorem \ref{th_216} \ref{th_216_b}, there is an integer $N > 0$ such that for all $n \geq N$, 
\begin{align*}
    \sqrt[n]{\left|a_n\right|} < \beta,
\end{align*}
which is equivalent to that $\left|a_n\right| < \beta^n$ for $n \geq N$. By Theorem \ref{th_221}, $\sum \beta^n$ converges. And the absolute convergence of $\sum a_n$ follows from the Comparison Test. 

If $\alpha > 1$, then again by Theorem \ref{th_216}, there is a subsequence $\{a_{n_k}\}$ such that 
\begin{align*}
    \sqrt[n_k]{a_{n_k}} \to \alpha,
\end{align*}
and hence $\left|a_n\right| > 1$ for infinitely many numbers of $n$, so the condition $a_n \to 0$, necessary for convergence of $\sum a_n$, does not hold (Theorem \ref{th_218}).
\end{proof}

\medskip

\begin{theorem}[Ratio Test]\label{th_229}
Given $\sum^\infty_{n=1} a_n$.
\begin{enumerate}[label=(\alph*)]
    \item If $\lim_{n\to\infty}\displaystyle  \left|\frac{a_{n+1}}{a_n}\right| < 1$, then $\sum^\infty_{n=1} a_n$ converges absolutely.
    
    \item If $\lim_{n\to\infty}\displaystyle  \left|\frac{a_{n+1}}{a_n}\right| > 1$, then $\sum^\infty_{n=1} a_n$ diverges.
\end{enumerate}
\end{theorem}

\begin{remark}
If $\lim_{n\to\infty}\displaystyle  \left|\frac{a_{n+1}}{a_n}\right| = 1$, the test gives no information, and the series in \eqref{remark_28_equ1}  this.
\end{remark}

\begin{proof}
Let $\alpha = \lim_{n\to\infty} \left|a_{n+1}/a_n\right|$. If $\alpha < 1$, then for $0 < \varepsilon < 1 - a$, there is an integer $N > 0$ such that for all $n \geq N$, 
\begin{align*}
    \left|\frac{a_{n+1}}{a_n}\right| < a + \varepsilon,
\end{align*}
then there is $\beta$ such that $a + \varepsilon < \beta < 1$, and hence for all $n \geq N$,
\begin{align*}
    \left|\frac{a_{n+1}}{a_n}\right| < \beta.
\end{align*}
In particular, $\left|a_{n}\right| < \left|a_N\right| \beta^{-N} \cdot \beta^n$ for $n \geq N$. Since $\sum \beta^n$ converges,  and $\sum a_n$ also converges absolutely by the Comparison Test.

If $\alpha > 1$, then there is an integer $N > 0$ such that for all $n \geq N$, $\left|a_{n+1}\right| > \left|a_n\right|$ and hence the condition $a_n \to 0$ does not hold. 
\end{proof}

\medskip

\begin{theorem}
For any sequence $\{a_n\}$ of positive numbers,
\begin{align*}
    \liminf_{n\to\infty} \frac{a_{n+1}}{a_n} & \leq \liminf_{n\to\infty} \sqrt[n]{a_n}, \\
    \limsup_{n\to\infty} \sqrt[n]{a_n} & \leq \limsup_{n\to\infty} \frac{a_{n+1}}{a_n}.
\end{align*}
\end{theorem}
\begin{proof}
For the second inequality, let $\alpha = \limsup_{n\to\infty} a_{n+1}/a_n$. If $\alpha = + \infty$, we are done. If $\alpha < \infty$, let $\beta > \alpha$. There is an integer $N > 0$ such that for all $n \geq N$,
\begin{align*}
    \frac{a_{n+1}}{a_n} \leq \beta.
\end{align*}
For $n \geq N$, we have $a_n \leq a_N \beta^{-N} \beta^n$, and hence $\sqrt[n]{a_n} \leq \sqrt[n]{a_N \beta^{-N}} \beta$. Since $a_N \beta^{-N} > 0$, we have
\begin{align*}
    \limsup_{n\to\infty} \sqrt[n]{a_n} \leq \limsup_{n\to\infty} \sqrt[n]{a_N \beta^{-N}} \beta = \beta,
\end{align*}
and this holds for all $\beta \geq \alpha$, thus 
\begin{align*}
    \limsup_{n\to\infty} \sqrt[n]{a_n} \leq \alpha.
\end{align*}

The proof of the first inequality is similar.
\end{proof}

\begin{remark}
This theorem shows that the Ratio Test is easier to apply than the Root Test, since it is usually easier to compute ratio than $n$th roots. However, the Ratio Test has wider application, i.e., when the Ratio Test shows convergence, the Root Test does too, and when the Root Test is inconclusive, the Ratio Test is too. We show some examples below to see this explicitly. 
\end{remark}

\medskip

\begin{example}
~\begin{enumerate}[label=(\alph*)]
    \item Consider the series
    \begin{align*}
        \sum^\infty_{n=1} a_n = \frac{1}{2} + \frac{1}{2} + \frac{1}{2^2} + \frac{1}{3^2} + \frac{1}{2^3} + \frac{1}{3^3} + \frac{1}{2^4} + \frac{1}{3^4} + \cdots,
    \end{align*}
    and we have
    \begin{align*}
        \liminf_{n\to\infty} \frac{a_{n+1}}{a_n} = \lim_{n\to\infty} \frac{2^n}{3^n} = 0, \quad & \liminf_{n\to\infty} \sqrt[n]{a_n} = \lim_{n\to\infty} \sqrt[2n]{\frac{1}{3^n}} = \frac{1}{\sqrt{3}}, \\
        \limsup_{n\to\infty} \frac{a_{n+1}}{a_n} = \lim_{n\to\infty} \frac{3^n}{2^{n+1}} = + \infty, \quad & \limsup_{n\to\infty} \sqrt[n]{a_n} = \lim_{n\to\infty} \sqrt[2n]{\frac{1}{2^n}} = \frac{1}{\sqrt{2}}.
    \end{align*}
    The Root Test shoes convergence, however, the Ratio Test does not apply here.
    
    \item The same it true for the series
    \begin{align*}
        \sum^\infty_{n=1} a_n = \frac{1}{2} + 1 + \frac{1}{8} + \frac{1}{4} + \frac{1}{32} + \frac{1}{16} + \frac{1}{128} + \frac{1}{64} + \cdots,
    \end{align*}
    and we have
    \begin{align*}
        \liminf_{n\to\infty} \frac{a_{n+1}}{a_n} = \frac{1}{8}, \quad \limsup_{n\to\infty} \frac{a_{n+1}}{a_n} = 2,
    \end{align*}
    but we also have
    \begin{align*}
        \limsup_{n\to\infty} \sqrt[n]{a_n} = \lim_{n\to\infty} \sqrt[n]{\frac{1}{2^n}} = \frac{1}{2}, \quad \liminf_{n\to\infty} \sqrt[n]{a_n} = \lim_{n\to\infty} \sqrt[n+2]{\frac{1}{2^n}} = \frac{1}{2}.
    \end{align*}
\end{enumerate}
\end{example}

\medskip

\begin{theorem}
Let $\{a_n\}, \{b_n\}$ be two sequences such that $a_n, b_n > 0$, and there is an integer $N > 0$ such that for all $n \geq N$,
\begin{align*}
    \frac{a_{n+1}}{a_n} \leq  \frac{b_{n+1}}{b_n}.
\end{align*}
If $\sum^\infty_{n=1} b_n$ converges, then $\sum^\infty_{n=1} a_n$ converges.
\end{theorem}
\begin{proof}
Let $c_n = a_n/b_n$, then for all $n \geq N$,
\begin{align*}
    c_{n+1} = \frac{a_{n+1}}{b_{n+1}} \leq \frac{a_n}{b_n} = c_n,
\end{align*}
which implies $\{c_n\}$ is decreasing starting from $n = N$. Hence $\{c_n\}$ is bounded, that is there is $M$ such that $c_n \leq M$ for all $n$. Hence $a_n = c_n b_n \leq M b_n$. Since $\sum^\infty_{n=1} Mb_n = M \sum^\infty_{n=1} b_n$ converges, by the Comparison Test, $\sum^\infty_{n=1} a_n$ converges too.
\end{proof}

\medskip


\section{Power Series}

\begin{definition}\label{def_212}
Given a sequence $\{c_n\}$ of complex numbers, the series
\begin{align*}
    \sum^\infty_{n=0} c_n z^n
\end{align*}
is called a power series. The numbers $c_n$ are called the coefficients of the series, $z$ is a complex number.
\end{definition}

\medskip

\begin{theorem}[Cauchy-Hadamard]\label{th_232}
Given $\sum^\infty_{n=0} c_n z^n$, let
\begin{align*}
    \alpha = \limsup_{n\to\infty} \sqrt[n]{\left|c_n\right|}, \quad R = \frac{1}{\alpha}.
\end{align*}
If $\alpha = 0, R = + \infty$, if $\alpha = + \infty, R = 0$. Then $\sum^\infty_{n=0} c_n z^n$ converges if $\left|z\right| < R$ and diverges if $\left|z\right| > R$.
\end{theorem}
\begin{proof}
Let $a_n = c_n z^n$, and applying the Root Test yields
\begin{align*}
    \limsup_{n\to\infty} \sqrt[n]{\left|a_n\right|} = \left|z\right| \cdot \limsup_{n\to\infty} \sqrt[n]{\left|c_n\right|} = \frac{\left|z\right|}{R}.
\end{align*}
Hence $\left|z\right|/R < 1$ implies the convergence and $\left|z\right|/R > 1$ implies the divergence.
\end{proof}

\begin{remark}
$R$ is called the radius of convergence of $\sum^\infty_{n=0} c_n z^n$.
\end{remark}

\medskip


\section{Summation by Parts}

\begin{theorem}
Given two sequences $\{a_n\}, \{b_n\}$, let
\begin{align*}
    A_n = \sum^n_{k=0} a_k,
\end{align*}
if $n \geq 0$. Also let $A_{-1} = 0$. Then, if $0 \leq m \leq n$, we have
\begin{align}\label{th_233_equ1}
    \sum^n_{k=m} a_k b_k = \sum^{n-1}_{k=m} A_k(b_k - b_{k+1}) + A_n b_n - A_{m-1} b_m.
\end{align}
\end{theorem}
\begin{proof}
\begin{align*}
    \sum^n_{k=m} a_k b_k = \sum^n_{k=m} (A_k - A_{k-1}) b_k = \sum^n_{k=m} A_k b_k - \sum^{n-1}_{k=m-1} A_k b_{k+1},
\end{align*}
and the last term is clearly equal to the right hand side of \eqref{th_233_equ1}.
\end{proof}

\medskip

\begin{theorem}\label{th_234}
Suppose
\begin{enumerate}[label=(\alph*)]
    \item the partial sum $A_n$ of $\sum^\infty_{n=0} a_n$ form a bounded sequence;
    
    \item $b_0 \geq b_1 \geq b_2 \geq \cdots$;
    
    \item $\lim_{n\to\infty} b_n = 0$.
\end{enumerate}
Then $\sum^\infty_{n=0} a_n b_n$ converges.
\end{theorem}
\begin{proof}
Let $M > 0$ be such that $\left|A_n\right| \leq M$ for all $n$. Since $\lim_{n\to\infty} b_n = 0$ and $\{b_n\}$ is decreasing, for every $\varepsilon > 0$, there is an integer $N > 0$ such that $b_N \leq \varepsilon/(2M)$. Now for $n \geq m \geq N$, we have
\begin{align*}
    \left|\sum^n_{k=m} a_nb_n\right| & = \left|\sum^{n-1}_{k=m} A_k(b_k - b_{k+1}) + A_n b_n - A_{m-1} b_m\right| \\
    & \leq M \left|\sum^{n-1}_{k=m} (b_k - b_{k+1}) + b_n - b_m\right| \\
    & = 2M b_m \leq 2M b_N \leq \varepsilon.
\end{align*}
Now convergence of $\sum^\infty_{n=0} a_n b_n$ follows from the Cauchy criterion.
\end{proof}

\medskip

\begin{theorem}[Leibnitz]
Suppose
\begin{enumerate}[label=(\alph*)]
    \item $\left|c_1\right| \geq \left|c_2\right| \geq \left|c_2\right| \geq \cdots$;
    
    \item $c_{2k-1} \geq 0, c_{2k} \leq 0, k = 1,2,3,\cdots$;
    
    \item $\lim_{n\to\infty} c_n = 0$.
\end{enumerate}
Then $\sum^\infty_{n=0} c_n$ converges.
\end{theorem}
\begin{proof}
Apply Theorem \ref{th_234} with $a_n = (-1)^{n+1}$ and $b_n = \left|c_n\right|$.
\end{proof}

\medskip

\begin{theorem}
Suppose the radius of convergence of $\sum^\infty_{n=0} c_n z^n$ is $1$, and suppose $c_0 \geq c_1 \geq c_2 \geq \cdots$, $\lim_{n\to\infty} c_n = 0$. Then $\sum^\infty_{n=0} c_n z^n$ converges at every point on the circle $\left|z\right| = 1$, expect possibly at $z = 1$.
\end{theorem}
\begin{proof}
Let $a_n = z^n$, $b_n = c_n$, and if $\left|z\right| = 1, z \neq 1$, we have
\begin{align*}
    \left|A_n\right| = \left|\sum^n_{k=0} z^k\right| = \left|\frac{1 - z^{n+1}}{1 - z}\right| \leq \frac{2}{\left|1 - z\right|}.
\end{align*}
Hence the assumptions of Theorem \ref{th_234} are satisfied and the convergence follows.
\end{proof}

\medskip



\section{Addition and Multiplication of Series}

\begin{theorem}
If $\sum^\infty_{n=0} a_n = A$, and $\sum^\infty_{n=0} b_n = B$, then $\sum^\infty_{n=0} (a_n + b_n) = A + B$, and $\sum^\infty_{n=0} ca_n = cA$, for any fixed $c$.
\end{theorem}
\begin{proof}
Let
\begin{align*}
    A_n = \sum^n_{k=0} a_k, \quad B_n = \sum^n_{k=0} b_k.
\end{align*}
Then
\begin{align*}
    A_n + B_n = \sum^n_{k=0} (a_k + b_k).
\end{align*}
Since $\lim_{n\to\infty} A_n = A$ and $\lim_{n\to\infty} B_n = B$, we have 
\begin{align*}
    \lim_{n\to\infty}(A_n + B_n) = A + B.
\end{align*}
Proof of the second equality is easy.
\end{proof}

\medskip

\begin{theorem}
Given $\sum^\infty_{n=0} a_n$ and $\sum^\infty_{n=10} b_n$, let
\begin{align*}
    c_n = \sum^n_{k=0} a_k b_{n-k}, \,\, n = 1,2,3,\cdots,
\end{align*}
and call $\sum^\infty_{n=0} c_n$ the product of the two given series.
\end{theorem}


\begin{theorem}
If the series $\sum^\infty_{n=0} a_n$ converges absolutely and the series $\sum^\infty_{n=0} b_n$ converges, then
\begin{align*}
    \left(\sum^\infty_{n=0} a_n\right) \left(\sum^\infty_{n=0} b_n\right) = \sum^\infty_{n=0} c_n,
\end{align*}
where 
\begin{align*}
    c_n = \sum^n_{k=0} a_k b_{n-k}.
\end{align*}
\end{theorem}
\begin{proof}
Let
\begin{align*}
    A_n = \sum^n_{k=0} a_n, \quad B_n = \sum^n_{k=0} b_n, \quad C_n = \sum^n_{k=0} c_n, 
\end{align*}
and $A = \sum^\infty_{n=0} a_n, B = \sum^\infty_{n=0} b_n$. Also, we define $\beta_n = B_n - B$. Then,
\begin{align*}
    C_n & = a_0b_0 + (a_0b_1 + a_1b_0) + \cdots + (a_0b_n + a_1b_{n-1} + \cdots + a_nb_0) \\
    & = a_0 B_n + a_1 B_{n-1} + \cdots + a_n B_0 \\
    & = a_0 (B + \beta_n) + a_1 (B + \beta_{n-1}) + \cdots + a_n (B + \beta_0) \\
    & = A_n B + a_0 \beta_n + a_1 \beta_{n-1} + \cdots a_n \beta_0.
\end{align*}
Let
\begin{align*}
    \gamma_n = a_0 \beta_n + a_1 \beta_{n-1} + \cdots a_n \beta_0,
\end{align*}
and it suffices to show that $\lim_{n\to\infty} \gamma_n = 0$, since $A_nB \to AB$. Since $\sum a_n$ converges absolutely, let
\begin{align*}
    \alpha = \sum^\infty_{n=0} \left|a_n\right|.
\end{align*}
For any $\varepsilon > 0$, since $\sum b_n$ converges, $\beta_n \to 0$, then there is an integer $N > 0$ such that for all $n \geq N$, $\left|\beta_n\right| < \varepsilon$, and hence
\begin{align*}
    \left|\gamma_n\right| & \leq \left|a_n \beta_0 + \cdots + a_{n-N} \beta_N\right| + \left|a_{n-N+1} \beta_{N+1} + \cdots + a_0 \beta_n\right| \\
    & \leq \left|a_n \beta_0 + \cdots + a_{n-N} \beta_N\right| + \alpha \varepsilon.
\end{align*}
Keeping $N$ fixed and letting $n \to \infty$ implies
\begin{align*}
    \limsup_{n\to\infty} \left|\gamma_n\right| \leq \alpha \varepsilon,
\end{align*}
since $a_k \to 0$ as $k \to \infty$. Since $\varepsilon$ is arbitrary, $\lim_{n\to\infty} \gamma_n = 0$.
\end{proof}

\medskip

\begin{definition}
Let $\sum^\infty_{n=1} a_n$ be a series and let $\phi: \mathbb{N} \to \mathbb{N}$, then a series $\sum^\infty_{n=1} a_{\phi(n)}$ is obtained from $\sum^\infty_{n=1} a_n$ by rearrangement of the elements. 
\end{definition}

\medskip

\begin{theorem}
If $\sum^\infty_{n=1} a_n$ converges absolutely and $\phi: \mathbb{N} \to \mathbb{N}$ is a bijection, then $\sum^\infty_{n=1} a_{\phi(n)}$ converges and 
\begin{align*}
    \sum^\infty_{n=1} a_{\phi(n)} = \sum^\infty_{n=1} a_n.
\end{align*}
\end{theorem}
\begin{proof}
By Cauchy criterion, for every $\varepsilon > 0$, there is an integer $N > 0$ such that for all $n \geq N$,
\begin{align*}
    \left|a_N\right| + \left|a_{N+1}\right| + \cdots + \left|a_n\right| < \varepsilon.
\end{align*}
Let $A_n = \sum^n_{k=1} a_k$ and $R_n = \sum^n_{k=1} a_{\phi(k)}$. Chooses $p$ so large such that
\begin{align*}
    \{1,2,\cdots,N-1\} \subset \{\phi(1),\phi(2),\cdots,\phi(p)\}.
\end{align*}
If $n > p$, then the numbers $a_1,a_2, \cdots, a_{N-1}$ will cancel out in the difference of the partial sums
\begin{align*}
    A_n - R_n = \sum^n_{k=1} a_k - \sum^n_{k=1} a_{\phi(k)}.
\end{align*}
And the remaining terms will be $a_i$ with $i \geq N$ and the signs are $+$ or $-$. Hence there is an integer $m > N$ such that 
\begin{align*}
    \left|A_n - R_n\right| \leq \sum^m_{k=N} \left|a_k\right| < \varepsilon.
\end{align*}
We proved that for every $\varepsilon > 0$, there is an integer $N > 0$ such that for all $n \geq N$, $\left|A_n - R_n\right| < \varepsilon$, which implies that $R_n$ converges to the same limit as $A_n$.
\end{proof}

\medskip

\begin{example}
Consider the series
\begin{align*}
    1 - \frac{1}{2} + \frac{1}{3} - \frac{1}{4} + \frac{1}{5} + \cdots.
\end{align*}
The series is convergent conditionally, but not absolutely. Let $\{s_n\}$ be the sequence of its partial sums. It follows that
\begin{align*}
    \frac{5}{6} = 1 - \frac{1}{2} + \frac{1}{3} = s_3 > s_5 > s_7 > \cdots.
\end{align*}
Now we change the order of elements as follows
\begin{align*}
    \underbrace{1 + \frac{1}{3} - \frac{1}{2}}_{>\,0} + \underbrace{\frac{1}{5} + \frac{1}{7} - \frac{1}{4}}_{>\,0} + \underbrace{\frac{1}{9} + \frac{1}{11} - \frac{1}{6}}_{>\,0} + \cdots.
\end{align*}
and clearly the series cannot converges to $5/6$.
\end{example}

\medskip

\begin{theorem}[Riemann]
If a series $\sum^\infty_{n=1} a_n$ converges conditionally, but not absolutely, then for every $g \in \overline{\mathbb{R}}$, there is a bijection $\phi: \mathbb{N} \to \markright{N}$ such that
\begin{align*}
    \sum^\infty_{n=1} a_{\phi(n)} = g.
\end{align*}
\end{theorem}




\chapter{Continuity}


\section{Limits of Functions}

\begin{definition}
Let $X$ and $Y$ be metric spaces, suppose $E \subset X$, $f$ maps $E$ into $Y$, and $x_0$ is a limit point of $E$. We write $f(x) \to y_0$ as $x \to x_0$, or
\begin{align*}
    \lim_{x \to x_0} f(x) = y_0,
\end{align*}
if for every $\varepsilon > 0$, there exists a $\delta > 0$ such that $d_Y(f(x), y_0) < \varepsilon$ for all points $x \in E$ for which $0 < d_X(x,x_0) < \delta$.

The symbols $d_X$ and $d_Y$ refer to the distance in $X$ and $Y$, respectively. 
\end{definition}

\begin{remark}
Note that $x_0 \in X$, but $x_0$ need not be a point of $E$. 
\end{remark}

\medskip

\begin{theorem}\label{th_31}
Let $X$, $Y$, $E$, $f$ and $x_0$ be as in the definition above. Then $\lim_{x \to x_0} f(x) = y_0$ if and only if $\lim_{n \to \infty} f(x_n) = y_0$ for every sequence $\{x_n\}$ in $E$ such that $\lim_{n\to\infty} x_n = x_0$ and $x_n \neq x$.
\end{theorem}
\begin{proof}
Suppose $\lim_{x \to x_0} f(x) = y_0$. Let $\{x_n\}$ be a sequence in $E$ such that $x_n \neq x$ and $\lim_{n\to\infty} x_n = x_0$. For every $\varepsilon > 0$, there exists $\delta > 0$ such that $d_Y(f(x),y_0) < \varepsilon$ for all $x \in E$ with $d_X(x,x_0) < \delta$. Also, there exists an integer $N > 0$ such that for all $n \geq N$, $d_X(x_n,x_0) < \delta$. Hence, for all $n \geq N$, we have $d_Y(x_n,x_0) < \varepsilon$, which implies $\lim_{n \to \infty} f(x_n) = y_0$.

Conversely, suppose $\lim_{x \to x_0} f(x) \neq y_0$. Then there exists some $\varepsilon$ such that for every $\delta > 0$, there is a point $x \in E$ for which $d_X(x,x_0) < \delta$ but $d_Y(f(x),y_0) > \varepsilon$. Taking $\delta = 1/n, n = 1,2,3,\cdots$, we obtain a sequence $\{x_n\}$ in $E$ such that $\lim_{n\to\infty} x_n = x_0$ but $\lim_{n\to\infty} f(x_n) \neq y_0$, which is a contradiction.
\end{proof}

\medskip

\begin{theorem}
If $f$ has a limit at $x_0$, this limit is unique. 
\end{theorem}
\begin{proof}
It follows from Theorem \ref{th_21} \ref{th_21_b} and Theorem \ref{th_31}.
\end{proof}

\medskip

\begin{theorem}
Suppose $E \subset X$, a metric space, $x_0$ is a limit point of $E$. $f$ and $g$ are complex functions on $E$, and
\begin{align*}
    \lim_{x\to x_0} f(x) = A, \quad \lim_{x\to x_0} g(x) = B.
\end{align*}
Then,
\begin{enumerate}[label=(\alph*)]
    \item $\lim_{x\to x_0} (f + g)(x) = A + B$;
    
    \item $\lim_{x\to x_0} (fg)(x) = AB$;
    
    \item $\lim_{x\to x_0} \displaystyle \left(\frac{f}{g}\right)(x) = \frac{A}{B}$, if $B \neq 0$.
\end{enumerate}
\end{theorem}
\begin{proof}
The proof is obvious.
\end{proof}



\medskip



\section{Continuous Functions}

\begin{definition}
Suppose $X$ and $Y$ are metric spaces, $E \subset X$ and $x_0 \in E$, and $f: E \to Y$. Then $f$ is said to be continuous at $x_0$ if for every $\varepsilon > 0$, there exists a $\delta > 0$ such that 
\begin{align*}
    d_Y(f(x),f(x_0)) < \varepsilon,
\end{align*}
for all points $x \in E$ for which $d_X(x,x_0) < \delta$. 

If $f$ is continuous at every point of $E$, then $f$ is said to be continuous on $E$.
\end{definition}

\begin{remark}
If $x_0$ is an isolated point of $E$, then the definition implies that every function $f$ which has $E$ as its domain is continuous at $x_0$. Indeed, for every $\varepsilon > 0$, we can choose $\delta > 0$ os that the only point $x \in E$ for which $d_X(x,x_0) < \delta$ is $x = x_0$, then $d_Y(f(x),f(x_0)) = 0 < \varepsilon$.
\end{remark}

\medskip

\begin{theorem}
With the same assumption in the above definition, and also assume that $x_0$ is a limit point of $E$. Then $f$ is continuous at $x_0$ if and only if $\lim_{x\to x_0} f(x) = f(x_0)$.
\end{theorem}

\medskip

\begin{theorem}
Suppose $X, Y, Z$ are metric spaces, $E \subset X$, $f: E \to Y$ and $g: f(E) \to Z$, and $h$ is the mapping of $E$ into $Z$ defined by
\begin{align*}
    h(x) = g(f(x)).
\end{align*}
If $f$ is continuous at a point $x_0 \in E$ and if $g$ is continuous at the point $f(x_0)$, then $h$ is continuous at $x_0$.
\end{theorem}
\begin{proof}
For every $\varepsilon > 0$, since $g$ is continuous at $f(x_0)$, there exists $\eta > 0$ such that 
\begin{align*}
    d_Z(g(y),g(x_0)) < \varepsilon,
\end{align*}
if $d_Y(y,f(x_0)) < \eta$ for $y \in f(E)$. Since $f$ is continuous at $x_0$, there exists a $\delta > 0$ such that
\begin{align*}
    d_Y(f(x),f(x_0)) < \eta,
\end{align*}
if $d_X(x,x_0) < \delta$ for $x \in E$. It follows that
\begin{align*}
    d_Z(h(x),h(x_0)) = d_Z(g(f(x)), g(f(x_0))) < \varepsilon,
\end{align*}
if $d_X(x,x_0) < \delta$ for $x \in E$. Thus $h$ is continuous at $x_0$.
\end{proof}

\medskip

\begin{theorem}\label{th_36}
A mapping $f$ of a metric space $X$ into a metric space $Y$ is continuous if and only if $f^{-1}(V)$ is open in $X$ for every open set $V$ in $Y$.
\end{theorem}
\begin{proof}
Suppose $f$ is continuous on $X$ and $V$ is an open set in $Y$. We need to show that every point of $f^{-1}(V)$ is an interior point of $f^{-1}(V)$. So suppose $x \in X$, and $f(x) \in V$. Since $V$ is open, there exists $\varepsilon > 0$ such that for all points $y$ with $d_Y(f(x),y) < \varepsilon$, $y \in V$. Since $f$ is continuous, then there exists a $\delta > 0$ such that $d_Y(f(x), f(x')) < \varepsilon$ if $d_X(x,x') < \delta$. Hence $x' \in f^{-1}(V)$ if $d_X(x,x') < \delta$.

Conversely, suppose $f^{-1}(V)$ is open in $X$ for every open set $V$ in $Y$. For $x \in X$ and every $\varepsilon > 0$, let $V$ be the set of all $y \in Y$ such that $d_Y(y,f(x)) < \varepsilon$. Clearly $V$ is open, hence $f^{-1}(V)$ is open, hence there exits $\delta > 0$ such that $x' \in f^{-1}(V)$ if $d_X(x,x') < \delta$. But if $x' \in f^{-1}(V)$, then $f(x') \in V$, so $d_Y(f(x), f(x')) < \varepsilon$.
\end{proof} 

\medskip

\begin{corollary}\label{coro_361}
A mapping $f$ of a metric space $X$ into a metric space $Y$ is continuous if and only if $f^{-1}(V)$ is closed in $X$ for every closed set $V$ in $Y$.
\end{corollary}
\begin{proof}
If follows from the theorem, since a set is closed if and only if its complement is open, and since $f^{-1}(E^c) = \left(f^{-1}(E)\right)^c$ for every $E \subset X$.
\end{proof}

\medskip

\begin{theorem}\label{th_37}
Let $f$ and $g$ be complex continuous functions on a metric space $X$. Then $f + g$, $fg$ and $f/g$ are continuous on $X$, when $g(x) \neq 0$ for all $x \in X$.
\end{theorem}

\medskip

\begin{theorem}\label{th_38}
~\begin{enumerate}[label=(\alph*)]
    \item Let $f_1, \cdots, f_n$ be real functions on a metric space $X$, and let $f: X \to \mathbb{R}^k$ be defined by
    \begin{align*}
        f(x) = \left(f_1(x), \cdots, f_n(x)\right),
    \end{align*}
    then $f$ is continuous if and only if each of the functions $f_1, \cdots, f_n$ is continuous. \label{th_38_a}
    
    \item If $f,g: X \to \mathbb{R}^n$ are continuous mappings, then $f + g$ and $fg$ are continuous. \label{th_38_b}
\end{enumerate}
\end{theorem}
\begin{proof}
Part \ref{th_38_a} follows from the inequalities
\begin{align*}
    \left|f_i(x) - f_i(y)\right| \leq \left|f(x) - f(y)\right| = \left(\sum^n_{i=1} \left|f_i(x) - f_i(y)\right|^2\right)^{1/2},
\end{align*}
for $i = 1,2,\cdots,n$. Part \ref{th_38_b} follows from \ref{th_38_a} and Theorem \ref{th_37}.
\end{proof}

\medskip




\section{Continuity and Compactness}

\begin{definition}
Suppose $E \subset X$. A mapping $f: E \to \mathbb{R}^n$ is said to be bounded if there is a real number $M$ such that $\left|f(x)\right| \leq M$ for all $x \in E$. 
\end{definition}

\medskip

\begin{theorem}\label{th_39}
Suppose $f$ is a continuous mapping of a compact metric space $X$ into a metric space $Y$, then $f(X)$ is compact.
\end{theorem}
\begin{proof}
Let $\{V_{\alpha}\}$ be an open cover of $f(X)$. Since $f$ is continuous, by Theorem \ref{th_36}, each $f^{-1}(V_{\alpha})$ is open. Since $X$ is compact, there is a finite subcoverings $\{V_{\alpha_1}, \cdots, V_{\alpha_n}\}$ such that
\begin{align*}
    X \subset \bigcup^n_{i=1} f^{-1}(V_{\alpha_i}).
\end{align*}
Since $f\left(f^{-1}(E)\right) \subset E$ for every $E \subset Y$, hence we have
\begin{align*}
    f(X) \subset \bigcup^n_{i=1} V_{\alpha_i},
\end{align*}
and hence there is a finite subcoverings that covers $f(X)$, which implies $f(X)$ is compact.
\end{proof}

\medskip

\begin{theorem}\label{th_310}
If $f$ is a continuous mapping of a compact metric space $X$ into $\mathbb{R}^n$, then $f(X)$ is closed and bounded. Thus, $f$ is bounded.
\end{theorem}
\begin{proof}
It follows from Theorem \ref{th_123}.
\end{proof}

\begin{remark}
This result is important when $f$ is a real function.
\end{remark}

\medskip

\begin{theorem}\label{th_311}
Suppose $f$ is a continuous real function on a compact metric space $X$, and
\begin{align*}
    M = \sup_{x\in X} f(x), \quad m = \inf_{x\in X} f(x).
\end{align*}
Then there exist points $x_1, x_2 \in X$ such that $f(x_1) = M$ and $f(x_2) = m$.
\end{theorem}
\begin{proof}
By Theorem \ref{th_310}, $f(X)$ is a bounded and closed subset of $\mathbb{R}$, hence $f(X)$ contains $M$ and $m$ by Theorem \ref{th_113}.
\end{proof}

\begin{remark}
This theorem can also be stated as follows: There exists $x_1, x_2 \in X$ such that $f(x_2) \leq f(x) \leq f(x_1)$ for all $x \in X$, that is, $f$ attains its maximum and minimum. Now we provide another proof to this theorem.
\end{remark}

\medskip

\begin{proof}[Second Proof of Theorem \ref{th_311}]
clearly there is a sequence $\{x_n\} \in X$ such that $\lim_{n\to\infty} f(x_n) = \sup_{x\in X} f(x)$ (even if the supremum equals $\infty$).  Since the sequence $\{x_n\}$ is bounded, then it has a convergent subsequence $\{x_{n_k}\}$ by Theorem \ref{th_210}, for which $x_{n_k} \to x_1 \in X$ and hence by the continuity of $f$,
\begin{align*}
    \sup_{x \in X} f(x) = \lim_{n\to\infty} f(x_{n_k}) = f(x_1).
\end{align*}
Similarly, there is $x_2 \in X$ such that $f(x_2) = \inf_{x\in X} f(x)$. 
\end{proof}

\medskip

\begin{theorem}
Suppose $f$ is a continuous one-to-one mapping of a compact metric space $X$ onto a metric space $Y$. Then the inverse mapping $f^{-1}$ defined on $Y$ given by
\begin{align*}
    f^{-1}(f(x)) = x, \,\, x \in X,
\end{align*}
is a continuous mapping of $Y$ onto $X$.
\end{theorem}
\begin{proof}
Applying Theorem \ref{th_36} to $f^{-1}$ rather than $f$, and it suffices to show that $f(V)$ is open whenever $V$ is open on $X$. Fix such a set $V$.

The complement $V^c$ of $V$ is closed in $X$, hence compact by Theorem \ref{th_116}, hence $f(V^c)$ is a compact subset of $Y$ by Theorem \ref{th_39}. Since $f$ is one-to-one and onto, $f(V)$ is the complement of $f(V^c)$. Hence $f(V)$ is open.
\end{proof}

\medskip


\section{Uniform Continuity}

\begin{definition}
Let $f: X \to Y$. We say that $f$ is uniformly continuous on $X$ if for every $\varepsilon > 0$ there exists $\delta > 0$ such that 
\begin{align*}
    d_Y(f(x),f(y)) < \varepsilon,
\end{align*}
for all $x,y \in X$ for which $d_X(x,y) < \delta$.
\end{definition}

\begin{remark}
The continuity and uniformly continuity are equivalent on compact sets, which will be shown in the next theorem.
\end{remark}

\medskip

\begin{theorem}\label{th_313}
Let $f$ be a continuous mapping of a compact metric space $X$ into a metric space $Y$. Then, $f$ is uniformly continuous.
\end{theorem}
\begin{proof}
Let $\varepsilon > 0$ be given. Since $f$ is continuous, then for each $x \in X$, there is $\phi(x)$ such that
\begin{align}\label{th_313_equ1}
    d_Y(f(x),f(y)) < \frac{\varepsilon}{2},
\end{align}
for all $y \in X$ with $d_X(x,y) < \phi(x)$. Let $B_x = \{y \in X \,:\, d_X(x,y) < \phi(x)/2\}$. Clearly $\{B_x\}$ is an open cover of $X$, and since $X$ is compact, there is a finite subcovering $\{B_{x_1}, \cdots, B_{x_n}\}$ such that 
\begin{align*}
    X \subset \bigcup^n_{i=1} B_{x_i}.
\end{align*}
Let $\delta = \min\{\phi(x_1), \cdots, \phi(x_n)\}/2$. Now for any $x,y \in X$ such that $d_X(x,y) < \delta$. Also, there is $j \in \{1,\cdots,n\}$ such that $x \in B_{x_j}$, hence $d_X(x,x_j) < \phi(x_j)/2$. And we have
\begin{align*}
    d_X(y, x_j) \leq d_X(y,x) + d_X(x, x_j) < \delta + \frac{\phi(x_j)}{2} \leq \phi(x_j).
\end{align*}
Finally, \eqref{th_313_equ1} implies
\begin{align*}
    d_Y(f(x),f(y)) \leq d_Y(f(x), f(x_j)) + d_Y(f(x_j), f(y)) < \varepsilon,
\end{align*}
and thus $f$ is uniformly continuous.
\end{proof}



\begin{proof}[Second Proof of Theorem \ref{th_313}]
Suppose to the contrary that $f$ is not uniformly continuous, that is, there exists $\varepsilon > 0$ such that for all $\delta > 0$, there are $x,y \in X$ such that $d_Y(f(x),f(y)) \geq \varepsilon$ if $d_X(x,y) < \delta$. 

Taking $\delta = 1/n$, then there exist sequences $\{x_n\}$ and $\{y_n\}$ such that $d_Y(f(x_n),f(y_n)) \geq \varepsilon$ if $d_X(x_n,y_n) < 1/n$. Since $X$ is compact, then $\{x_n\}$ has a subsequence $\{x_{n_k}\}$ such that $x_{n_k} \to x_0 \in X$. Since $d_X(x_{n_k},y_{n_k}) < 1/n_k$, $y_{n_k}$ also converges to $x_0$. By the continuity of $f$, we have 
\begin{align*}
    \lim_{k\to\infty} f(x_{n_k}) = f(x_0), \quad \lim_{k\to\infty} f(y_{n_k}) = f(x_0),
\end{align*}
and this contradicts with $_Y(f(x_{n_k}),f(y_{n_k})) \geq \varepsilon$.
\end{proof}

\medskip

\begin{theorem}\label{th_314}
Let $E$ be a noncompact set in $\mathbb{R}$. Then,
\begin{enumerate}[label=(\alph*)]
    \item there exists a continuous function on $E$ which is not bounded; \label{th_314_a}
    
    \item there exists a continuous and bounded function on $E$ which has no maximum. \label{th_314_b}
\end{enumerate}
If, in addition, $E$ is bounded, then
\begin{enumerate}[label=(\alph*)]
    \setcounter{enumi}{2}
    \item there exists a continuous function on $E$ which is not uniformly continuous. \label{th_314_c}
\end{enumerate}
\end{theorem}
\begin{proof}
Suppose first that $E$ is bounded, so that there exists a limit point $x_0$ of $E$ which is not a point of $E$. Consider
\begin{align*}
    f(x) = \frac{1}{x - x_0}, \,\, x \in E.
\end{align*}
Clearly $f$ is continuous but not unbounded. Also, $f$ is not uniformly continuous. Now consider 
\begin{align*}
    g(x) = \frac{1}{1 + (x - x_0)^2}, \,\, x \in E.
\end{align*}
Hence $g$ is continuous on $E$ and is bounded, since $0 < g(x) < 1$. It is clear that
\begin{align*}
    \sup_{x\in E} g(x) = 1,
\end{align*}
and thus $g$ has no maximum on $E$. 

Now suppose $E$ is unbounded. Then $f(x) = x$ proves part \ref{th_314_a}, and 
\begin{align*}
    h(x) = \frac{x^2}{1 + x^2}, \,\, x \in E,
\end{align*}
proves part \ref{th_314_b}. For part \ref{th_314_c}, let $E = \mathbb{Q} \cap [0,2]$, and clearly $E$ is bounded. Now define $f$ on $E$ as
\begin{align*}
    l(x) = \begin{cases}
        0, & 0 \leq x < \sqrt{2}, \\
        1, & \sqrt{2} < x \leq 2.
    \end{cases}
 \end{align*}
Clearly $l$ is continuous on $E$, since $f^{-1}(\{0\})$ and $f^{-1}(\{1\})$ are closed in $\mathbb{Q}$. However, $f$ is not uniformly continuous.
\end{proof}

\medskip

\begin{theorem}\label{th_315_added}
Let $X,Y$ be two metric spaces, and $f: X \to Y$ be uniformly continuous. If $\{x_n\}$ is a Cauchy sequence in $X$, then $\{f(x_n)\}$ is a Cauchy sequence in $Y$.
\end{theorem}
\begin{proof}
For any $\varepsilon > 0$, since $f$ is uniformly continuous, then there is a $\delta$ such that $d_Y(f(x), f(y)) < \varepsilon$ for all $x,y \in A$ if $d_X(x,y) < \delta$. Since $\{x_n\}$ is a Cauchy sequence, then for the $\delta$ as before, there is an integer $N = N(\delta)$ such that $d_X(x_n, x_m) < \delta$ for all $n,m \geq N$. Hence $d_Y(f(x_n),f(x_m)) < \varepsilon$ for all $n, m \geq N$.
\end{proof}

\medskip

\begin{theorem}[Extension Theorem]\label{th_316_added}
Let $Y$ be a complete metric space, and $X$ a metric space. Let $A \subset X$ and $f: A \to Y$ be a uniformly continuous function, then there is a unique uniformly continuous function $g: \overline{A} \to Y$ such that $g(x) = f(x)$.
\end{theorem}
\begin{proof}
Let $\overline{x} \in \overline{A}$ and let sequence $\{x_n\} \in A$ such that $\lim_{n\to\infty} x_n = \overline{x}$. By Theorem \ref{th_315_added}, $\{f(x_n)\}$ is a Cauchy sequence in $Y$, and hence convergent. If $\{y_n\}$ is another sequence in $X$ such that $\lim_{n\to\infty} y_n = \overline{x}$, then $\lim_{n\to\infty} f(x_n) = \lim_{n\to\infty} f(y_n)$. This is easy to verify.

Now let $g(\overline{x}) = \lim_{n\to\infty} f(x_n)$. If $\overline{x} \in A$, then we can let $x_n = \overline{x}$, and clearly $f(\overline{x}) = g(\overline{x})$. It remains to prove that $g$ is uniformly continuous. For every $\varepsilon > 0$, since $f$ is uniformly continuous, there is a $\delta > 0$ such that 
\begin{align*}
    d_Y(f(x),f(y)) < \frac{\varepsilon}{3},
\end{align*}
for all $x,y \in A$ for which $d_X(x,y) < \delta$. Now let $\overline{x}, \overline{y} \in \overline{A}$ and $d_X(\overline{x}, \overline{y}) < \delta/3$. And there are sequence $\{x_n\}, \{y_n\} \in A$ such that $x_n \to \overline{x}, y_n \to \overline{y}$. Then there are $x_k$ and $y_k$ such that
\begin{align}\label{th_316_added_equ1}
    d_X(x_k,\overline{x}) < \frac{\delta}{3}, \quad d_X(y_k,\overline{y}) < \frac{\delta}{3}, 
\end{align}
and 
\begin{align}\label{th_316_added_equ2}
    d_Y(g(\overline{x}),f(x_k)) < \frac{\varepsilon}{3}, \quad d_Y(g(\overline{y}),f(y_k)) < \frac{\varepsilon}{3}.
\end{align}
Indeed, since $x_n \to \overline{x}$ and $g(\overline{x}) = \lim_{n\to\infty} f(x_n)$. Then for $\varepsilon/3$, there is an integer $N_1$ such that $d_Y(g(\overline{x}), f(x_n)) < \varepsilon/3$ for all $n \geq N_1$. Also, for $\delta/3$, there is an integer $N_2$ such that $d_X(x_n, \overline{x}) < \delta/3$ for all $n \geq N_2$. Similarly there are $N_3, N_4$ for $y_n$ and letting $k \geq \max\{N_1, N_2, N_3, N_4\}$ gives \eqref{th_316_added_equ1} and \eqref{th_316_added_equ2}. Hence we have
\begin{align*}
    d_X(x_k, y_k) \leq d_X(x_k,\overline{x}) + d_X(\overline{x},\overline{y}) + d_X(\overline{y}, y_k) < \frac{\delta}{3} + \frac{\delta}{3} + \frac{\delta}{3} = \delta,
\end{align*}
and by uniformly continuity of $f$, we have
\begin{align*}
    d_Y(f(x_k), f(y_k)) < \frac{\varepsilon}{3}.
\end{align*}
Finally we have
\begin{align*}
    d_Y(g(\overline{x}),g(\overline{y})) \leq d_Y(g(\overline{x}),f(x_k)) + d_Y(f(x_k), f(y_k)) + d_Y(f(y_k), g(\overline{y})) < \frac{\varepsilon}{3} + \frac{\varepsilon}{3} + \frac{\varepsilon}{3} = \varepsilon,
\end{align*}
and thus $g$ is uniformly continuous.
\end{proof}
 

\medskip


\section{Continuity and Connectedness}

\begin{theorem}\label{th_315}
If $f: X \to Y$ is a continuous mapping, and if $E \subset X$ is connected, then $f(E)$ is connected.
\end{theorem}
\begin{proof}
Suppose to the contrary that $f(E) = A \cup B$, where $A$ and $B$ are nonempty separated subsets of $Y$. Let $G = E \cap f^{-1}(A)$ and $H = E \cap f^{-1}(B)$.

Then $E = G \cup H$ and neither $G$ nor $H$ is empty. Since $A \subset \overline{A}$, we have $G \subset f^{-1}(\overline{A})$. By the continuity of $f$ and Corollary \ref{coro_361}, $f^{-1}(\overline{A})$ is closed, and hence $\overline{G} \subset f^{-1}(\overline{A})$. It follows that $f(\overline{G}) \subset \overline{A}$. Since $f(H) = B$ and $\overline{A} \cap B$ is empty, we have $\overline{G} \cap H$ is empty. The same argument implies that $G \cap \overline{H}$ is empty. Hence $G$ and $H$ are separated, which is a contradiction.
\end{proof}

\medskip

\begin{theorem}[Intermediate Value Property]\label{th_316}
Let $f$ be a continuous real function on $[a,b]$. If $f(a) < f(b)$ and if $c$ is a number such that $f(a) < c < f(b)$, then there exists a point $x \in (a,b)$ such that $f(x) = c$. 
\end{theorem}

\begin{remark}
A similar result holds if $f(a) > f(b)$. Hence this theorem says that a continuous real function assumes all intermediate values on an interval.
\end{remark}

\begin{proof}
By Theorem \ref{th_126}, $[a,b]$ is connected, hence Theorem \ref{th_315} shows that $f([a,b])$ is connected, and applying Theorem \ref{th_126} again implies the theorem.
\end{proof}

\medskip

\begin{proof}[Second Proof of Theorem \ref{th_316}]
Let $A = \{t \in [a,b] \,:\, f(t) < c\}$. Then the set $A$ is nonempty, since $a \in A$. Also, the set $A$ is bounded from above by $b$, so $z = \sup A \leq b$. Since $a \in A$ and $z = \sup A$, we have $a \leq z$. By Theorem \ref{th_126}, $z \in [a,b]$.

It remains to prove that $f(z) = c$. Consider the case when $f(z) < c$. And $z \leq b$, let $\varepsilon = c - f(z)$. Since $f$ is continuous, there is a $\delta > 0$ such that if $z < t < z + \delta$, then
\begin{align*}
    \left|f(z) - f(t)\right| < \varepsilon = c - f(z),
\end{align*}
and hence $f(t) < c$. Then $t \in A$ and this is a contradiction with the fact that $z = \sup A$. The case when $f(z) > c$ is similar.
\end{proof}

\medskip






\section{One-sided Limits}


\begin{definition}
Let $f$ be defined on $(a,b)$ and let $x_0 \in [a,b)$. We say that $g \in \mathbb{R}$ is the right-hand limit of $f$ at $x_0$ if for every $\varepsilon > 0$, there is a $\delta > 0$ such that for all $x \in (a,b)$, $\left|f(x) - g\right| < \varepsilon$ if $x_0 < x < x_0 + \delta$. Then we write
\begin{align*}
    \lim_{x \to x_0^+} f(x) = g.
\end{align*}
Similarly, we can define the left-hand limit
\begin{align*}
    \lim_{x \to x_0^-} f(x) = g.
\end{align*}
It is clear that for any point $x_0 \in (a,b)$, $\lim_{x \to x_0}f(x)$ exists if and only if 
\begin{align*}
    \lim_{x \to x_0^+} f(x) = \lim_{x \to x_0^-} f(x) = g.
\end{align*}
\end{definition}

\medskip

\begin{definition}\label{def_36}
Let $f$ be defined on $(a,b)$. If $f$ is discontinuous at $x_0$ and if $\lim_{x \to x_0^+} f(x)$ and $\lim_{x \to x_0^-} f(x)$ exist, then $f$ is said to have a discontinuity of the first kind, or a simple discontinuity. Otherwise, the discontinuity is said to be of the second kind.

There are two ways in which a function can have a simple discontinuity: either 
\begin{align*}
    \lim_{x \to x_0^+} f(x) \neq \lim_{x \to x_0^-} f(x),
\end{align*}
or
\begin{align*}
    \lim_{x \to x_0^+} f(x) = \lim_{x \to x_0^-} f(x) \neq f(x_0).
\end{align*}
\end{definition}

\medskip

\begin{definition}
Let $f$ be real on $(a,b)$, then $f$ is said to be monotonically increasing on $(a,b)$ if $a < x < y < b$ implies $f(x) \leq f(y)$. Similarly we can define the monotonically decreasing function.
\end{definition}

\medskip

\begin{theorem}\label{th_317}
Let $f$ be monotonically increasing on $(a,b)$. Then $\lim_{t \to x^{+}} f(x)$, $\lim_{t \to x^{-}} f(x)$ exist at every point $x$ of $(a,b)$. More precisely,
\begin{align}\label{th_317_equ1}
    \sup_{a < t < x} f(t) = \lim_{t \to x^-} f(x) \leq f(x) \leq \lim_{t \to x^+} f(x) = \inf_{x < t < b} f(t).
\end{align}
Furthermore, if $a < x < y < b$, then
\begin{align}\label{th_317_equ2}
    \lim_{t \to x^+} f(x) \leq \lim_{t \to y^-} f(y).
\end{align}
\end{theorem}
\begin{proof}
By hypothesis, the set $E = \{f(t) \,:\, a < t < x\}$ is bounded from above by $f(x)$ and hence has a least upper bound, denoted by $A = \sup E$. Clearly $A \leq f(x)$. We need to show that $A = \lim_{t\to x^-} f(x)$.

Let $\varepsilon > 0$ and since $A = \sup E$, then there exists a $\delta > 0$ such that for $x - \delta \in (a,x)$, we have
\begin{align*}
    A - \varepsilon < f(x - \delta) \leq A.
\end{align*}
Since $f$ is monotonically increasing, then for $t \in (x - \delta, x)$, we have
\begin{align*}
    f(x - \delta) < f(t) < A.
\end{align*}
Hence for this $\varepsilon > 0$, there is a $\delta > 0$ such that for $t \in (x - \delta, x)$, we have
\begin{align*}
    \left|f(t) - A\right| < \varepsilon,
\end{align*}
and hence $A = \lim_{t\to x^-} f(x)$. And the second part of \eqref{th_317_equ1} can be proved in the same way.

Next if $a < x < y < b$, since $f$ is monotonically increasing, we have
\begin{align*}
    \lim_{t\to x^+} f(x) = \inf_{x < t < b} f(t) = \inf_{x < t < y} f(t).
\end{align*}
Similarly, we have
\begin{align*}
    \lim_{t\to y^-} f(y) = \sup_{a < t < y} f(t) = \sup_{x < t < y} f(t).
\end{align*}
Hence, by \eqref{th_317_equ1}, we have
\begin{align*}
    \sup_{x < t < y} f(t) \leq \inf_{x < t < y} f(t),
\end{align*}
and thus \eqref{th_317_equ2} follows.
\end{proof}

\medskip

\begin{corollary}
Monotonic functions have no discontinuities of the second kind.
\end{corollary}

\medskip

\begin{theorem}
Let $f$ be monotonic on $(a,b)$. Then the set of points of $(a,b)$ at which $f$ is discontinuous is at most countable.
\end{theorem}
\begin{proof}
Suppose that $f$ is increasing and $E$ is the set of points at which $f$ is discontinuous. With every point $x \in E$, there is a rational number $r(x)$ such that 
\begin{align*}
    \lim_{t\to x^-} f(t) < r(x) < \lim_{t\to x^+} f(t),
\end{align*}
since $f$ only have discontinuities of the first kind. If $x_1 < x_2$, then
\begin{align*}
    \lim_{t\to x_1^+} f(t) \leq \lim_{t\to x_2^-} f(t),
\end{align*}
hence $r(x_1) \neq r(x_2)$ if $x_1 \neq x_2$. We have established a one-to-one correspondence between $E$ and a subset of $\mathbb{Q}$, which is countable.
\end{proof}

\medskip

\section{Limits Involving Infinity}

\begin{definition}
For any real number $a$, the set $\{x \,:\, x > c\}$ is called a neighborhood of $+ \infty$ and is written as $(a, \infty)$. Similarly, the set $(- \infty, a)$ is a neighborhood of $- \infty$.
\end{definition}

\medskip

\begin{definition}
Let $f: A \to \mathbb{R}$, $A \subset \mathbb{R}$ be given and let $x_0$ be a limit point of $A$. We say that $f$ diverges to $\infty$ at $x_0$ if for all $M > 0$, there is a $\delta > 0$ such that $f(x) > M$ for all $x \in A$ for which $\left|x - x_0\right| < \delta$. Then we write
\begin{align*}
    \lim_{x \to x_0} f(x) = \infty.
\end{align*}
Similarly we can define limit to $- \infty$.
\end{definition}

\medskip

\begin{definition}
Let $f: A \to \mathbb{R}$, $A \subset \mathbb{R}$ is unbounded from above, that is every interval $(M, \infty)$ contains points of $A$. We say $g \in \mathbb{R}$ is the limit of $f$ as $x \to \infty$, if for every $\varepsilon > 0$, there exist $M > 0$ such that for all $x > M, x \in A$, $\left|f(x) - g\right| < \varepsilon$. Then we write
\begin{align*}
    \lim_{x \to \infty} f(x) = g.
\end{align*}
Similarly, we define limit at $-\infty$, $\lim_{x \to -\infty} f(x) = g$. 
\end{definition}

\medskip

\begin{theorem}\label{th_319}
$\lim_{x\to\infty} \displaystyle \left(1 + \frac{1}{x}\right)^x = e$.
\end{theorem}
\begin{proof}
Let $x_n \to \infty$, and let $k_n = \floor*{x_n}$ be the integer part of $x_n$. Hence, $k_n \leq x_n < k_n + 1$, and hence
\begin{align*}
    1 + \frac{1}{k_n + 1} < 1 + \frac{1}{x_n} \leq 1 + \frac{1}{k_n},
\end{align*}
and then 
\begin{align*}
    \left(1 + \frac{1}{k_n + 1}\right)^{k_n} < \left(1 + \frac{1}{x_n}\right)^{x_n} \leq \left(1 + \frac{1}{k_n}\right)^{k_n+1},
\end{align*}
and this can be written as
\begin{align}\label{th_319_equ1}
    \left(1 + \frac{1}{k_n + 1}\right)^{k_n+1} \frac{1}{1 + \frac{1}{k_n + 1}} < \left(1 + \frac{1}{x_n}\right)^{x_n} \leq \left(1 + \frac{1}{k_n}\right)^{k_n} \left(1 + \frac{1}{k_n}\right).
\end{align}
Letting $k_n \to \infty$ as $n \to \infty$, the right-hand side and left-hand side of \eqref{th_319_equ1} converges to $e$. Thus,
\begin{align*}
    \lim_{n\to\infty} \left(1 + \frac{1}{x_n}\right)^{x_n} = e,
\end{align*}
which implies
\begin{align*}
    \lim_{x\to\infty} \left(1 + \frac{1}{x}\right)^{x} = e.
\end{align*}
\end{proof}

\medskip

\begin{proposition}\label{prop_31}
If $\lim_{x\to\infty} f(x) = g \in \overline{\mathbb{R}}$, where $\overline{\mathbb{R}}$ is the same as in Definition \ref{def_22}, then
\begin{align*}
    \lim_{x \to 0^+} f\left(\frac{1}{x}\right) = g.
\end{align*}
\end{proposition}

\medskip

\begin{theorem}
$\lim_{x\to 0} \left(1 + x\right)^{1/x} = e$.
\end{theorem}

\begin{remark}
We consider two-sided limit here, that is $x$ can be negative.
\end{remark}

\begin{proof}
By Theorem \ref{th_319} and Proposition \ref{prop_31}, we have
\begin{align*}
    \lim_{x\to 0^+} \left(1 + x\right)^{1/x} = \lim_{x \to \infty} \left(1 + \frac{1}{x}\right)^{x} = e.
\end{align*}
It remains to prove the left-hand limit. We have
\begin{align*}
    \lim_{x\to 0^-} \left(1 + x\right)^{1/x} & = \lim_{x\to 0^+} \left(1 - x\right)^{-1/x} = \lim_{x\to \infty} \left(1 - \frac{1}{x}\right)^{-x} \\
    & = \lim_{x\to \infty} \left(\frac{x}{x-1}\right)^x = \lim_{x\to \infty} \left(1 + \frac{1}{x-1}\right)^{x-1} \left(1 + \frac{1}{x-1}\right) = e.
\end{align*}
\end{proof}








\chapter{Differentiation}

\section{The Derivative of Real Functions}

\begin{definition}\label{def_41}
Let $f$ be a real function defined on $[a,b]$. For any $x \in [a,b]$ define 
\begin{align}\label{def_41_equ1}
    f'(x) = \lim_{t \to x} \frac{f(t) - f(x)}{t - x}, \,\, a < t < b, t \neq x, \nu
\end{align}
provided this limit exists. We associate with the function $f$ a function $f'$ whose domain is the set of points at which the limit \eqref{def_41_equ1} exists, and $f'$ is called the derivative of $f$.

If $f'$ is defined at a point $x$, we say that $f$ is differentiable at $x$. If $f'$ is defined at every point of a set $E \subset [a,b]$, we sat that $f$ is differentiable on $E$.
\end{definition}

\medskip

\begin{theorem}
Let $f$ be defined on $[a,b]$. If $f$ is differentiable at a point $x \in [a,b]$, then $f$ is is continuous at $x$.
\end{theorem}
\begin{proof}
As $t \to x$, we have 
\begin{align*}
    f(t) - f(x) = \frac{f(t) - f(x)}{t - x} \cdot (t - x) \to f'(x) \cdot 0 = 0.
\end{align*}
\end{proof}

\begin{remark}
The converse of this theorem is not true. It is easy to construct continuous functions which is not differentiable at an isolated point. 
\end{remark}

\medskip

\begin{definition}
We say that a function $f(h)$ is of order $O(h^{\alpha})$ if there is $\varepsilon > 0$ and $M > 0$ such that 
\begin{align*}
    \frac{\left|f(h)\right|}{\left|h\right|^{\alpha}} \leq M,
\end{align*}
for all $0 < \left|h\right| < \varepsilon$. We sat that $f(h)$ is of order $o(h^{\alpha})$ if 
\begin{align*}
    \frac{\left|f(h)\right|}{\left|h\right|^{\alpha}} \to 0,
\end{align*}
as $h \to 0$. Clearly, the condition $o(h^{\alpha})$ is stronger than $O(h^{\alpha})$. 
\end{definition}

\medskip

\begin{theorem}\label{th_42}
Let $f: E \to \mathbb{R}$, $E \subset \mathbb{R}$ and $x \in  \operatorname{int}(E)$. Then the following conditions are equivalent:
\begin{enumerate}[label=(\alph*)]
    \item $f$ is differentiable at $x$. \label{th_42_a}
    
    \item There is $L \in \mathbb{R}$ such that $f(x+h) = f(x) + Lh + o(h)$. \label{th_42_b}
    
    \item There is $L \in \mathbb{R}$ and a function $\psi$ that is continuous at $x$, $\psi(x) = 0$ such that $f(x+h) = f(x) + Lh + h\psi(h)$. \label{th_42_c}
\end{enumerate}
Moreover, $L = f'(x)$.
\end{theorem}
\begin{proof}
\ref{th_42_c} $\Rightarrow$ \ref{th_42_b} Since 
\begin{align*}
    \frac{h \psi(h)}{h} = \psi(h) \xrightarrow[]{h \to 0} 0,
\end{align*}
we conclude that $h\psi(h)$ is order $o(h)$.\\
\ref{th_42_b} $\Rightarrow$ \ref{th_42_a} Assume that $f$ satisfies $f(x+h) = f(x) + Lh + o(h)$. Then 
\begin{align*}
    \frac{f(x+h) - f(x)}{h} = L + \frac{o(h)}{h} \xrightarrow[]{h \to 0} L,
\end{align*}
so $f$ is differentiable at $x$ and $f'(x) = L$.\\
\ref{th_42_a} $\Rightarrow$ \ref{th_42_c} Since $f$ is differentiable at $x$, then we have
\begin{align*}
    \lim_{h\to 0} \frac{f(x+h) - f(x)}{h} = f'(x),
\end{align*}
and hence
\begin{align*}
    \lim_{h\to 0} \frac{f(x+h) - f(x) - f'(x)h}{h} = 0.
\end{align*}
And this implies
\begin{align*}
    \psi(h) = \begin{cases}
        \frac{f(x+h) - f(x) - f'(x)h}{h}, & h \neq 0, \\
        0, & h = 0,
    \end{cases}
\end{align*}
is continuous at $0$ and $\psi(0) = 0$. Clearly,
\begin{align*}
    f(x+h) = f(x) + f'(x)h + \psi(h) h.
\end{align*}
Thus \ref{th_42_c} follows $L = f'(x)$.
\end{proof}

\medskip

\begin{theorem}\label{th_43}
Suppose $f$ and $g$ are defined on $[a,b]$ and are differentiable at a point $x \in [a,b]$. Then, $f + g$, $fg$ and $f/g$ are differentiable at $x$, and
\begin{enumerate}[label=(\alph*)]
    \item $(f + g)'(x) = f'(x) + g'(x)$. \label{th_43_a}
    
    \item $(fg)'(x) = f'(x)g(x) + f(x)g'(x)$. \label{th_43_b}
    
    \item $\displaystyle \left(\frac{f}{g}\right)'(x) = \frac{f'(x)g(x) - f(x)g'(x)}{g^2(x)}$. \label{th_43_c}
\end{enumerate}
In part \ref{th_43_c}, we assume $g(x) \neq 0$.
\end{theorem}
\begin{proof}
Part \ref{th_43_a} is clear. Now let $h = fg$, then
\begin{align*}
    h(t) - h(x) = f(t)[g(t) - g(x)] + g(x)[f(t) - f(x)],
\end{align*}
Since $f, g$ are continuous and $f',g'$ are defined on $x$, by Theorem \ref{th_22} we have
\begin{align*}
    \frac{h(t) - h(x)}{t - x} = f(t) \frac{g(t) - g(x)}{t - x} + g(x) \frac{f(t) - f(x)}{t - x} \xrightarrow[]{t \to x} f(x)g'(x) + g(x) f'(x).
\end{align*}
Now let $h = f/g$, again by Theorem \ref{th_22} we have
\begin{align*}
    \frac{h(t) - h(x)}{t - x} = \frac{1}{g(t)g(x)} \left[g(x) \frac{f(t) - f(x)}{t - x} - f(x) \frac{g(t) - g(x)}{t - x} \right] \xrightarrow[]{t \to x} \frac{f'(x)g(x) - f(x)g'(x)}{g^2(x)}.
\end{align*}
\end{proof}

\medskip

\begin{theorem}[Chain Rule]
Suppose $f$ is continuous on $[a,b]$, $f'(x)$ exists at some point $x \in [a,b]$, $g$ is defined on an interval $I$ which contains the range of $f$ and $g$ is differentiable at the point $f(x)$. If $h(t) = g(f(t))$ for $a \leq t \leq b$, then $h$ is differentiable at $x$, then
\begin{align*}
    h'(x) = g'(f(x)) f'(x).
\end{align*}
\end{theorem}
\begin{proof}
Let $f(x) = y$. Since $g$ is differentiable at $y$, Theorem \ref{th_42} implies that 
\begin{align*}
    g(y') = g(y) + g'(y)(y' - y) + (y' - y) \psi(y' - y),
\end{align*}
where $\psi$ is continuous at $0$ and $\psi(0) = 0$. Substituting $y$ by $f(x)$ and $y'$ by $f(x')$ gives
\begin{align*}
    g(f(x')) = g(f(x)) + g'(f(x))(f(x') - f(x)) + (f(x') - f(x)) \psi(f(x') - f(x)),
\end{align*}
and hence
\begin{align*}
    \frac{(g \circ f)(x') - (g \circ f)(x)}{x' - x} = g'(f(x))\frac{f(x') - f(x)}{x' - x} + \frac{f(x') - f(x)}{x' - x} \psi(f(x') - f(x)).
\end{align*}
Since $f(x') \to f(x)$ and $\psi(f(x') - f(x)) \to 0$ as $x' \to x$, thus we have
\begin{align*}
    \lim_{x' \to x} \frac{(g \circ f)(x') - (g \circ f)(x)}{x' - x}  = g'(f(x)) f'(x).
\end{align*}
\end{proof}

\medskip





\section{Mean Value Theorem}

\begin{definition}
Let $f$ be a real function defined on a metric space $X$. We say that $f$ has a local maximum at a point $x_0 \in X$ if there exists $\delta > 0$ such that $f(x) \leq f(x_0)$ for all $x \in X$ with $d_X(x,x_0) < \delta$.
\end{definition}

\medskip

\begin{theorem}\label{th_45}
Let $f$ be defined on $[a,b]$, if $f$ has a local maximum or a local minimum at a point $x \in (a,b)$, then $f'(x) = 0$.
\end{theorem}
\begin{proof}
Choose $\delta$ as in the above definition. If $x - \delta < t < x$, then we have
\begin{align*}
    \lim_{t\to x^-}\frac{f(t) - f(x)}{t - x} \geq 0,
\end{align*}
and for $x < t < x + \delta$, we have
\begin{align*}
    \lim_{t\to x^+}\frac{f(t) - f(x)}{t - x} \leq 0.
\end{align*}
Two inequalities implies that $f'(x) = 0$. Similar argument would implies the theorem for local minimum.
\end{proof}

\medskip

\begin{theorem}[Rolle]\label{th_46}
If $f: [a,b] \to \mathbb{R}$ is continuous and differentiable in $(a,b)$ and $f(a) = f(b)$, then there is $c \in (a,b)$ such that $f'(c) = 0$.
\end{theorem}
\begin{proof}
If $f$ is constant, then the result is obvious. Since $f$ is continuous on $[a,b]$, by Theorem \ref{th_311}, $f$ attains its maximum and minimum, that is there are $c_1$ and $c_2$ such that
\begin{align*}
    f(c_1) = \sup_{x\in[a,b]} f(x), \quad f(c_2) = \inf_{x\in[a,b]} f(x).
\end{align*}
Since $f$ is not constant, then either $f(c_1) > f(a)$ or $f(c_2) < f(a)$. Suppose $f(c_1) > f(a)$, then $f'(c_1) = 0$ by the previous theorem.
\end{proof}

\medskip

\begin{theorem}[Cauchy Mean Value Theorem]\label{th_47}
Suppose $f$ and $g$ are continuous real functions on $[a,b]$ which are differentiable in $(a,b)$, and $g'(x) \neq 0$ for all $x \in (a,b)$, then there is a point $c \in (a,b)$ at which 
\begin{align*}
    \frac{f(b) - f(a)}{g(b) - g(a)} = \frac{f'(c)}{g'(c)}.
\end{align*}
\end{theorem}
\begin{remark}
Note that differentiability is not required at the endpoints.
\end{remark}
\begin{proof}
Let
\begin{align*}
    h(t) = [f(b) - f(a)]g(t) - [g(b) - g(a)]f(t), \,\, a \leq t \leq b.
\end{align*}
Clearly $h$ is continuous on $[a,b]$ and differentiable on $(a,b)$, and
\begin{align*}
    h(a) = f(b)g(a) - f(a)g(b) = h(b).
\end{align*}
By Rolle theorem \ref{th_46}, there is $c \in (a,b)$ such that $h(c) = 0$ and the theorem follows.
\end{proof}

\medskip

This theorem is the {\em generalized Mean Value theorem}, and the following is the easy one, the one called {\em mean value theorem}.

\medskip

\begin{theorem}[Mean Value Theorem]\label{th_48}
If $f: [a,b] \to \mathbb{R}$ is continuous and differentiable on $(a,b)$, then there is a point $c \in (a,b)$ such that
\begin{align*}
    f(b) - f(a) = (b - a) f'(c).
\end{align*}
\end{theorem}
\begin{proof}
Take $g(x) = x$ in Theorem \ref{th_47}.
\end{proof}

\medskip

\begin{theorem}
Suppose $f$ is differentbale in $(a,b)$.
\begin{enumerate}[label=(\alph*)]
    \item If $f'(x) \geq 0$ for all $x \in (a,b)$, then $f$ is monotonically increasing.
    
    \item If $f'(x) = 0$ for all $x \in (a,b)$, then $f$ is constant.
    
    \item If $f'(x) \leq 0$ for all $x \in (a,b)$, then $f$ is monotonically decreasing.
\end{enumerate}
\end{theorem}
\begin{proof}
All conclusions follows from
\begin{align*}
    f(x_1) - f(x_2) = (x_1 - x_2) f'(c),
\end{align*}
for any $x_1, x_2 \in (a,b)$ and for some $c$ between $x_1$ and $x_2$.
\end{proof}

\medskip

\begin{theorem}
If $f: [a,b] \to \mathbb{R}$ is differentiable in $(a,b)$ and the derivative is bounded, $\left|f'(x)\right| \leq M$ for all $x \in (a,b)$, then 
\begin{align*}
    \left|f(x) - f(y)\right| \leq M \left|x - y\right|,
\end{align*}
for all $x,y \in (a,b)$. Hence $f$ is Lipschitz and thus uniformly continuous.
\end{theorem}
\begin{proof}
By Theorem \ref{th_48}, $f(x) - f(y) = f(c)'(x - y)$ for all $x,y \in [a,b]$ and for some $c$ between $x$ and $y$. Hence,
\begin{align*}
    \left|f(x) - f(y)\right| \leq \left|f(c)'\right| \left|x - y\right|,
\end{align*}
and the uniform continuity follows.
\end{proof}

\medskip

\begin{example}
~\begin{enumerate}[label=(\alph*)]
    \item Let $f$ be defined as
    \begin{align*}
        f(x) = \begin{cases}
            x \sin \frac{1}{x}, & x \neq 0, \\
            0, x = 0.
        \end{cases}
    \end{align*}
    Clearly, we have for $x \neq 0$,
    \begin{align*}
        f'(x) = \sin \frac{1}{x} - \frac{1}{x} \cos \frac{1}{x}.
    \end{align*}
    For $x = 0$, we have
    \begin{align*}
        \frac{f(t) - f(0)}{t} = \sin \frac{1}{t},
    \end{align*}
    which does not converge as $t \to 0$, hence $f'(0)$ does not exist.
    
    \item Let $f$ be defined as
    \begin{align*}
        f(x) = \begin{cases}
            x^2 \sin \frac{1}{x}, & x \neq 0, \\
            0, x = 0.
        \end{cases}
    \end{align*}
    Similarly, for $x \neq 0$,
    \begin{align*}
        f'(x) = 2x \sin \frac{1}{x} - \cos \frac{1}{x}.
    \end{align*}
    At $x = 0$, we have
    \begin{align*}
        \frac{f(t) - f(0)}{t} = t \sin \frac{1}{t} \xrightarrow[]{t \to 0} 0, 
    \end{align*}
    and hence $f'(0) = 0$. Thus, $f$ is differentiable at every point, however, $f'(x)$ is not continuous, since $f'(x)$ does not converge as $x \to 0$.
\end{enumerate}
\end{example}

\medskip

Now we know that a function $f$ may have a derivative $f'$ which exists at every point, but is discontinuous at some point. However, $f'$ also satisfies the intermediate value property, just like a continuous function (see Theorem \ref{th_316}).

\medskip

\begin{theorem}
Suppose $f$ is a real differentiable function on $[a,b]$ and $f'(a) < \lambda < f'(x)$, then there is a point $c \in (a,b)$ such that $f'(c) = \lambda$. Similar result holds for $f'(a) > f'(b)$.
\end{theorem}
\begin{proof}
Let $g(x) = f(x) - \lambda x$, then $g'(a) < 0$, and hence $g(x_1) < g(a)$ for some $x_1 \in (a,b)$. Similarly, $g'(b) > 0$ and there is $x_2 \in (a,b)$ such that $g(x_2) < g(b)$. Since $g$ is continuous on a compact set, $g$ attains its maximum and minimum, and the above properties implies that the maximum and minimum are not attained at the endpoints. Hence $g$ attains its maximum at $x_1 \in (a,b)$ and thus $g'(x_1) = 0$, which implies $f'(x_1) = \lambda$.
\end{proof}

\medskip

\begin{corollary}
If $f$ is differentiable on $[a,b]$, then $f'$ cannot have any simple discontinuities (see Definition \ref{def_36}) on $[a,b]$.
\end{corollary}

\medskip







\section{L'Hospital's Rule}

\begin{theorem}[L'Hospital's Rule]\label{th_412}
Suppose $f$ and $g$ are real and differentiable in $(a,b)$, and $g'(x) \neq 0$ for all $x \in (a,b)$, where $-\infty \leq a \leq b \leq + \infty$. Suppose
\begin{align}\label{th_412_equ1}
    \lim_{x\to a} \frac{f'(x)}{g'(x)} = A.
\end{align}
If
\begin{align}\label{th_412_equ2}
    \lim_{x\to a} f(x) = \lim_{x\to a} g(x) = 0,
\end{align}
or if
\begin{align}\label{th_412_equ3}
    \lim_{x\to a} g(x) = \infty,
\end{align}
then
\begin{align}\label{th_412_equ4}
    \lim_{x\to a} \frac{f(x)}{g(x)} = A.
\end{align}
\end{theorem}

\begin{remark}
The analogous statement is also true id $x \to b$ or if $g(x) \to -\infty$.
\end{remark}

\begin{proof}
We first consider the case when $- \infty \leq A < +\infty$. Choose real number $r, q$ such that $A < r < q$. By \eqref{th_412_equ1}, there is a point $c \in (a,b)$ such that $a < x < c$ implies that 
\begin{align*}
    \frac{f'(c)}{g'(c)} < r.
\end{align*}
If $a < x < y < c$, then Theorem \ref{th_47} implies there is a point $t \in (x,y)$ such that \begin{align}\label{th_412_equ5}
    \frac{f(x) - f(y)}{g(x) - g(y)} = \frac{f'(t)}{g'(t)} < r.
\end{align}

Suppose \eqref{th_412_equ2} holds, and letting $x \to a$ in \eqref{th_412_equ5} gives
\begin{align}\label{th_412_equ6}
    \frac{f(y)}{g(y)} \leq r < q,  \,\, a < y < c.
\end{align}

Now suppose \eqref{th_412_equ3} holds. Fix $y$ in \eqref{th_412_equ5}, we can choose $c_1 \in (a,y)$ such that $g(x) > g(y)$ and $g(x) > 0$ if $a < x < c_1$. Multiplying \eqref{th_412_equ5} by $[g(x) - g(y)]/g(x)$ implies that for $a < x < c_1$,
\begin{align}\label{th_412_equ7}
    \frac{f(x)}{g(x)} < r - r \frac{g(y)}{g(x)} + \frac{f(y)}{g(x)}.
\end{align}
Letting $x \to a$ in \eqref{th_412_equ7} shows there is a point $c_2 \in (a,c_1)$ such that for $a < x < c_2$,
\begin{align}\label{th_412_equ8}
    \frac{f(x)}{g(x)} < q.
\end{align}

Hence, \eqref{th_412_equ6} and \eqref{th_412_equ8} show that for any $q > A$, there is a point $c_2$ such that $f(x)/g(x) < q$ if $a < x < c_2$. And similarly, if $-\infty < A \leq + \infty$, and for any $p < A$, there is a $c_3$ such that for $a < x < c_3$,
\begin{align*}
    \frac{f(x)}{g(x)} > p,
\end{align*}
and the theorem follows.
\end{proof}

\medskip


\section{Taylor's Theorem}

\begin{definition}
If $f$ has a derivative $f'$ on an interval, and if $f'$ is differentiable, we denote the derivative of $f'$ by $f''$ and call $f''$ the second derivative of $f$. Continuing in this way and we have
\begin{align*}
    f, f', f'', f^{(3)}, \cdots, f^{(n)},
\end{align*}
each of which is the derivative of the previous one, $f^{(n)}$ is called the $n$-th derivative of $f$.
\end{definition}

\medskip

\begin{theorem}
Suppose $f$ is a real function on $[a,b]$, $n$ is a positive integer, $f^{(n-1)}$ is continuous on $[a,b]$, $f^{(n)}(x)$ exists for every $x \in (a,b)$. Let $\alpha, \beta$ be distinct points of $[a,b]$, and define
\begin{align*}
    P(x) = \sum^{n-1}_{k=0} \frac{f^{(k)}(\alpha)}{k!} (x - \alpha)^k.
\end{align*}
Then there exists a point $\widetilde{x}$ between $\alpha$ and $\beta$ such that
\begin{align*}
    f(\beta) = P(\beta) + \frac{f^{(n)}(\widetilde{x})}{n!} (\beta - \alpha)^n.
\end{align*}
For $n = 1$, this is just the Mean Value theorem \ref{th_48}. In general, this theorem shows that $f$ can be approximated by a polynomial of degree $n - 1$.
\end{theorem}
\begin{proof}
Let $M$ be defined as 
\begin{align*}
    M = \frac{f(\beta) - P(\beta)}{(\beta - \alpha)^n},
\end{align*}
and let 
\begin{align*}
    g(x) = f(x) - P(x) - M(x - \alpha)^n, \,\, x \in (a,b).
\end{align*}
We need to show that $n!M = f^{(n)}(\widetilde{x})$ for some $\widetilde{x}$ between $\alpha$ and $\beta$. Clearly, for $x \in (a,b)$,
\begin{align*}
    g^{(n)}(x) = f^{(n)}(x) - n!M.
\end{align*}

Since $P^{(k)}(\alpha) = f^{(k)}(\alpha)$ for $k = 0,1,\cdots,n-1$, we have
\begin{align*}
    g(\alpha) = g'(\alpha) = g''(\alpha) = \cdots = g^{(n-1)}(\alpha) = 0.
\end{align*}
Also, $g(\beta) = 0$, hence by the Mean Value theorem $g'(x_1) = 0$ for some $x_1$ between $\alpha$ and $\beta$. Since $g'(\alpha) = 0$, hence $g''(x_2) = 0$ for some $x_2$ between $\alpha$ and $x_1$. After $n$ steps we have $g^{(n)}(x_n) = 0$ for some $x_n$ between $\alpha$ and $x_{n-1}$, and hence $f^{(n)}(x_n) = n!M$.
\end{proof}

\medskip


\section{Differentiation of Vector-valued Functions} 
For functions $f$ which map $[a,b]$ into some $\mathbb{R}^n$, we may still apply Definition \ref{def_41} to define $f'(x)$. The limit in \ref{def_41_equ1} is considered as norm of $\mathbb{R}^n$, that is, $f'(x)$ is that point of $\mathbb{R}^n$ (provided the limit exists) for which \begin{align*}
    \lim_{t \to x} \left|\frac{f(t) - f(x)}{t - x} - f'(x)\right| = 0,
\end{align*}
and $f'$ is again a function with values in $\mathbb{R}^n$. If $f = (f_1, \cdots, f_n)$, then
\begin{align*}
    f' = (f_1', \cdots, f_n'),
\end{align*}
and $f$ is differentiable at a point $x$ if and only if each of $f_1, \cdots, f_n$ is differentiable at $x$.

\medskip

\begin{example}
Define for real $x$, $f(x) = e^{ix} = \cos x + i \sin x$. Then $f(2\pi) - f(0) = 1 - 1 = 0$, but $f'(x) = i e^{ix}$, so $\left|f'(x)\right| = 1$ for all real $x$. Hence, Mean Value theorem \ref{th_48} does not hold in this case.
\end{example}

\medskip

\begin{example}
On the segment $(0,1)$, define $f(x) = x$ and $g(x) = x + x^2 e^{i/x^2}$. Then
\begin{align*}
    \lim_{x \to 0} \frac{f(x)}{g(x)} = 1.
\end{align*}
Now we have
\begin{align*}
    g'(x) = 1 + \left(2x - \frac{2i}{x}\right) e^{i/x^2}, \,\, 0 < x < 1,
\end{align*}
so that
\begin{align*}
    \left|g'(x)\right| \geq \left|2x - \frac{2i}{x}\right| - 1 \geq \frac{2}{x} - 1.
\end{align*}
Hence,
\begin{align*}
    \left|\frac{f'(x)}{g'(x)}\right| = \left| \frac{1}{g'(x)} \right| \leq \frac{x}{2 - x},
\end{align*}
and so
\begin{align*}
    \lim_{x \to 0} \frac{f'(x)}{g'(x)} = 0.
\end{align*}
Thus we can see that L'Hospital's rule fails in this case. However, there is a consequence of Mean Value theorem, and it is true for all vector-valued functions.
\end{example}

\medskip

\begin{theorem}[Mean Value Theorem]\label{th_414}
Suppose $f$ is a continuous mapping of $[a,b]$ into $\mathbb{R}^n$ and $f$ is differentiable in $(a,b)$. Then there exists $x \in (a,b)$ such that
\begin{align*}
    \left|f(b) - f(a)\right| \leq (b - a) \left|f'(x)\right|.
\end{align*}
\end{theorem}
\begin{proof}
Let $z = f(b) - f(a)$, and define $\varphi(t) = z \cdot f(t)$ for $a \leq t \leq b$. Then $\varphi$ is a real-valued continuous on $[a,b]$ which is differentiable in $(a,b)$. The Mean Value theorem shows that there is $x \in (a,b)$, such that
\begin{align*}
    \varphi(b) - \varphi(a) = (b - a) \varphi'(x) = (b - a) z \cdot f'(x).
\end{align*}
On the other hand,
\begin{align*}
    \varphi(b) - \varphi(a) = z \cdot f(b) - z \cdot f(a) = z \cdot z = \left|z\right|^2.
\end{align*}
By the Cauchy-Schwarz inequality, we have
\begin{align*}
    \left|z\right|^2 = (b - a)  \left|z \cdot f'(x)\right| \leq (b - a) \left|z\right| \left|f'(x)\right|,
\end{align*}
and hence $\left|z\right| \leq (b - a) \left|f'(x)\right|$.
\end{proof}







\chapter{The Riemann-Stieltjes Integral}

\section{Definition and Existence of the Integral}

\begin{definition}
Let $[a,b]$ be a given interval. By a partition $P$ of $[a,b]$ we mean a finite set of points $x_0, x_1, \cdots, x_n$, where
\begin{align*}
    a = x_0 \leq x_1 \leq \cdots \leq x_{n-1} \leq x_n = b.
\end{align*}
We write $\Delta x_i = x_i = x_{i-1}$ for $i = 1,2,\cdots,n$. Now suppose $f$ is bounded real function defined on $[a,b]$. Corresponding to each partition $P$ of $[a,b]$, we let
\begin{align*}
    M_i = \sup f(x), \quad m_i = \inf f(x), \,\, x_{i-1} \leq x \leq x_i,
\end{align*}
and also define
\begin{align*}
    U(P,f) = \sum^n_{i=1} M_i \Delta x_i, \quad L(P,f) = \sum^n_{i=1} m_i \Delta x_i.
\end{align*}
Finally, we define 
\begin{align*}
    \overline{\int}^b_a f \,dx = \inf U(P,f), \quad \underline{\int}^b_a f \,dx = \sup L(P,f),
\end{align*}
where the $\inf$ and $\sup$ are taken over all partitions $P$ of $[a,b]$. These two terms are called the upper and lower Riemann integrals of $f$ over $[a,b]$.

If the upper and lower integrals are equal, we say that $f$ is Riemann integrable on $[a,b]$, we write $f \in \mathscr{R}$ (where $\mathscr{R}$ denotes the set of all integrable functions), and we denote the value by
\begin{align*}
    \int^b_a f \,dx, \quad \text{or} \quad \int^b_a f(x) \,dx.
\end{align*}
This is the Riemann integral of $f$ over $[a,b]$. 
\end{definition}

\medskip

Since $f$ is bounded, there exists two numbers $m$ and $M$ such that $m \leq f(x) \leq M$ for all $x \in [a,b]$. Hence, for every $P$,
\begin{align*}
    m(b - a) \leq L(P,f) \leq U(P,f) \leq M(b - a),
\end{align*}
so that $L(P,f)$ and $U(P,f)$ form a bounded set. This shows that the {\em the upper and lower integrals} are defined for every bounded function $f$. 

\medskip

\begin{definition}
Let $\alpha$ be a monotonically increasing function on $[a,b]$ (since $\alpha(a)$ and $\alpha(b)$ are finite, $\alpha$ is bounded on $[a,b]$). Corresponding to each partition $P$ of $[a,b]$, we write $\Delta \alpha_i = \alpha(x_i) - \alpha(x_{i-1})$. Clearly, $\Delta x_i \geq 0$. For any bounded real function $f$ on $[a,b]$, let
\begin{align*}
    U(P,f,\alpha) = \sum^n_{i=1} M_i \Delta \alpha_i, \quad L(P,f,\alpha) = \sum^n_{i=1} m_i \Delta \alpha_i,
\end{align*}
where $m_i$ and $M_i$ are the same as the previous definition. Also, let
\begin{align*}
    \overline{\int}^b_a f \,d\alpha = \inf U(P,f,\alpha), \quad \underline{\int}^b_a f \,d\alpha = \sup L(P,f,\alpha),
\end{align*}
and if these two terms are equal, we denote the common value by
\begin{align*}
    \int^b_a f \,d\alpha, \quad \text{or} \quad \int^b_a f(x) \,d\alpha(x).
\end{align*}
This is the Riemann-Stieltjes integral of $f$ with respect to $\alpha$ over $[a,b]$. 

By taking $\alpha(x) = x$, the Riemann integral is a special case of the Riemann-Stieltjes integral. 
\end{definition}

\medskip

\begin{definition}
We say that a partition $P^*$ is a refinement of $P$ is  $P^* \supset P$. Given two partitions $P_1$ and $P_2$, we say that $P^*$ is their common refinement if $P^* = P_1 \cup P_2$.
\end{definition}


\medskip

\begin{theorem}\label{th_51}
If $P^*$ is a refinement of $P$, then 
\begin{align*}
    L(P,f,\alpha) \leq L(P^*,f,\alpha), \quad U(P^*,f,\alpha) \leq U(P,f,\alpha).
\end{align*}
\end{theorem}
\begin{proof}
Suppose $P^*$ contains one more point than $P$, and let this point be $x^*$. Then, $x^* \in (x_{i-1}, x_i)$ for some $i$, where $x_{i-1}$ and $x_i$ are in $P$. Let
\begin{align*}
    w_1 & = \inf f(x), \,\, x_{i-1} \leq x \leq x^*, \\
    w_2 & = \inf f(x), \,\, x^* \leq x \leq x_{i}.
\end{align*}
Clearly $w_1 \geq m_i$ and $w_2 \geq m_i$. Hence,
\begin{align*}
    L(P^*,f,\alpha) - L(P,f,\alpha) & = w_1 \left(\alpha(x^*) - \alpha(x_{i-1})\right) + w_2 \left(\alpha(x_i) - \alpha(x^*)\right) - m_i \left(\alpha(x_i) - \alpha(x_{i-1})\right) \\
    & = (w_1 - m_i) \left(\alpha(x^*) - \alpha(x_{i-1})\right) + (w_2 - m_i) \left(\alpha(x_i) - \alpha(x^*)\right) \geq 0.
\end{align*}
If $P^*$ contains $k$ points more than $P$, then we can repeat this process. Similar argument will prove the second inequality.
\end{proof}

\medskip

\begin{theorem}
$\displaystyle \underline{\int}^b_a f \,d\alpha \leq \overline{\int}^b_a f \,d\alpha$.
\end{theorem}
\begin{proof}
Let $P^*$ be the common refinement of two partitions $P_1$ and $P_2$. By Theorem \ref{th_51}, we have
\begin{align*}
    L(P_1,f,\alpha) \leq L(P^*,f,\alpha) \leq U(P^*,f,\alpha) \leq U(P_2,f,\alpha).
\end{align*}
Hence, $L(P_1,f,\alpha) \leq U(P_2,f,\alpha)$. Taking $\inf$ and $\sup$ over all partition $P$ over $[a,b]$.
\end{proof}

\medskip

\begin{theorem}\label{th_53}
$f \in \mathscr{R}(\alpha)$ if and only if for every $\varepsilon > 0$, there exists a partition $P$ such that 
\begin{align}\label{th_53_equ1}
    U(P,f\alpha) - L(P,f,\alpha) < \varepsilon.
\end{align}
\end{theorem}
\begin{proof}
For every partition $P$, 
\begin{align*}
    L(P,f,\alpha) \leq \underline{\int} f \,d\alpha \leq \overline{\int} f \,d\alpha \leq U(P,f,\alpha).
\end{align*}
Hence \eqref{th_53_equ1} implies that
\begin{align*}
    0 < \overline{\int} f \,d\alpha - \underline{\int} f \,d\alpha < \varepsilon,
\end{align*}
and since this holds for all $\varepsilon > 0$, we have 
\begin{align*}
    \overline{\int} f \,d\alpha = \underline{\int} f \,d\alpha,
\end{align*}
and hence $f \in \mathscr{R}(\alpha)$.

Conversely, suppose $f \in \mathscr{R}(\alpha)$ and let $\varepsilon > 0$, then there exist partitions $P_1$ and $P_2$ such that
\begin{align*}
    U(P_1,f,\alpha) & < \int f \,d\alpha + \frac{\varepsilon}{2}, \\
    L(P_2,f,\alpha) & > \int f \,d\alpha - \frac{\varepsilon}{2}.
\end{align*}
Let $P = P_1 \cup P_2$, then Theorem \ref{th_51} implies
\begin{align*}
    U(P,f,\alpha) \leq U(P_1,f,\alpha) < \int f \,d\alpha + \frac{\varepsilon}{2} < L(P_2,f,\alpha) + \varepsilon \leq L(P,f,\alpha) + \varepsilon,
\end{align*}
and hence \eqref{th_53_equ1} holds.
\end{proof}

\medskip

\begin{theorem}\label{th_54}
~\begin{enumerate}[label=(\alph*)]
    \item If \eqref{th_53_equ1} holds for some $P$ and some $\varepsilon$, then \eqref{th_53_equ1} holds for every refinement of $P$, with the same $\varepsilon$. \label{th_54_a}
    
    \item If \eqref{th_53_equ1} holds for $P = \{x_0, x_1, \cdots, x_n\}$ and if $s_i, t_i$ are arbitrary points in $[x_{i-1},x_i]$, then
    \begin{align*}
        \sum^n_{i=1} \left|f(s_i) - f(s_i)\right| \Delta \alpha_i < \varepsilon.
    \end{align*} \label{th_54_b}
    
    \item If $f \in \mathscr{R}(\alpha)$ and the assumption of \ref{th_54_b} holds, then
    \begin{align*}
        \left|\sum^n_{i=1} f(t_i) \Delta \alpha_i - \int^b_a f \,d\alpha \right| < \varepsilon.
    \end{align*} \label{th_54_c}
\end{enumerate}
\end{theorem}
\begin{proof}
Theorem \ref{th_51} implies \ref{th_54_a}. Under the assumption of \ref{th_54_b}, $f(s_i), f(t_i) \in [m_i, M_i]$, and then 
\begin{align*}
    \left|f(s_i) - f(t_i)\right| \leq M_i - m_i,
\end{align*}
hence we have
\begin{align*}
    \sum^n_{i=1} \left|f(s_i) - f(s_i)\right| \Delta \alpha_i \leq U(P,f,\alpha) - L(P,f,\alpha) < \varepsilon.
\end{align*}
And the inequalities
\begin{align*}
    L(P,f,\alpha) \leq \sum f(t_i) \Delta \alpha_i \leq U(P,f,\alpha)
\end{align*}
and 
\begin{align*}
    L(P,f,\alpha) \leq \int f \,d\alpha \leq U(P,f,\alpha)
\end{align*}
prove \ref{th_54_c}.
\end{proof}

\medskip

\begin{theorem}
If $f$ is continuous on $[a,b]$, then $f \in \mathscr{R}(\alpha)$ on $[a,b]$.
\end{theorem}
\begin{proof}
Let $\varepsilon > 0$, and let $\eta > 0$ such that $\left|\alpha(b) - \alpha(a)\right| \eta < \varepsilon$. Since $f$ is uniformly continuous on $[a,b]$, then there is a $\delta > 0$ such that $\left|f(x) - f(y)\right| < \eta$ if $\left|x - y\right| < \delta$. Let $P$ be any partition of $[a,b]$ such that $\Delta x_i < \delta$ for all $i$, then $M_i - m_i \leq \eta$ for all $i = 1,2,\cdots,n$. Hence,
\begin{align*}
    U(P,f,\alpha) - L(P,f,\alpha) & \leq \eta \sum^n_{i=1} \Delta \alpha_i = \eta \left|\alpha(b) - \alpha(a)\right| < \varepsilon,
\end{align*}
and by Theorem \ref{th_53}, $f \in \mathscr{R}(\alpha)$.
\end{proof}

\medskip

\begin{theorem}
If $f$ is monotonic on $[a,b]$ and if $\alpha$ is continuous on $[a,b]$, then $f \in \mathscr{R}(\alpha)$. (We still assume that $\alpha$ is monotonic.)
\end{theorem}
\begin{proof}
Let $\varepsilon > 0$, and for any positive integer $n$, choose a partition such that
\begin{align*}
    \Delta \alpha_i = \frac{\alpha(b) - \alpha(a)}{n}.
\end{align*}
We assume that $f$ is monotonically increasing, then $M_i = f(x_i), m_i = f(x_{i-1})$ for $i = 1,2,\cdots,n$, and hence for $n$ large enough,
\begin{align*}
    U(P,f,\alpha) - L(P,f,\alpha) & = \frac{\alpha(b) - \alpha(a)}{n} \sum^n_{i=1} \left(f(x_i) - f(x_{i-1})\right) \\
    & = \frac{\alpha(b) - \alpha(a)}{n}  \left(f(b) - f(a)\right) < \varepsilon.
\end{align*}
Thus  $f \in \mathscr{R}(\alpha)$.
\end{proof}

\medskip

\begin{theorem}
Suppose $f$ is bounded on $[a,b]$, $f$ has only finitely many points of discontinuity on $[a,b]$, and $\alpha$ is continuous at every point at which $f$ is discontinuous. Then $f \in \mathscr{R}(\alpha)$.
\end{theorem}
\begin{proof}
Let $\varepsilon > 0$, and let $M = \sup \left|f(x)\right|$. Also let $E$ be the set of points at which $f$ is discontinuous. Since $E$ is finite and $\alpha$ is continuous at every point of $E$, we can cover $E$ by finitely many disjoint intervals $[u_i,v_i] \subset [a,b]$ such that the sum of the corresponding differences $\alpha(v_i) - \alpha(u_i)$ is less than $\varepsilon$. 

Removing the segments $(u_i,v_i)$ from $[a,b]$. Then $K \coloneqq [a,b] \setminus \bigcup (u_i,v_i)$ is compact, and hence $f$ is uniformly continuous on $K$, and there exists $\delta > 0$ such that $\left|f(x) - f(y)\right| < \varepsilon$ if $x,y \in K$ and $\left|x - x\right| < \delta$. Now we form a partition $P$, where each $u_i,v_i$ appear in $P$ and no segment $(u_i,v_i)$ is in $P$. If $x_{i-1}$ is not one of the $u_i$, then $\Delta x_i < \delta$. 
Also, $M_i - m_i \leq 2M$ for every $i$ and $M_i - m_i < \varepsilon$ unless $x_{i-1}$ is one of the $u_i$. Hence, we have
\begin{align*}
    U(P,f,\alpha) - L(P,f,\alpha) \leq \left(\alpha(b) - \alpha(a)\right) \varepsilon + 2M \varepsilon,
\end{align*}
and since $\varepsilon > 0$ is arbitrary, $f \in \mathscr{R}(\alpha)$.
\end{proof}

\begin{remark}
If $f$ and $\alpha$ have a common point of discontinuity, then $f$ need not be in $\mathscr{R}(\alpha)$.
\end{remark}

\medskip

\begin{theorem}\label{th_58}
Suppose $f \in \mathscr{R}(\alpha)$ on $[a,b]$, $m \leq M$, $\phi$ is continuous on $[m,M]$, and $h(x) = \phi(f(x))$ on $[a,b]$. Then $f \in \mathscr{R}(\alpha)$ on $[a,b]$.
\end{theorem}
\begin{proof}
Let $\varepsilon > 0$, since $\phi$ is uniformly continuous on $[m,M]$, there exists $\delta > 0$ such that $\delta < \varepsilon$ and $\left|\phi(s) - \phi(t)\right| < \varepsilon$ if $\left|s - t\right| < \delta$ and $s,t \in [m,M]$. Since $f \in \mathscr{R}(\alpha)$, there is a partition $P$ of $[a,b]$ such that 
\begin{align}\label{th_58_equ1}
    U(P,f,\alpha) - L(P,f,\alpha) < \delta^2.
\end{align}
Let $M^*_i$ and $m^*_i$ be the analogous numbers for $h$. Divide $1,2,\cdots,n$ into two classes: $i \in A$ if $M_i - m_i < \delta$, $i \in B$ if $M_i - m_i > \delta$.

For $i \in A$, the choice of $\delta$ implies that $M^*_i - m^*_i < \varepsilon$. For $i \in B$, $M^*_i - m^*_i \leq 2K$, where $K = \sup \left|\phi(t)\right|, t \in [m,M]$. By \eqref{th_58_equ1}, we have
\begin{align*}
    \delta \sum_{i \in B} \Delta \alpha_i \leq \sum_{i \in B} (M_i - m_i) \Delta \alpha_i < \delta^2,
\end{align*}
and hence $\sum_{i \in B} \Delta \alpha_i < \delta$. It follows that
\begin{align*}
    U(P,h,\alpha) - L(P,h,\alpha) & = \sum_{i \in A} (M^*_i - m^*_i) \Delta \alpha_i + \sum_{i \in B} (M^*_i - m^*_i) \Delta \alpha_i \\
    & = \varepsilon \left(\alpha(b) - \alpha(a)\right) + 2K\delta < \varepsilon \left(\alpha(b) - \alpha(a) + 2K\right).
\end{align*}
Since $\varepsilon$ is arbitrary, $h \in \mathscr(\alpha)$.
\end{proof}

\medskip



\section{Properties of the Integral}

\begin{theorem}
~\begin{enumerate}[label=(\alph*)]\label{th_59}
    \item If $f_1, f_2 \in \mathscr{R}(\alpha)$ on $[a,b]$, then $f_1 + f_2 \in \mathscr{R}(\alpha)$, $cf \in \mathscr{R}(\alpha)$ for every constant $c$, and
    \begin{align*}
        \int^b_a (f_1 + f_2) \,d\alpha & = \int^b_a f_1 \,d\alpha + \int^b_a f_2 \,d\alpha, \\
        \int^b_a cf \,d\alpha & = c \int^b_a f \,d\alpha.
    \end{align*} \label{th_59_a}
    
    \item If $f_1(x) \leq f_2(x)$ on $[a,b]$, then
    \begin{align*}
        \int^b_a f_1 \,d\alpha \leq \int^b_a f_2 \,d\alpha.
    \end{align*} \label{th_59_b}
    
    \item If $f \in \mathscr{R}(\alpha)$ on $[a,b]$ and if $a < c < b$, then $f \in \mathscr{R}(\alpha)$ on $[a,c]$ and $[c,b]$, and
    \begin{align*}
        \int^b_a f \,d\alpha = \int^c_a f \,d\alpha + \int^b_c f \,d\alpha.
    \end{align*} \label{th_59_c}
    
    \item If $f \in \mathscr{R}(\alpha)$ on $[a,b]$ and if $\left|f(x)\right| \leq M$ on $[a,b]$, then
    \begin{align*}
        \left|\int^b_a f \,d\alpha\right| \leq M \left(\alpha(b) - \alpha(a)\right).
    \end{align*} \label{th_59_d}
    
    \item If $f \in \mathscr{R}(\alpha_1)$ and $f \in \mathscr{R}(\alpha_2)$, then $f \in \mathscr{R}(\alpha_1 + \alpha_2)$ and
    \begin{align*}
        \int^b_a f \,d(\alpha_1 + \alpha_2) = \int^b_a f \,d\alpha_1 + \int^b_a f \,d\alpha_2.
    \end{align*}
    If $f \in \mathscr{R}(\alpha)$ and $c$ is a positive constant, then $f \in \mathscr{R}(c\alpha)$ and 
    \begin{align*}
        \int^b_a f \,d(c\alpha) = c \int^b_a f \,d\alpha.
    \end{align*} \label{th_59_e}
    
    \item (Mean Value Theorem) If $f: [a,b] \to \mathbb{R}$ is continuous, there is $\xi \in [a,b]$ such that
    \begin{align*}
        \frac{1}{\alpha(b) - \alpha(a)} \int^b_a f \,d\alpha = f(\xi).
    \end{align*} \label{th_59_f}
\end{enumerate}
\end{theorem}
\begin{proof}
If $f = f_1 + f_2$ and $P$ is any partition of $[a,b]$, then we have
\begin{align}\label{th_59_equ1}
    L(P,f_1,\alpha) + L(P,f_2,\alpha) \leq L(P,f,\alpha) \leq U(P,f,\alpha) \leq U(P,f_1,\alpha) + U(P,f_2,\alpha).
\end{align}
If $f_1 \in \mathscr{R}(c\alpha)$ and $f_2 \in \mathscr{R}(c\alpha)$, let $\varepsilon > 0$ be given, then there are partitions $P_1, P_2$ such that
\begin{align*}
    U(P_i,f_i,\alpha) - L(P_i,f_i,\alpha) < \varepsilon, \,\, i = 1,2.
\end{align*}
These inequalities still hold it $P_1$ and $P_2$ are replaced by their common refinement $P$. By \eqref{th_59_equ1}, we have $U(P,f,\alpha) - L(P,f,\alpha) < 2\varepsilon$, which implies $f \in \mathscr{R}(\alpha)$.

With the partition $P$ as before, we have
\begin{align*}
    U(P,f,\alpha) < \int f_i \,d\alpha + \varepsilon, \quad L(P,f,\alpha) >  \int f_i \,d\alpha - \varepsilon, \,\, i = 1,2,
\end{align*}
and hence
\begin{align*}
    \int f \,d\alpha & \leq U(P,f,\alpha) < \int f_1 \,d\alpha + \int f_2 \,d\alpha + 2 \varepsilon, \\
    \int f \,d\alpha & \geq L(P,f,\alpha) > \int f_1 \,d\alpha + \int f_2 \,d\alpha - 2 \varepsilon,
\end{align*}
and \ref{th_59_a} is proved. For \ref{th_59_b} - \ref{th_59_e}, similar argument could work.

For \ref{th_59_f}, since $f$ is continuous on compact set $[a,b]$, then $f$ attains its maximum and minimum at $x_1$ and $x_2$. Hence
\begin{align*}
    f(x_2) (\alpha(b) - \alpha(a)) \leq \int^b_a f \,d\alpha \leq f(x_1) (\alpha(b) - \alpha(a)),
\end{align*}
and this implies
\begin{align*}
    f(x_2) \leq \frac{1}{\alpha(b) - \alpha(a)} \int^b_a f \,d\alpha \leq f(x_1),
\end{align*}
hence by the Intermediate Value theorem, there is $\xi \in [a,b]$ such that
\begin{align*}
    \frac{1}{\alpha(b) - \alpha(a)} \int^b_a f \,d\alpha = f(\xi).
\end{align*}
\end{proof}

\medskip

\begin{theorem}\label{th_510}
If $f \in \mathscr{R}(\alpha)$ and $g \in \mathscr{R}(\alpha)$ on $[a,b]$, then
\begin{enumerate}[label=(\alph*)]
    \item $fg \in \mathscr{R}(\alpha)$; \label{th_510_a}
    
    \item $\left|f\right| \in \mathscr{R}(\alpha)$ and $\displaystyle \left|\int^b_a f \,d\alpha\right| \leq \int^b_a \left|f\right| \,d\alpha$. \label{th_510_b}
\end{enumerate}
\end{theorem}
\begin{proof}
Taking $\phi(t) = t^2$ in Theorem \ref{th_58} implies $f^2 \in \mathscr{R}(\alpha)$. Then the equation $fg = [(f + g)^2 - (f - g)^2]/4$ completes the proof of part \ref{th_510_a}.

Now taking $\phi(t) = \left|t\right|$ shows $\left|f\right| \in \mathscr{R}(\alpha)$. Choose $c = \pm 1$ such that $c \int f \,d\alpha \geq 0$. Thus we have
\begin{align*}
    \left|\int^b_a f \,d\alpha\right| = c \int^b_a f \,d\alpha = \int^b_a cf \,d\alpha \leq \int^b_a \left|f\right| \,d\alpha.
\end{align*}
\end{proof}

\medskip

\begin{definition}
The unit step function $I$ id defined by
\begin{align*}
    I(x) = \begin{cases}
        0, & x \leq 0, \\
        1, & x > 0.
    \end{cases}
\end{align*}
\end{definition}

\medskip

\begin{theorem}\label{th_511}
If $a < s < b$, $f$ is bounded on $[a,b]$, $f$ is continuous at $s$, and $\alpha(x) = I(x - s)$, then
\begin{align*}
    \int^b_a f \,d\alpha = f(s).
\end{align*}
\end{theorem}
\begin{proof}
Consider the partitions $P = \{x_0, x_1, x_2, x_3\}$, where $x_0 = a, x_1 = s < x_2 < x_3 = b$. Since $x_1 = s$, we have
\begin{align*}
    U(P,f,\alpha) = M_2 \left(I(x_2) - I(s)\right) + M_3 \left(I(x_3) - I(x_2)\right) = M-2 + 0 = M_2,
\end{align*}
similarly, $L(P,f,\alpha) = m_2$. Since $f$ is continuous at $s$, $M_2$ and $m_2$ converge to $f(s)$ as $x_2 \to s$.
\end{proof}

\medskip

\begin{theorem}
Suppose $c_n \geq 0$ for $n = 1,2,3,\cdots$, $\sum c_n$ converges and $\{s_n\}$ is a sequence of distinct points in $(a,b)$, and
\begin{align*}
    \alpha(x) = \sum^\infty_{n=1} c_n I(x - s_n).
\end{align*}
Let $f$ be continuous on $[a,b]$. Then,
\begin{align*}
    \int^b_a f \,d\alpha = \sum^\it_{n=1} c_n f(s_n).
\end{align*}
\end{theorem}
\begin{proof}
The comparison test shows that $\alpha(x)$ converges for every $x$, Also, $\alpha(x)$ is monotonically increasing with $\alpha(0) = 0$ and $\alpha(b) = \sum_n c_n$. Let $\varepsilon > 0$, and choose $N$ such that $\sum^\infty_{N+1} c_n < \varepsilon$. Let
\begin{align*}
    \alpha_1(x) = \sum^N_{n=1} c_n I(x - s_n), \quad \alpha_2(x) = \sum^\infty_{n=N+1} c_n I(x - s_n).
\end{align*}
By Theorem \ref{th_59} and \ref{th_511}, we have
\begin{align*}
    \int^b_a f \,d\alpha_1 = \sum^N_{n=1} c_n f(s_n).
\end{align*}
Since $\alpha_2(b) - \alpha_2(a) < \varepsilon$, we have
\begin{align*}
    \left|\int^b_a f \,d\alpha_2\right| \leq M \varepsilon,
\end{align*}
where $M = \sup \left|f(x)\right|$. Since $\alpha = \alpha_1 + \alpha_2$, we have
\begin{align*}
    \left|\int^b_a f \,d\alpha - \sum^N_{n=1} c_n f(s_n)\right| < M \varepsilon,
\end{align*}
and hence the theorem follows.
\end{proof}

\medskip

\begin{theorem}\label{th_513}
Assume $\alpha$ increases monotonically and $\alpha' \in \mathscr{R}$ on $[a,b]$. Let $f$ be a bounded real function on $[a,b]$. Then for $f \in \mathscr{R}(\alpha)$ if and only if $f \alpha' \in \mathscr{R})$. In that case,
\begin{align*}
    \int^b_a f(x) \,d\alpha = \int^b_a f(x) \alpha'(x) \,dx.
\end{align*}
\end{theorem}

\medskip

\begin{theorem}[Change of variable]\label{th_514}
Suppose $\varphi$ is a strictly increasing continuous function that maps an interval $[A,B]$ onto $[a,b]$. Suppose $\alpha$ is monotonically increasing on $[a,b]$ and $f \in \mathscr{R}(\alpha)$ on $[a,b]$. Define $\beta$ and $g$ on $[A,B]$ by 
\begin{align*}
    \beta(y) = \alpha(\varphi(y)), \quad g(y) = f(\varphi(y)).
\end{align*}
Then $g \in \mathscr{R}(\beta)$ and
\begin{align}\label{th_514_equ1}
    \int^B_A g \,d\beta = \int^b_a f \,d\alpha.
\end{align}
\end{theorem}
\begin{proof}
For each partition $P = \{x_0,x_1,\cdots,x_n\}$ of $[a,b]$ corresponds a partition $Q = \{y_0,y_1,\cdots,y_n\}$ pf $[A,B]$ such that $x_i = \varphi(y_i)$. Hence we have
\begin{align*}
    U(Q,g,\beta) = U(P,f,\alpha),  \quad L(Q,g,\beta) = L(P,f,\alpha).
\end{align*}
Since $f \in \mathscr{R}(\alpha)$, $P$ can be chosen such that both $U(P,f,\alpha)$ and $L(P,f,\alpha)$ are close to $\int f \,d\alpha$. By Theorem \ref{th_53}, $g \in \mathscr{R}(\beta)$ and hence \eqref{th_514_equ1} holds.
\end{proof}

\medskip

Next we discuss a special case of change of variable, which will be often used throughout analysis.

\medskip

\begin{corollary}[Change of variable]\label{coro_5141}
Taking $\alpha(x) = x$ in the above theorem. Then $\beta = \varphi$, assume $\varphi' \in \mathscr{R}$ on $[A,B]$. Applying Theorem \ref{th_513} to the left side of \eqref{th_514_equ1} implies
\begin{align*}
    \int^b_a f(x) \,dx = \int^B_A f(\varphi(y)) \varphi'(y) \,dy.
\end{align*}
\end{corollary}

\medskip



\section{Integration and Differentiation}

\begin{theorem}[The Fundamental Theorem of Calculus]
Let $f \in \mathscr{R}$ on $[a,b]$. For $x \in [a,b]$, let
\begin{align*}
    F(x) = \int^x_a f(t) \,dt.
\end{align*}
Then $F$ is continuous on $[a,b]$, furthermore, if $f$ is continuous at a point $x_0$ of $[a,b]$, then $F$ is differentiable at $x_0$, and $F'(x_0) = f(x_0)$.
\end{theorem}
\begin{proof}
Since $f \in \mathscr{R}$, $f$ is bounded. Suppose $\left|f(t)\right| \leq M$ for $a \leq t \leq b$. If $a \leq x < y \leq b$, then
\begin{align*}
    \left|F(y) - F(x)\right| = \left|\int^y_x f(t) \,dt\right| \leq M(y - x).
\end{align*}
Given $\varepsilon > 0$, we have $\left|F(y) - F(x)\right| < \varepsilon$ if $\left|y - x\right| < \varepsilon/M$. This proves uniformly continuity of $F$.

Now suppose $f$ is continuous at $x_0$, let $\varepsilon > 0$ and choose $\delta > 0$ such that $\left|f(t) - t(x_0)\right| < \varepsilon$ if $\left|t - x_0\right| < \delta, t \in [a,b]$. Hence, if $x_0 - \delta < s \leq x_0 \leq t < x_0 + \delta$ and $a \leq s < t \leq b$, we have
\begin{align*}
    \left|\frac{F(t) - F(s)}{t - s} - f(x_0)\right| = \left|\frac{1}{t - s} \int^t_s (f(u) - f(x_0)) \,du \right| < \varepsilon.
\end{align*}
It follows that $F'(x_0) = f(x_0)$.
\end{proof}

\medskip

\begin{theorem}[The Fundamental Theorem of Calculus]\label{th_516}
If $f \in \mathscr{R}$ on $[a,b]$ and if there is a differentiable function $F$ on $[a,b]$ such that $F' = f$, then
\begin{align*}
    \int^b_a f(x) \,dx = F(b) - F(a).
\end{align*}
\end{theorem}
\begin{proof}
Let $\varepsilon > 0$ be given, choose a partition $P = \{x_0,x_1,\cdots,x_n\}$ of $[a,b]$ such that $U(P,f) - L(P,f) < \varepsilon$. By the Mean Value theorem, there are $t_i \in [x_{i-1},x_i]$ such that 
\begin{align*}
    F(x_i) - F(x_{i-1}) = f(t_i) \Delta x_i, \,\, i = 1,2,\cdots,n.
\end{align*}
Hence we have 
\begin{align*}
    \sum^n_{i=1} f(t_i) \Delta x_i = F(b) - F(a).
\end{align*}
By Theorem \ref{th_54} \ref{th_54_c}, we have
\begin{align*}
    \left|F(b) - F(a) - \sum^n_{i=1} f(t_i) \Delta x_i \right| < \varepsilon.
\end{align*}
\end{proof}

\medskip

\begin{theorem}[Integral by Parts]
Suppose $F$ and $G$ are differentiable functions on $[a,b]$, $F' = f \in \mathscr{R}$ and $G' = g \in \mathscr{R}$. Then,
\begin{align*}
    \int^b_a F(x)g(x) \,dx = (FG)\bigg|^b_a - \int^b_a f(x)G(x) \,dx.
\end{align*}
\end{theorem}
\begin{proof}
Let $H(x) = F(x)G(x)$ and apply Theorem \ref{th_516} to its derivative $H'$.
\end{proof}

\medskip






\section{Existence of Antiderivative}

\begin{definition}
Given a continuous function $f: I \to \mathbb{R}$ defined on an interval $I$, we say that $F: I \to \mathbb{R}$ is an antiderivative of $f$, if $F'(x) = f(x)$ for all $x \in I$. (We do not assume $I$ to be open here.)
\end{definition}

\medskip

\begin{proposition}
If $f: I \to \mathbb{R}$ is a continuous function defined on an interval and $F: I \to \mathbb{R}$ is antiderivative of $f$, then $F_1: I \to \mathbb{R}$ is an antiderivative of $f$ if and only if there is a constant $c$ such that $F_1(x) = F(x) + c$ for all $x \in I$.
\end{proposition}

\medskip

\begin{theorem}\label{th_518}
If $f: (a,b) \to \mathbb{R}$ is a continuous function, then there is a function $F: (a,b) \to \mathbb{R}$ such that $F'(x) = f(x)$ for all $x \in (a,b)$.
\end{theorem}

\medskip

We need a lemma to prove this theorem.

\medskip

\begin{lemma}\label{lemma_51}
If $f: [a,b] \to \mathbb{R}$ is a continuous function, then there is a function $F: [a,b] \to \mathbb{R}$ such that $F'(x) = f(x)$ for all $x \in [a,b]$.
\end{lemma}
\begin{proof}
Assume $f > 0$ on $[a,b]$. We define $F(x)$ to be the area under the graph of $f|_{[a,x]}$. If $a \leq x < b$, then for $0 < h < b - x$, $F(x+h) - F(x)$ is the area under the graph of $f|_{[x,x+h]}$. Then we have
\begin{align*}
    h \cdot \inf_{t \in [x,x+h]} f(t) \leq F(x+h) - F(x) \leq h \cdot \sup_{t \in [x,x+h]} f(t).
\end{align*} 
Then, by the continuity of $f$, we have
\begin{align*}
    f(x) \xleftarrow[]{h \to 0^+} \inf_{t \in [x,x+h]} f(t) \leq \frac{F(x+h) - F(x)}{h} \leq \sup_{t \in [x,x+h]} f(t) \xrightarrow[]{h \to 0^+} f(x),
\end{align*}
and hence
\begin{align*}
    F'_+(x) = \lim_{h \to 0^+} \frac{F(x+h) - F(x)}{h} = f(x).
\end{align*}
Similarly, we can have
\begin{align*}
    F'_-(x) = \lim_{h \to 0^-} \frac{F(x+h) - F(x)}{h} = \lim_{h \to 0^+} \frac{F(x-h) - F(x)}{-h} = f(x).
\end{align*}
Hence if $x \in (a,b)$, $F'_+(x) = F'_-(x) = f(x)$, which implies $F'(x) = f(x)$. 

If $f$ is continuous, but not necessarily positive, then there is $M > 0$ such that $f + M$ is positive on $[a,b]$. By what we have proved above, there is $G: [a,b] \to \mathbb{R}$ such that $G'(x) = f(x) + M$ for all $x \in [a,b]$, and hence $F(x) = G(x) - Mx$ satisfies $F'(x) = f(x)$.
\end{proof}

\medskip

\begin{proof}[Proof of Theorem \ref{th_518}]
Let $f: (a,b) \to \mathbb{R}$ be continuous. Fix $x_0 \in (a,b)$ and take a strictly decreasing sequence $\{a_n\}$ with $a_n \to a$ and a strictly increasing sequence $\{b_n\}$ with $b_n \to b$ such that 
\begin{align*}
    x_0 \in [a_1,b_1] \subset [a_2,b_2] \subset \cdots \subset (a,b),
\end{align*}
and clearly 
\begin{align*}
    \bigcup^\infty_{n=1} [a_n,b_n] = (a,b).
\end{align*}
By Lemma \ref{lemma_51}, for each $n$ there is a function $F_n: [a_n,b_n] \to \mathbb{R}$ such that $F_n'(x) = f(x)$ for $x \in (a_n,b_n)$ and we can assume that $F_n(x_0) = 0$, since we could always add a constant to an antiderivative.

If $m > n$, then $F_m = f_n$ on $[a_n,b_n] \subset [a_m,b_m]$. Indeed, $(F_n - F_m)'(x) = f(x) - f(x) = 0$ for $x \in [a_n,b_n]$, so $F_n - F_m$ is constant on $[a_n,b_n]$. Since $(F_n - F_m)(x_0) = 0$, we have $F_n = F_m$ on $[a_n,b_n]$. Define function $F$ as
\begin{align}\label{th_518_equ1}
    F(x) = F_n(x), \,\, x \in [a_n,b_n],
\end{align}
and hence $F$ is well-defined on $(a,b)$. Indeed, each $x \in (a,b)$ belongs to many intervals $[a_n,b_n]$, but if $x \in [a_n,b_n]$ and $x \in [a_m,b_m]$, then $F_n(x) = F_m(x)$ so it does not matter what interval we choose in \eqref{th_518_equ1}. Hence $F'(x) = f(x)$ for all $x \in (a,b)$.
\end{proof}


\medskip



\section{Improper Integral}

\begin{definition}
If $f: (a,b) \to \mathbb{R}$ is continuous and nonnegative, then we define
\begin{align*}
    \int^b_a f(x) \,dx = \lim_{x \to b^-} F(x) - \lim_{x \to a^+} F(x),
\end{align*}
where $F$ is an antiderivative of $f$. Similarly, we can define the integral in the case in which $f \leq 0$. If $f \geq 0$, then $F$ is increasing so the above limit exists. However, if may happen that 
\begin{align*}
    \lim_{x \to b^-} F(x) = + \infty, \,\, \text{and/or} \,\, \lim_{x \to a^+} F(x) = -\infty.
\end{align*}
\end{definition}

\medskip

We can assume for $g \in \mathbb{R}$,
\begin{align*}
    + \infty - g = + \infty, \quad g - (-\infty) = + \infty, \quad + \infty - (- \infty) = + \infty.
\end{align*}
Therefore, if $f \geq 0$, then $\int^b_a f(x) \,dx$ is finite or equal to $+ \infty$. Similarly, if $f \leq 0$, then $\int^b_a f(x) \,dx$ is finite or $- \infty$.

\medskip

\begin{definition}
If $f: [a,b) \to \mathbb{R}$ is continuous, then we define the improper integral by
\begin{align*}
    \int^b_a f(x) \,dx = \lim_{t \to b^-} \int^t_a f(x) \,dx,
\end{align*}
provided the limit exists and is finite. In this case we say that the integral converges (conditionally). If the limit does not exist or if it has infinite value, we say that the integral diverges.

Similarly, we can define the improper integral for $f:(a,b] \to \mathbb{R}$. 
\end{definition}

\medskip

\begin{definition}
If $f:(a,b) \to \mathbb{R}$, then we fix $c \in (a,b)$ and define the improper integral as
\begin{align*}
    \int^b_a f(x) \,dx = \lim_{s \to a^+} \int^c_s f(x) \,dx + \lim_{t \to b^-} \int^t_c f(x) \,dx,
\end{align*}
provided both limits exist and are finite.
\end{definition}

\medskip

\begin{definition}
We say that the improper integral $\int^b_a f(x) \,dx$ converges absolutely if $\int^b_a \left|f(x)\right| \,dx < \infty$.
\end{definition}

\begin{proposition}
If an improper integral converges absolutely, then it converges conditionally.
\end{proposition}

\medskip


\begin{theorem}[Integral Test]
If $f: [1,\infty) \to \mathbb{R}$ is continuous, nonnegative and monotonically decreasing, then
\begin{align*}
    \sum^\infty_{n=1} f(n) < \infty \,\,\,\, \text{if and only if} \,\,\,\, \int^\infty_{1} f(x) \,dx < \infty.
\end{align*}
\end{theorem}
\begin{proof}
Since
\begin{align*}
    \int^n_1 f(x) \,dx = \sum^{n-1}_{k=1} \int^{k+1}_k f(x) \,dx, \quad f(k+1) \leq \int^{k+1}_k f(x) \,dx \leq f(k),
\end{align*}
we have 
\begin{align*}
    f(2) + f(3) + \cdots + f(n) \leq \int^n_1 f(x) \,dx \leq f(1) + f(2) + \cdots + f(n - 1),
\end{align*}
and letting $n \to \infty$ shows the theorem.
\end{proof}

\medskip

\begin{example}
Prove that if $f: [a,b] \to \mathbb{R}$ is continuous, then
\begin{align}\label{ex_51_equ1}
    \lim_{n\to\infty} \sqrt[n]{\int^b_a \left|f(x)\right|^n \,dx} = \sup_{x \in [a,b]} \left|f(x)\right|.
\end{align}
\end{example}
\begin{proof}
Let $M = \sup_{x \in [a,b]} \left|f(x)\right|$. Then we have
\begin{align*}
    \limsup_{n\to\infty} \sqrt[n]{\int^b_a \left|f(x)\right|^n \,dx} \leq \limsup_{n\to\infty} M(b-a)^{1/n}  = M.
\end{align*}
Since $\left|f\right|$ is continuous on compact set $[a,b]$, then it attains $M$ at some point $x_0 \in [a,b]$. By the continuity of $\left|f\right|$, for any $\varepsilon > 0$, there is a neighborhood $I$ of $x_0$ such that $\left|f(x)\right| \geq M - \varepsilon$ for all $x \in I$. Then we have
\begin{align*}
    \liminf_{n\to\infty} \sqrt[n]{\int^b_a \left|f(x)\right|^n \,dx} \geq \liminf_{n\to\infty} \sqrt[n]{\int_I \left|f(x)\right|^n \,dx} \geq \liminf_{n\to\infty} (M - \varepsilon) \left|I\right|^{1/n} = M - \varepsilon.
\end{align*}
Hence for any $\varepsilon > 0$, we have
\begin{align*}
    M - \varepsilon \leq \liminf_{n\to\infty} \sqrt[n]{\int^b_a \left|f(x)\right|^n \,dx} \leq \limsup_{n\to\infty} \sqrt[n]{\int^b_a \left|f(x)\right|^n \,dx} \leq M,
\end{align*}
and thus the upper limit is equal to the lower limit, which implies the equality \eqref{ex_51_equ1}.
\end{proof}

\medskip





\chapter{Sequences and Series of Functions}

\section{Pointwise and Uniform Convergence}

\begin{definition}
Suppose $\{f_n\}, n = 1,2,\cdots$, is sequence of functions defined on a set $E$, and suppose that the sequence of numbers $\{f_n(x)\}$ converges for every $x \in E$. Then we can define a function $f$ by
\begin{align*}
    f(x) = \lim_{n\to\infty} f_n(x), \,\, x \in E.
\end{align*}
Then we say that $\{f_n\}$ converges on $E$ and that $f$ is the limit, or the limit function of $\{f_n\}$. Sometimes we say that $\{f_n\}$ converges to $f$ pointwise on $E$.
\end{definition}

\medskip

In other words, $\{f_n\}$ converges to $f$ pointwise on $E$ if for all $x \in E$, and let $\varepsilon > 0$ be arbitrary, then there is an integer $N > 0$ such that $\left|f_n(x) - f(x)\right| < \varepsilon$ for all $n \geq N$.

\medskip

\begin{definition}
If $\sum f_n(x)$ converges for every $x \in E$, and if we define
\begin{align*}
    f(x) = \sum^\infty_{n=1} f_n(x),  \,\, x \in E,
\end{align*}
the function $f$ is called the sum of the series $\sum f_n$.
\end{definition}

\medskip

The main problem is that if $f_n$ are continuous, or differentiable, or integrable, is it the same for the limit function? For example, to say $f$ is continuous at a point $x$ means
\begin{align*}
    \lim_{t\to x} f(t) = f(x),
\end{align*}
and under these assumptions, it means
\begin{align*}
    \lim_{t\to x} \lim_{n\to\infty} f_n(t) = \lim_{n\to\infty} \lim_{t\to x} f_n(t).
\end{align*}

\medskip

\begin{example}
For $m = 1,2,\cdots, n = 1,2,\cdots$, let
\begin{align*}
    s_{m,n} = \frac{m}{m + n}.
\end{align*}
Then for every fixed $n$, $\lim_{m\to\infty} s_{m,n} = 1$, and hence
\begin{align*}
    \lim_{n\to\infty} \lim_{m\to\infty} s_{m,n} = 1.
\end{align*}
On the other hand, for every fixed $m$, $\lim_{n\to\infty} s_{m,n} = 0$, and hence
\begin{align*}
    \lim_{m\to\infty} \lim_{n\to\infty} s_{m,n} = 0.
\end{align*}
\end{example}

\medskip

\begin{example}
Let
\begin{align*}
    f_n(x) = \frac{x^2}{\left(1 + x^2\right)^n},
\end{align*}
for real $x$ and $n = 0,1,2,\cdots$. Define
\begin{align*}
    f(x) = \sum^\infty_{n=0} f_n(x).
\end{align*}
Since $f_n(0) = 0$, $f(0) = 0$. For $x \neq 0$, since $x^2/(1 + x^2) < 1$ and by Theorem \ref{th_221}, we have
\begin{align*}
    f(x) = \sum^\infty_{n=0} \frac{x^2}{\left(1 + x^2\right)^n} = x^2 \frac{1}{1 - \frac{1}{1 + x^2}} = 1 + x^2.
\end{align*}
Hence, 
\begin{align*}
    f(x) = \begin{cases}
        0, & x = 0, \\
        1 + x^2, & x \neq 0,
    \end{cases}
\end{align*}
so that a convergent sequence of continuous functions may have a discontinuous sum.
\end{example}

\medskip

\begin{definition}
We say a sequence of functions $\{f_n\}, n = 1,2,\cdots$ converges uniformly on $E$ to a function $f$ if for every $\varepsilon > 0$, there is an integer $N > 0$ such that for all $x \in E$, we have $\left|f_n(x) - f(x)\right| \leq \varepsilon$ for all $n \geq N$. 

We say that the series $\sum f_n(x)$ converges uniformly on $E$ if the sequence $\{s_n\}$ of partial sums defined by
\begin{align*}
    s_n(x) = \sum^n_{i=1} f_i(x)
\end{align*}
converges uniformly on $E$.
\end{definition}

\medskip

The {\em Cauchy criterion} for uniform convergence is as follows.

\medskip

\begin{theorem}[Cauchy Criterion]\label{th_61}
The sequence of functions $\{f_n\}$ converges uniformly on $E$ if and only if for every $\varepsilon > 0$, there exists an integer $N > 0$ such that for all $x \in E$, if all $m,n \geq N$, we have $\left|f_n(x) - f_m(x)\right| \leq \varepsilon$.
\end{theorem}
\begin{proof}
Suppose $\{f_n\}$ converges uniformly on $E$, and let $f$ be the limit function. Then for any $\varepsilon > 0$, there is an integer $N > 0$ such that for all $x \in E$ and all $n \geq N$,
\begin{align*}
    \left|f_n(x) - f(x)\right| \leq \frac{\varepsilon}{2},
\end{align*}
and hence if $n,m > N$, we have
\begin{align*}
    \left|f_n(x) - f_m(x)\right| \leq \left|f_n(x) - f(x)\right| + \left|f_m(x) - f(x)\right| < \varepsilon.
\end{align*}
\end{proof}

\medskip

\begin{corollary}\label{coro_611}
Suppose $\lim_{n\to\infty} f_n(x) = f(x)$ for every $x \in E$. Let
\begin{align*}
    M_n = \sup_{x \in E} \left|f_n(x) - f(x)\right|.
\end{align*}
Then $f_n$ converges to $f$ uniformly on $E$ if and only if $M_n \to 0$ as $n \to \infty$.
\end{corollary}

\medskip

\begin{theorem}[Weierstrass $M$-test]\label{th_62}
Suppose $\{f_n\}$ is a sequence of functions defined on $E$ and suppose $\left|f_n(x)\right| \leq M_n$ for all $x \in E$ and all $n \in \mathbb{N}$. Then $\sum f_n$ converges uniformly on $E$ if $\sum M_n$ converges. 
\end{theorem}
\begin{proof}
If $\sum M_n$ converges, then for any $\varepsilon > 0$, we have
\begin{align*}
    \left|\sum^m_{i=n} f_n(x)\right| \leq \sum^m_{i=n} M_n < \varepsilon,
\end{align*}
for all $x \in E$ and for $m,n$ large enough. Uniform convergence follows from Theorem \ref{th_61}.
\end{proof}

\medskip







\section{Uniform Convergence and Continuity}

\begin{theorem}\label{th_63}
Let $f_n: E \to \mathbb{R}$ be a sequence of continuous functions that converges uniformly to $f_: E \to \mathbb{R}$, then $f$ is continuous.
\end{theorem}
\begin{proof}
Since $f_n$ converges uniformly to $f$, for any $\varepsilon > 0$, there is $N > 0$ such that $\left|f_n(x) - f(x)\right| < \varepsilon/3$ for any $x \in E$ if $n \geq N$. Also, by continuity of $f_n$, there is a $\delta > 0$ such that $\left|x - y\right| < \delta$ implies $\left|f_n(x) - f_n(y)\right| < \varepsilon/3$. Then for any $y \in E$ such that $\left|x - y\right| < \delta$ and for all $n \geq N$, we have
\begin{align*}
    \left|f(x) - f(y)\right| < \left|f(x) - f_n(x)\right| + \left|f_n(x) - f_n(y)\right| + \left|f_n(y) - f(y)\right| < \frac{\varepsilon}{3} + \frac{\varepsilon}{3} + \frac{\varepsilon}{3} = \varepsilon,
\end{align*}
which prove that $f$ is continuous at $x$. Since $x$ is arbitrary, $f$ is continuous on $E$.
\end{proof}

\medskip


\begin{corollary}\label{coro_631}
If $f_n: E \to \mathbb{R}$ are continuous functions and the series $f = \sum^\infty_{n=1} f_n$ converges uniformly, then $f: E \to \mathbb{R}$ is continuous.
\end{corollary}

\medskip

Theorem \ref{th_63} is a direct result following the following theorem.

\medskip

\begin{theorem}\label{th_64}
Suppose $f_n \to f$ uniformly on a set $E$ in a metric space. Let $x$ be a limit point of $E$, and suppose $\lim_{t\to x} f_n(t) = A_n$, then $\{A_n\}$ converges and
\begin{align}\label{th_64_equ1}
    \lim_{t\to x} f(t) = \lim_{n\to\infty} A_n.
\end{align}
In other words,
\begin{align*}
    \lim_{t\to x} \lim_{n\to\infty} f_n(t) = \lim_{n\to\infty} \lim_{t\to x} f_n(t).
\end{align*}
\end{theorem}
\begin{proof}
For any $\varepsilon > 0$, since $\{f_n\}$ converges uniformly, there exists $N > 0$ such that for all $n,m > N$ and all $t \in E$, we have $\left|f_n(t) - f_m(t)\right| < \varepsilon$. Letting $t \to x$ implies $\left|A_n - A_m\right| < \varepsilon$ for $n,m > N$, and hence $\{A_n\}$ is a Cauchy sequence and clearly it converges, say to $A$. 

Now choose $n$ large enough such that $\left|f_n(t) - f(t)\right| < \varepsilon/3$ for all $t \in E$, and such that $\left|A_n - A\right| < \varepsilon/3$. For this $n$, there exists a neighborhood $V$ of $x$ such that $\left|f_n(t) - A_n\right| < \varepsilon/3$ for $t \in V \cap E$. Hence, for this $n$ and $t \in V \cap E$, we have
\begin{align*}
    \left|f(t) - A\right| \leq \left|f(t) - f_n(t)\right| + \left|f_n(t) - A_n\right| + \left|A_n - A\right| < \varepsilon,
\end{align*}
which implies \eqref{th_64_equ1}.
\end{proof}

\medskip

With this theorem, we can reformulate Corollary \ref{coro_631} in the terms of exchanging the order of the limit and the sum:
\begin{align}\label{th_64_equ2}
    \lim_{x\to x_0} \sum^\infty_{n=1} f_n(x) = \sum^\infty_{n=1} \lim_{x\to x_0} f_n(x).
\end{align}
Indeed, since $f_n$ are continuous, the right hand side is just $\sum^\infty_{n=1} f_n(x_0) = f(x_0)$. On the other hand,  the left side is equal to $\lim_{x \to x_0} f(x)$. And \eqref{th_64_equ2} is a different representation of $\lim_{x\to x_0} f(x) = f(x_0)$, which is continuity of $f$ at $x_0$.

\medskip

In Theorem \ref{th_229}, we talked about Ratio Test and computed the radius of convergence of the power series in Theorem \ref{th_232}, now we have the following results.

\begin{theorem}
If $\lim_{n\to\infty} \displaystyle \left|\frac{c_{n+1}}{c_n}\right| = \lambda \in [0,\infty]$, then the radius of convergence of the series $\sum^\infty_{n=0} c_n x^n$ equals $R = 1/\lambda$, where we adopt the convention $1/0 = \infty$ and $1/\infty = 0$.
\end{theorem}

\medskip

\begin{remark}
This theorem is easy to use, but it applies to only few power series for which the limit of $\left|c_{n+1}/c_n\right|$ exists. However, Cauchy-Hadamard theorem (Theorem \ref{th_232}) works for {\em all} power series.
\end{remark}

\medskip

\begin{theorem}
Suppose $K$ is compact, and
\begin{enumerate}[label=(\alph*)]
    \item $\{f_n\}$ is a sequence of continuous functions on $K$,
    
    \item $\{f_n\}$ converges pointwise to a continuous function $f$ on $K$,
    
    \item $f_n(x) \geq f_{n+1}(x)$ for all $x \in K$, $n = 1,2,3,\cdots$.
\end{enumerate}
Then $f_n \to f$ uniformly on $K$.
\end{theorem}
\begin{proof}
Let $g_n = f_n - f$. Then $g_n$ is continuous and $g_n \to 0$ pointwise, also $g_n \geq g_{n+1}$. It suffices to prove that $g_n \to 0$ uniformly on $K$.

For any $\varepsilon > 0$. Let $K_n$ be the set of all $x \in K$ for which $g_n(x) \geq \varepsilon$. By Corollary \ref{coro_361}, $K_n$ is closed and hence compact (Theorem \ref{th_116}). Since $g_n \geq g_{n+1}$, we have $K_n \supset K_{n+1}$. Fix $x \in K$, and since $g_n(x) \to 0$, then $x \notin K_n$ for $n$ large enough. Hence $x \notin \bigcup K_n$. In other words, $\bigcup K_n = \emptyset$, and hence $k_N = \emptyset$ for some $N$. Thus $0 \leq g_n(x) < \varepsilon$ for all $x \in K$ and all $n \geq N$, and the theorem follows. 
\end{proof}

\medskip

\begin{remark}
The compactness is needed here. For example, if
\begin{align*}
    f_n(x) = \frac{1}{nx + 1}, \,\, 0 < x < 1, n = 1,2,3,\cdots,
\end{align*}
then $f_n(x) \to 0$ monotonically in $(0,1)$, but the convergence is not uniform.
\end{remark}

\medskip

\begin{definition}
If $X$ is a metric space, $C(X)$ will denote the set of all complex valued, continuous, bounded functions on domain $X$.
\end{definition}

\medskip

We associate with each $f \in C(X)$ its {\em supremum norm}:
\begin{align*}
    \left\|f\right\| = \sup_{x \in X} \left|f(x)\right|.
\end{align*}
Since $f$ is assumed to be bounded, $\left\|f\right\| < \infty$. It is obvious that $\left\|f\right\| = 0$ if and only if $f = 0$. Also, we have
\begin{align*}
    \left\|f + g\right\| \leq \left\|f\right\| + \left\|g\right\|.
\end{align*}
If we define the distance between $f$ and $g$ by $d_\infty(f,g) = \left\|f - g\right\|$, it follows from Definition \ref{def_19}, $(C(X),d)$ is a {\em metric space}.

\medskip

\begin{theorem}
The above metric makes $C(X)$ into a complete metric space.
\end{theorem}
\begin{proof}
Let $\{f_n\}$ be a Cauchy sequence in $C(X)$. Then for any $\varepsilon > 0$, there is a $N > 0$ such that $\left\|f_n - f_m\right\| < \varepsilon$ if $n,m \geq N$. Then $f_n$ converges uniformly to a function $f$ on $X$. By Theorem \ref{th_63}, $f$ is continuous. Also, $f$ is bounded, since for $\varepsilon = 1$, there is $n$ such that $\left\|f - f_n\right\| < 1$ for all $x \in X$ and $f_n$ is bounded. Thus, $f \in C(X)$, and hence $C(X)$ is complete.
\end{proof}


\medskip




\section{Uniform Convergence and Integration}

\begin{theorem}\label{th_68}
Let $\alpha$ be monotonically increasing on $[a,b]$. Suppose $f_n \in \mathscr{R}(\alpha)$ on $[a,b]$ and suppose $f_n \to f$ uniformly on $[a,b]$. Then $f \in \mathscr{R}(\alpha)$, and
\begin{align*}
    \int^b_a f \,d\alpha = \lim_{n\to\infty} \int^b_a f_n \,d\alpha.
\end{align*}
The existence of limit is in the conclusion.
\end{theorem}
\begin{proof}
It suffices to prove for real functions $f_n$. Let $\varepsilon_n = \sup \left|f_n(x) - f(x)\right|, x \in [a,b]$. Then,
\begin{align*}
    f_n - \varepsilon_n \leq f \leq f_n + \varepsilon_n,
\end{align*}
and hence
\begin{align}\label{th_68_equ1}
    \int^b_a (f_n - \varepsilon_n) \,d\alpha \leq \underline{\int}^b_a f \,d\alpha \leq \overline{\int}^b_a f \,d\alpha \leq \int^b_a (f_n + \varepsilon_n) \,d\alpha.
\end{align}
Then we have
\begin{align*}
    0 \leq \overline{\int}^b_a f \,d\alpha - \underline{\int}^b_a f \,d\alpha \leq 2 \varepsilon_n [\alpha(b) - \alpha(a)],
\end{align*}
and since $\varepsilon_n \to 0$ as $n \to \infty$ (Corollary \ref{coro_611}), the upper and lower integrals of $f$ are equal. Hence $f \in \mathscr{R}(\alpha)$, and \eqref{th_68_equ1} implies
that 
\begin{align*}
    \left|\int^b_a f \,d\alpha - \int^b_a f_n \,d\alpha \right| \leq \varepsilon_n [\alpha(b) - \alpha(a)],
\end{align*}
and the theorem follows.
\end{proof}

\medskip

\begin{theorem}
If $f_n \in \mathscr{R}(\alpha)$ on $[a,b]$ and if the series
\begin{align*}
    f(x) = \sum^\infty_{n=1} f_n(x), \,\, x \in [a,b]
\end{align*}
converges uniformly on $[a,b]$, then
\begin{align*}
    \int^b_a f \,d\alpha = \sum^\infty_{n=1} \int^b_a f_n \,d\alpha.
\end{align*}
In other words, the order of the sum and integral can be changed.
\end{theorem}

\medskip



\section{Uniform Convergence and Differentiation}

\begin{theorem}\label{th_610}
Suppose $\{f_n\}$ is a sequence of differentiable functions\footnote{We do not assume that the derivatives of $f_n$ are continuous, the result is still true if we only assume the differentiability of $f_n$. However, the assumption of continuous differentiability is sufficient for most of the applications.}on $[a,b]$ and such that $\{f_n(x_0)\}$ converges for some point $x_0 \in [a,b]$. If $\{f_n'\}$ converges uniformly on $[a,b]$, then $\{f_n\}$ converges uniformly on $[a,b]$ to a function $f$, and 
\begin{align*}
    f'(x) = \lim_{n\to\infty} f_n'(x),
\end{align*}
for all $x \in [a,b]$.
\end{theorem}
\begin{proof}
For any $\varepsilon > 0$. Choose $N > 0$ such that for all $n,m \geq N$, we have
\begin{align}\label{th_610_equ1}
    \left|f_n(x_0) - f_m(x_0)\right| < \frac{\varepsilon}{2},
\end{align}
and
\begin{align}\label{th_610_equ2}
    \left|f_n'(t) - f_m'(t)\right| < \frac{\varepsilon}{2(b - a)}, \,\, t \in [a,b].
\end{align}
Applying the Mean Value theorem \ref{th_47} to $f_n - f_m$, and \eqref{th_610_equ2} implies that 
\begin{align}\label{th_610_equ3}
    \left|f_n(x) - f_m(x) - f_n(t) + f_m(t)\right| < \frac{\varepsilon \left|x - t\right|}{2(b - a)} < \frac{\varepsilon}{2},
\end{align}
for any $x,t \in [a,b]$, if $n,m \geq N$. Also, by \eqref{th_610_equ1} and \eqref{th_610_equ3}, we have
\begin{align*}
    \left|f_n(x) - f_m(x)\right| \leq \left|f_n(x) - f_m(x) - f_n(x_0) + f_m(x_0)\right| + \left|f_n(x_0) - f_m(x_0)\right| < \varepsilon,
\end{align*}
for any $x \in [a,b]$ and $n,m \geq N$. Hence $\{f_n\}$ converges uniformly on $[a,b]$.

Let $f(x) = \lim_{n\to\infty} f_n(x)$ for all $x \in [a,b]$. For a fixed point $x \in [a,b]$, we define
\begin{align}\label{th_610_equ4}
    \phi_n(t) = \frac{f_n(t) - f_n(x)}{t - x}, \quad \phi(t) = \frac{f(t) - f(x)}{t - x},
\end{align}
for $t \in [a,b], t \neq x$. Clearly, $\lim_{t\to x} \phi_n(t) = f_n'(x)$ for $n = 1,2,3,\cdots$. And \eqref{th_610_equ3} implies
\begin{align}
    \left|\phi_n(t) - \phi_m(t)\right| < \frac{\varepsilon}{2(b - a)},
\end{align}
for $n,m \geq N$, and hence $\{\phi_n\}$ converges uniformly for $t \neq x$. Since $\{f_n\}$ converges uniformly to $f$, we concluded from \eqref{th_610_equ4} that $\phi_n \to \phi$ uniformly for $t \in [a,b], t \neq x$. By Theorem \ref{th_64}, we have
\begin{align*}
    \lim_{t\to x} \phi(t) = \lim_{t\to x} \lim_{n\to\infty} \phi_n(t) = \lim_{n\to\infty} \lim_{t\to x} \phi_n(t) = \lim_{n\to\infty} f_n'(x),
\end{align*}
and hence
\begin{align*}
    f'(x) = \lim_{t\to x} \phi(t) = \lim_{n\to\infty} f_n'(x).
\end{align*}
\end{proof}

\medskip

\begin{corollary}
If $f_n: (a,b) \to \mathbb{R}$ is a sequence of continuously differentiable functions that converges at some point $x_0 \in (a,b)$ and if the sequence $\{f_n'\}$ converges uniformly on every interval $[\alpha,\beta] \subset (a,b)$, then $\{f_n\}$ converges uniformly to a continuously differentiable function $f$ on every interval $[\alpha,\beta] \subset (a,b)$ and
\begin{align*}
    f'(x) = \lim_{n\to\infty} f_n'(x),
\end{align*}
for all $x \in (a,b)$.
\end{corollary}

\medskip

Applying this corollary to a sequence of partial sums we obtain the following result.

\medskip

\begin{theorem}
If $f_n: (a,b) \to \mathbb{R}$ is a sequence of continuously differentiable functions that the series $\sum^\infty_{n=1} f_n(x_0)$ converges at some point $x_0 \in (a,b)$ and if the sequence $\sum^\infty_{n=1} f_n'$ converges uniformly on every interval $[\alpha,\beta] \subset (a,b)$, then $\sum^\infty_{n=1} f_n$ converges uniformly to a continuously differentiable function $f$ on every interval $[\alpha,\beta] \subset (a,b)$ and
\begin{align*}
    \left(\sum^\infty_{n=1} f_n(x)\right)' = \sum^\infty_{n=1} f_n'(x),
\end{align*}
for all $x \in (a,b)$.
\end{theorem}

\medskip

Here is another easy but useful result following an induction argument.

\medskip

\begin{theorem}\label{th_612}
If $f_n \in C^\infty(a,b)$\footnote{We say that $f \in C^r(\Omega)$, where $r \geq 1$ is an integer, if all derivative of order less than or equal to $r$ are continuous. Also, $f \in C^\infty(\Omega)$ if $f$ is infinitely times differentiable.}and for every $k = 0,1,2,\cdots$, the series $\sum^\infty_{n=1} f_n^{(k)}$ converges uniformly on every interval $[\alpha,\beta] \subset (a,b)$, then $f(x) = \sum^\infty_{n=1} f_n(x) \in C^\infty(a,b)$ and
\begin{align*}
    \left(\sum^\infty_{n=1} f_n(x)\right)^{(k)} = \sum^\infty_{n=1} f_n^{(k)}(x),
\end{align*}
for all $x \in (a,b)$ and all $k \in \mathbb{N}$.
\end{theorem}

\medskip

It follows from $p$-Series Test (Theorem \ref{th_223}) that 
\begin{align}\label{th_613_equ1}
    \zeta(x) = \sum^\infty_{n=1} \frac{1}{n^x}
\end{align}
is well-defined for $x > 1$. Next we discuss an application of Theorem \ref{th_612}.

\medskip

\begin{theorem}
$\zeta \in C^\infty(1,\infty)$.
\end{theorem}
\begin{proof}
In order to prove the convergence of derivatives of \eqref{th_613_equ1}, we need the following lemma.

\medskip

\begin{lemma}\label{lemma_61}
If $a > 1$ and $k \geq 1$, then
\begin{align*}
    \sum^\infty_{k=2} \frac{(\log n)^k}{n^a} < \infty.
\end{align*}
\end{lemma}
\begin{proof}
Let $\varepsilon > 0$ be such that $a - \varepsilon > 1$. We claim that
\begin{align}\label{th_613_equ2}
    \lim_{n\to\infty} \frac{(\log n)^k}{n^\varepsilon} = 0.
\end{align}
Indeed, it suffices to show that
\begin{align*}
    \lim_{x\to\infty} \frac{(\log x)^k}{x^\varepsilon} = \lim_{x\to\infty} \left(\frac{\log x}{x^{\varepsilon/k}} \right)^k = 0,
\end{align*}
and this follows from L'Hospital's Rule, 
\begin{align*}
    \lim_{x\to\infty} \frac{\log x}{x^{\varepsilon/k}} = \lim_{x\to\infty} \frac{1/x}{\varepsilon/k x^{\varepsilon/k - 1}} = \lim_{x\to\infty} \frac{1}{\varepsilon/k x^{\varepsilon/k}} = 0.
\end{align*}
This implies \eqref{th_613_equ2}. And hence \eqref{th_613_equ2} is bounded, say by $M > 0$. Then by Theorem \ref{th_223}, we have
\begin{align*}
    \sum^\infty_{k=2} \frac{(\log n)^k}{n^a} = \sum^\infty_{k=2} \frac{(\log n)^k}{n^\varepsilon} \frac{1}{n^{a - \varepsilon}} \leq M \sum^\infty_{k=2} \frac{1}{n^{a - \varepsilon}} < \infty,
\end{align*}
since $a - \varepsilon > 1$.
\end{proof}

\medskip

Now we resume the proof of the theorem. By Theorem \ref{th_612}, it suffices to prove that for every $k \geq 1$ and $a > 1$, the series
\begin{align}\label{th_613_equ3}
    \sum^\infty_{n=1} \left(\frac{1}{n^x}\right)^{(k)}
\end{align}
converges uniformly on $[a,\infty)$. For $k = 0$, since $a > 1$, we have
\begin{align*}
    \sum^\infty_{n=1} \left|\frac{1}{n^x}\right| \leq \sum^\infty_{n=1} \left|\frac{1}{n^a}\right| < \infty.
\end{align*}
Hence the series $\zeta(x) = \sum^\infty_{n=1} \frac{1}{n^x}$ converges uniformly on $[a,\infty)$ by $M$-test (Theorem \ref{th_62}). For $k \geq 1$, we have
\begin{align*}
    \sum^\infty_{n=1} \left(\frac{1}{n^x}\right)^{(k)} = \sum^\infty_{n=1} \frac{(- \log n)^k}{n^x},
\end{align*}
and by Lemma \ref{lemma_61}, we have
\begin{align*}
    \sum^\infty_{n=1} \left|\left(\frac{1}{n^x}\right)^{(k)}\right| \leq \sum^\infty_{n=1} \frac{(\log n)^k}{n^x} < \infty,
\end{align*}
and hence the series \eqref{th_613_equ3} converges uniformly on $[a,\infty)$ by $M$-test for all $a > 1$. Thus, by Theorem \ref{th_612}, $\zeta \in C^\infty(1,\infty)$.
\end{proof}

\medskip



\section{Equicontinuous Families of Functions}

\begin{definition}
Let $\{f_n\}$ be a sequence of functions defined on a set $E$. We say $\{f_n\}$ is pointwise bounded on $E$ if the sequence $\{f_n(x)\}$ is bounded for every $x \in E$, that is, if there exists a finite-valued function $\phi$ defined on $E$ such that $\left|f_n(x)\right| < \phi(x)$ for every $x \in E$ and $n = 1,2,3,\cdots$. 

We say $\{f_n\}$ is uniformly bounded on $E$ if there exists a number $M$ such that $\left|f_n(x)\right| < M$ for every $x \in E$ and $n = 1,2,3,\cdots$. 
\end{definition}

\medskip

\begin{example}
Let $f_n(x) = \sin nx, 0 \leq x \leq 2\pi, n = 1,2,3,\cdots$. Suppose there exists a sequence $\{n_k\}$ such that $\{\sin n_kx\}$ converges for every $x \in [0,2\pi]$. In this case, we have
\begin{align*}
    \lim_{k\to\infty} \left(\sin n_kx - \sin n_{k+1}x\right) = 0,
\end{align*}
and hence
\begin{align*}
    \lim_{k\to\infty} \left(\sin n_kx - \sin n_{k+1}x\right)^2 = 0,
\end{align*}
for every $x \in [0,2\pi]$. By Lebesgue's dominated convergence theorem\footnote{{\bf Lebesgue's dominated convergence theorem:} Suppose that $\{f_n\}$ is a sequence of complex valued functions on $X$ defined a.e. If $f(x) = \lim_{n\to\infty} f_n(x)$ for almost all $x \in X$, and if there is a function $g \in L^1(\mu)$ such that $\left|f_n(x)\right| \leq g(x)$ for all $n = 1,2,3,\cdots$, then $f \in L^1(\mu)$,
\begin{align*}
    \lim_{n\to\infty} \int_X \left|f - f_n\right|\,d\mu = 0,\quad \text{and} \quad \lim_{n\to\infty} \int_X f_n\,d\mu = \int_X f\,d\mu.
\end{align*}}(which will be discussed later), we have
\begin{align*}
    \lim_{k\to\infty} \int^{2\pi}_0 \left(\sin n_kx - \sin n_{k+1}x\right)^2 = 0.
\end{align*}
However, we have
\begin{align*}
    \lim_{k\to\infty} \int^{2\pi}_0 \left(\sin n_kx - \sin n_{k+1}x\right)^2 = 2\pi,
\end{align*}
which is contradiction.
\end{example}

\medskip

This is an example showing that if $\{f_n\}$ is a uniformly bounded on a compact set $E$, there may not exist a subsequence which converges pointwise on $E$. Another question is whether every convergent sequence contains a
uniformly convergent subsequence. Our next example will show that this need not be so, even if the sequence is uniformly bounded on a compact set.

\medskip

\begin{example}\label{ex_64}
Let
\begin{align*}
    f_n(x) = \frac{x^2}{x^2 + (1 - nx)^2}, \quad 0 \leq x \leq 1, n = 1,2,3,\cdots.
\end{align*}
Then $\left|f_n(x)\right| \leq 1$, so that $\{f_n\}$ is uniformly bounded on $[0,1]$. Also, $\lim_{n\to\infty} f_n(x) = 0$ for every $x \in [0,1]$. However, $f_n(1/n) = 1$, and thus no subsequence can converges uniformly on $[0,1]$.
\end{example}

\medskip

\begin{example}
Another example is for $x \in [0,1]$,
\begin{align*}
    f_n(x) = \begin{cases}
        nx - 1, & \frac{1}{n} \leq x \leq \frac{2}{n}, \\
        -nx + 3, & \frac{2}{n} < x \leq \frac{3}{n}, \\
        0, & \text{otherwise}.
    \end{cases}
\end{align*}
Clearly $f_n$ converges to $0$ pointwise, but not uniformly, since $f_n(2/n) = 1$ for all $n$.
\end{example}

\medskip

\begin{definition}
A family $\mathscr{F}$ of complex functions $f$ defined on a set $E$ in a metric space $X$ is said to be equicontinuous on $E$ if for every $\varepsilon > 0$, there exists a $\delta > 0$ such that $\left|f(x) - f(y)\right| < \varepsilon$ whenever $d_X(x,y) < \delta$ for $x,y \in E$ and for every $f \in \mathscr{F}$.
\end{definition}

\begin{remark}
Clearly, every member of an equicontinuous family is uniformly continuous. The sequence in Example \ref{ex_64} is not equicontinuous.
\end{remark}

\medskip

\begin{theorem}\label{th_614}
If $\{f_n\}$ is a pointwise bounded sequence of complex functions on a countable set $E$, then $\{f_n\}$ has a subsequence $\{f_{n_k}\}$ such that $\{f_{n_k}(x)\}$ converges for every $x \in E$.
\end{theorem}
\begin{proof}
Let $\{x_i\}, i = 1,2,3,\cdots$, be the points of $E$. Since $\{f_n(x_1)\}$ is bounded, there exists a subsequence $\{f_{1,k}\}$ such that $\{f_{1,k}(x_1)\}$ converges as $k \to \infty$.

Now consider sequences $S_1, S_2, S_3, \cdots$, where
\begin{align*}
    S_1: f_{1,1}, f_{1,2}, f_{1,3}, \cdots \\
    S_2: f_{2,1}, f_{2,2}, f_{2,3}, \cdots \\
    S_3: f_{3,1}, f_{3,2}, f_{3,3}, \cdots \\
    \cdots
\end{align*}
and these sequences have the following properties:
\begin{enumerate}[label=(\alph*)]
    \item $S_n$ is a subsequence of $S_{n-1}$ for $n = 2,3,4,\cdots$.
    
    \item $\{f_{n,k}(x_n)\}$ converges as $k \to \infty$, since the boundeness of $\{f_n(x_n)\}$ makes it possible to choose $S_n$ in this way.
    
\end{enumerate}
Now we choose the diagonal of the array, that is, we consider the sequence
\begin{align*}
    S: f_{1,1}, f_{2,2}, f_{3,3}, \cdots,
\end{align*}
Clearly $S$ is a subsequence of $S_n, n = 1,2,3,\cdots$. Hence $\{f_{n,n}(x_i)\}$ converges as $n \to \infty$ for every $x_i \in E$.
\end{proof}

\medskip

\begin{theorem}
If $K$ is a compact metric space, if $f_n \in C(X)$ for $n = 1,2,3,\cdots$ and if $\{f_n\}$ converges uniformly on $K$, then $\{f_n\}$ is equicontinuous on $K$.
\end{theorem}
\begin{proof}
For any $\varepsilon > 0$. Since $\{f_n\}$ converges uniformly, then there is an integer $N > 0$ such that $\left|f_n(x) - f_N(x)\right| < \varepsilon$ for all $x \in K$ if $n \geq N$. Since continuous functions are uniformly continuous on a compact set, there is a $\delta_i > 0$ such that $\left|f_i(x) - f_i(y)\right| < \varepsilon$ if $d(x,y) < \delta_i$ for $1 \leq i \leq N$. Now let $\delta = \min\{\delta_1, \cdots, \delta_N\}$, then for this $\delta$, 
\begin{align}\label{th_615_equ1}
    \left|f_i(x) - f_i(y)\right| < \varepsilon, \,\, i = 1,2,\cdots,N,
\end{align}
if $d(x,y) < \delta$ for all $x,y \in K$. Now if $n > N$ and $d(x,y) < \delta$, we have
\begin{align*}
    \left|f_n(x) - f_n(y)\right| \leq \left|f_n(x) - f_N(x)\right| + \left|f_N(x) - f_N(y)\right| + \left|f_N(y) - f_n(y)\right| < 3\varepsilon,
\end{align*}
and along with \eqref{th_615_equ1}, the theorem follows.
\end{proof}

\medskip

Before talking about Arzelà-Ascoli theorem, we need to discuss some material from the theory of the metric spaces.

\medskip

\begin{definition}
$C(K,\mathbb{R})$ is the metric space of all continuous functions $f: E \to \mathbb{R}$ with the metric
\begin{align*}
    d_{\infty}(f,g) = \sup_{x \in E} \left|f(x) - g(x)\right|.
\end{align*}
\end{definition}

\begin{remark}
The compactness of $K$ is needed. Indeed, if $f(x) = 0$ and $g(x) = 0$ for $x \in \mathbb{R}$, clearly $f,g \in C(\mathbb{R},\mathbb{R})$, but 
\begin{align*}
    d_{\infty}(f,g) = \sup_{x \in \mathbb{R}} \left|x\right| = \infty,
\end{align*}
so $d_{\infty}$ is not a metric in $C(\mathbb{R},\mathbb{R})$.
\end{remark}

\medskip

\begin{lemma}\label{lemma_62}
Suppose $f_n, f \in C(K,\mathbb{R}), n = 1,2,3,\cdots$. Then $f_n$ pointwise converges to $f$ in $C(K,\mathbb{R})$ if and only if $f_n$ uniformly converges to $f$ in $C(K,\mathbb{R})$.
\end{lemma}
\begin{proof}
Suppose $f_n \to f$ pointwise in $C(K,\mathbb{R})$, then $d_{\infty}(f_n,f) \to 0$ as $n \to \infty$. For any $\varepsilon > 0$, there is an integer $N > 0$ such that for all $n \geq N$, $d_{\infty}(f_n,f) < \varepsilon$. This implies that for this $\varepsilon$ and $N$, for any $x \in E$ and $n \geq N$, $\left|f_n(x) - f(x)\right| \leq d_{\infty}(f_n,f) < \varepsilon$, which proves the uniform convergence.

Conversely, if $f_n$ uniformly converges to $f$ in $C(K,\mathbb{R})$, then for any $\varepsilon > 0$, there is an integer $N > 0$ such that for any $x \in E$ and $n \geq N$, $\left|f_n(x) - f(x)\right| < \varepsilon$. Taking supremum on the left hand side over all $x \in E$ implies $d_\infty(f_n,f) < \varepsilon$ for all $n \geq N$, which proves the pointwise convergence.
\end{proof}

\medskip

\begin{theorem}\label{th_616}
Suppose $K \subset X$ is compact. Then $C(K,\mathbb{R})$ is a complete metric space.
\end{theorem}
\begin{proof}
Suppose $\{f_n\}$ is a Cauchy seuqnece in $C(K,\mathbb{R})$, then for any $\varepsilon > 0$, there is an integer $N > 0$ such that $d_{\infty}(f_n,f_m) < \varepsilon$ if $n,m \geq N$. Then, for all $x \in E$, 
\begin{align}\label{th_616_equ1}
    \left|f_n(x) - f_m(x)\right| \leq d_{\infty}(f_n,f_m) < \varepsilon,
\end{align}
if $n,m \geq N$. Hence $\{f_n(x)\}$ is a Cauchy sequence for every $x \in E$ and hence is convergent. Denote the limit by 
\begin{align*}
    f(x) = \lim_{n\to\infty} f_n(x).
\end{align*}
Letting $m \to \infty$ and fix $n$ in \eqref{th_616_equ1} implies for $\varepsilon$ as before, there is $N > 0$ such that for all $n \geq N$ and all $x \in E$, we have
\begin{align*}
    \left|f_n(x) - f(x)\right| < \varepsilon,
\end{align*}
and hence $f_n$ uniformly converges to $f$. By  Theorem \ref{th_63}, $f$ is continuous and hence $f \in C(K,\mathbb{R})$. Thus $C(K,\mathbb{R})$ is complete.
\end{proof}



\medskip




\section{Arzelà-Ascoli Theorem}

\begin{theorem}[Arzelà-Ascoli]\label{th_617}
Let $K \subset X$ be compact, then the set $\mathcal{F} \subset C(K,\mathbb{R}^n)$ is compact if and only if it is bounded, closed and equicontinuous.
\end{theorem}
\begin{proof}
Suppose $\mathcal{F}$ is compact, then $\mathcal{F}$ is closed and bounded. It remains to prove that $\mathcal{F}$ is equicontinuous. Suppose $\mathcal{F}$ is not equicontinuous, then there exists $\varepsilon > 0$ and for all $\delta > 0$, there is $f \in \mathcal{F}$ and $x, y \in E$ such that $d(x,y) < \delta$ but $\left|f(x) - f(y)\right| \geq \varepsilon$. 

In particular, let $\delta = 1/n, n = 1,2,3,\cdots$, we can find $f_n \in \mathcal{F}$ and $x_n, y_n \in E$ such that
\begin{align}\label{th_617_equ1}
    d(x_n,y_n) < 1/n, \quad \text{and} \quad \left|f_n(x_n) - f_n(y_n)\right| \geq \varepsilon.
\end{align}
Since $\mathcal{F}$ is compact, by Theorem \ref{th_616}, $\{f_n\}$ has a convergent subsequence $\{f_{n_k}\}$ uniformly converging to $f$. By Theorem \ref{th_63}, $f$ is also continuous on $E$, then as $n_k \to \infty$, we have
\begin{align*}
    \left|f_{n_k}(x_{n_k}) - f_{n_k}(y_{n_k})\right| \leq \underbrace{\left|f_{n_k}(x_{n_k}) - f(x_{n_k})\right|}_{\to\,0} + \underbrace{\left|f(x_{n_k}) - f(y_{n_k})\right|}_{\to\,0} + \underbrace{\left|f(y_{n_k}) - f_{n_k}(y_{n_k})\right|}_{\to\,0} \to 0,
\end{align*}
which is a contradiction to \eqref{th_617_equ1}. Hence $\mathcal{F}$ is equicontinuous.

Conversely, suppose $\mathcal{F}$ is bounded, closed and equicontinuous. Let $\{f_n\}$ be a sequence in $\mathcal{F}$, and it suffices to show that $\{f_n\}$ has uniformly convergent subsequence in $\mathcal{F}$. Since $K$ is compact, $K$ has a countable dense subset,\footnote{{\em A compact set $K$ has a countable dense subset.} For any $\varepsilon > 0$, there is an integer $N > 0$ such that $1/N < \varepsilon$, let $U_n = \left\{B(x,1/n) \,:\, x \in K, n \geq N\right\}$, and then $U_n$ is an open covering of $K$, and hence there is a finite subcovering $F_n = \bigcup^N_{k=1} B(x_{n_k},1/n_k)$ of $K$. Hence $A = \{x_{n_k}\}^N_{k=1}$ is a countable set. To see that $A$ is dense in $K$, for any $y \in K$, there is a $x_{n_j} \in A$ such that $y \in B(x_{n_j},1/n_j)$ and $1/n_j < 1/N < \varepsilon$. Hence $y \in A \cap B(x_{n_j}, \varepsilon)$, and thus $A$ is dense.}which can be expressed as $\{x_1, x_2, x_3, \cdots\}$. Next we can use the diagonal method as in Theorem \ref{th_614} to show there is a subsequence that converges to $f$, and thus $K$ is compact.
\end{proof}

\medskip

\begin{proof}[Second Proof of Theorem \ref{th_617}]\cite{4}
Now we provide another approach to prove the Arzelà-Ascoli theorem with only using basic definitions. To see that $\mathcal{F}$ is equicontinuous, for any $\varepsilon > 0$, since $\mathcal{F}$ is compact, there are $f_1, \cdots, f_N \in C(K,\mathbb{R}^n)$ such that
\begin{align*}
    \mathcal{F} \subset B(f_1,\varepsilon/3) \cup \cdots \cup B(f_N,\varepsilon/3).
\end{align*}
Also, each $f_N$ is uniformly continuous since $K$ is compact, and hence there is a $\delta > 0$ such that for all $1 \leq n \leq N$ and all $x, y \in E$, 
\begin{align*}
    \left|f_n(x) - f_n(y)\right| < \frac{\varepsilon}{3},
\end{align*}
if $d(x,y) < \delta$. For any $f \in \mathcal{F}$, there is $1 \leq j \leq N$ such that $f \in B(f_j,\varepsilon/3)$, and hence $d_\infty(f,f_j) < \varepsilon/3$. Now, for any $x, y \in K$, if $d(x,y) < \delta$, we have
\begin{align*}
    \left|f(x) - f(y)\right| & \leq \left|f(x) - f_j(x)\right| + \left|f_j(x) - f_j(y)\right| + \left|f_j(y) - f(y)\right| \\
    & \leq d_\infty(f,f_j) + \left|f_j(x) - f_j(y)\right| + d_\infty(f,f_j) < \varepsilon,
\end{align*}
which proves that $\mathcal{F}$ is equicontinuous.

Conversely, it suffices to show that $\mathcal{F}$ is complete and totally bounded (Theorem \ref{th_125}). Let $\{f_n\}$ be a Cauchy sequence in $\mathcal{F}$, and since $C(K,\mathbb{R}^n)$ is complete, $f_n$ converges to some $f \in C(K,\mathbb{R}^n)$. Since $\mathcal{F}$ is closed, $f \in \mathcal{F}$ and hence $\mathcal{F}$ is complete.

It remains to prove that $\mathcal{F}$ is totally bounded. For any $\varepsilon > 0$, since $\mathcal{F}$ is equicontinuous, there is a $\delta > 0$ such that for all $x,y \in K$ and all $f \in \mathcal{F}$, 
\begin{align*}
    \left|f(x) - f(y)\right| < \frac{\varepsilon}{4},
\end{align*}
if $d(x,y) < \delta$. Then the collection $\{B(x,\delta)\}_{x \in K}$ is an open covering of $K$, and since $K$ is compact, there are $x_1, \cdots, x_N$ such that
\begin{align*}
    K \subset \bigcup^N_{i=1} B(x_i,\delta).
\end{align*}
Since $\mathcal{F}$ is bounded, the set $F = \{f(x_i) \,:\, 1 \leq i \leq N, f \in \mathcal{F}\}$ is bounded and hence $F$ is totally bounded. Then there are points $y_1, \cdots, y_M$ such that
\begin{align*}
    F \subset \bigcup^M_{i=1} B\left(y_i, \frac{\varepsilon}{4}\right).
\end{align*}
For any map $\varphi: \{1,\cdots,N\} \to \{1,\cdots,M\}$, deifne
\begin{align*}
    \mathcal{F}_{\varphi} = \left\{f \in \mathcal{F} \,:\, f(x_i) \in B\left(y_{\varphi(i)}, \frac{\varepsilon}{4}\right), i = 1,\cdots,N \right\}.
\end{align*}
Note there are only finitely many sets $\mathcal{F}_{\varphi}$, and every $f \in \mathcal{F}$ belongs to one of the sets $\mathcal{F}_{\varphi}$. We claim that the diameter of $\mathcal{F}_{\varphi}$ is finite. Indeed, take $f, g \in \mathcal{F}_{\varphi}$ and $x \in K$. Then $x \in B(x_i,\delta)$ for some $i$ and 
\begin{align*}
    \left|f(x) - g(x)\right| \leq \left|f(x) - f(x_i)\right| + \left|f(x_i) - y_{\varphi(i)}\right| + \left|y_{\varphi(i)} - g(x_i)\right| + \left|g(x_i) - g(x)\right| < 4 \cdot \frac{\varepsilon}{4} = \varepsilon.
\end{align*}
Hence $d_{\infty}(f,g) < \varepsilon$ and hence $\mathcal{F}$ can be covered by finitely many sets of diameter less than $\varepsilon$. Thus $\mathcal{F}$ is totally bounded and the compactness follows.
\end{proof}

\medskip

Next theorem is a result following Theorem \ref{th_617}, and will be used frequently. We also call it Arzelà-Ascoli theorem.

\medskip

\begin{theorem}[Arzelà-Ascoli]\label{th_618}
If $K$ is compact, $f_n \in \mathcal{F} \subset C(K,\mathbb{R}^n)$ and $\{f_n\}$ is pointwise bounded and equicontinuous on $K$, then
\begin{enumerate}[label=(\alph*)]
    \item $\{f_n\}$ is uniformly bounded on $K$. \label{th_618_a}
    
    \item $\{f_n\}$ contains a uniformly convergent subsequence. \label{th_618_b}
\end{enumerate}
\end{theorem}
\begin{proof}
Since $\{f_n\}$ is equicontinuous, for every $\varepsilon > 0$, there exists $\delta > 0$ such that for all $n$ , $\left|f_n(x) - f_n(y)\right| < \varepsilon$ if $d(x,y) < \delta$ for all $x,y \in K$. Clearly there is a open covering of $K$ by balls of radius $\delta$, that is
\begin{align*}
    K \subset \bigcup_{x \in K} B(x,\delta).
\end{align*}
Since $K$ is compact, there is a finite covering such that
\begin{align*}
    K \subset \bigcup^N_{i=1} B(x_i,\delta), \quad x_i \in K.
\end{align*}
Since $\{f_n\}$ is pointwise bounded, there exists $M_i$ such that $\left|f_n(x_i)\right| < M_i$ for all $1 \leq i \leq N$. Let $M = \max \{M_1, \cdots, M_N\}$, then for any $x \in K$, $x \in B(x_j,\delta)$ for some $j$ and with $d(x,x_j) < \delta$, we have $\left|f_n(x) - f_n(x_j)\right| < \varepsilon$, and this implies
\begin{align*}
    \left|f_n(x)\right| < \left|f_n(x_j)\right| + \varepsilon < M + \varepsilon.
\end{align*}
This proves \ref{th_618_a}.

Let $E$ be a countable dense subset of $K$. By Theorem \ref{th_614}, $\{f_n\}$ ha s asubsequence $\{f_{n_k}\}$ such that $\{f_{n_k}(x)\}$ converges for every $x \in E$. Let $g_k = f_{n_k}$, we need to prove that $\{g_k\}$ converges uniformly on $K$. Since $\{g_k(x)\}$ converges for every $x \in K$, then there is an integer $N > 0$ such that
\begin{align*}
    \left|g_n(x_i) - g_m(x_i)\right| < \varepsilon,
\end{align*}
for $n,m \geq N$ and $1 \leq i \leq N$. For any $x \in K$, $x \in B(x_i,\delta)$ for some $i$, and by equicontinuity, $\left|g_n(x) - g_n(x_i)\right| < \varepsilon$ for all $n$. Hence for all $n,m \geq N$, we have
\begin{align*}
    \left|g_n(x) - g_m(x)\right| \leq \left|g_n(x) - g_m(x_i)\right| + \left|g_n(x_i) - g_m(x_i)\right| + \left|g_m(x_i) - g_m(x)\right| < 3 \varepsilon,
\end{align*}
and this proves \ref{th_618_b}.
\end{proof}

\medskip






\section{The Stone-Weiestrass Theorem}


\begin{theorem}[Weiestrass]\label{th_619}
If $f: [a,b] \to \mathbb{R}$ is continuous, then for every $\varepsilon > 0$, there is a polynomial $p_{\varepsilon}$ such that $\left|f(x) - p_{\varepsilon}(x)\right| < \varepsilon$ for all $x \in [a,b]$.
\end{theorem}

\begin{remark}
This theorem can be equivalent stated as: there is a sequence $\{p_n\}$ of polynomials that $p_n$ uniformly converges to $f$ on $[a,b]$.
\end{remark}

\begin{proof}
It suffices to prove it for $[a,b] = [0,1]$. Indeed, if $f: [a,b] \to \mathbb{R}$ is continuous and $L:[0,1] \to \mathbb{R}$ is defined by $L(x) = a + (b-a)x$, then $f \circ L:[0,1] \to \mathbb{R}$ is continuous. And $q_{\varepsilon}$ is a polynomial such that $\left|(f \circ L)(x) - q_{\varepsilon}(x)\right| < \varepsilon$ for every $x \in [0,1]$, then taking 
\begin{align*}
    p_{\varepsilon}(x) = q_{\varepsilon} \left(\frac{x - a}{b - a}\right)
\end{align*}
would show the theorem. The theorem follows from the following theorem.
\end{proof}

\medskip

\begin{theorem}[Bernstein]\label{th_620}
If $f: [0,1] \to \mathbb{R}$ is continuous, then the sequence of polynomials
\begin{align*}
    p_n(x) = \sum^n_{k=0} \binom{n}{k} x^k (1 - x)^{n-k} f \left(\frac{k}{n}\right), \quad where \quad \binom{n}{k} = \frac{n!}{k!(n-k)!},
\end{align*}
converges uniformly to $f$ on $[0,1]$.
\end{theorem}
\begin{proof}
Consider the binomial formula:
\begin{align}\label{th_620_equ1}
    (x + y)^n = \sum^n_{k=0} \binom{n}{k} x^k y^{n-k},
\end{align}
and taking derivative with respect to $x$ twice yields
\begin{align}
    n(x + y)^{n-1} & = \sum^n_{k=0} k\binom{n}{k} x^{k-1} y^{n-k}, \label{th_620_equ2}\\
    n(n - 1)(x + y)^{n-2} & = \sum^n_{k=0} k(k - 1)\binom{n}{k} x^{k-2} y^{n-k}. \label{th_620_equ3}
\end{align}
Multiplying both sides of \eqref{th_620_equ2} by $x$ and both sides of \eqref{th_620_equ3} by $x^2$ gives
\begin{align*}
    nx(x + y)^{n-1} = \sum^n_{k=0} k\binom{n}{k} x^{k} y^{n-k}, \quad n(n - 1)x^2(x + y)^{n-2} = \sum^n_{k=0} k(k - 1)\binom{n}{k} x^{k} y^{n-k}.
\end{align*}
Let $y = 1 - x$  and
\begin{align*}
    r_n(x) = \binom{n}{k} x^k y^{n-k},
\end{align*}
then $\sum^n_{k=0} r_k(x) = 1$ and $\sum^n_{k=0} k r_k(x) = nx$. Then we have
\begin{align*}
    n(n - 1)x^2 = \sum^n_{k=0} k(k - 1) r_k(x) = \sum^n_{k=0} k^2 r_k(x) - \underbrace{\sum^n_{k=0} k r_k(x)}_{= \, nx}.
\end{align*}
Therefore, we have
\begin{align*}
    \sum^n_{k=0} k^2 r_k(x) = n(n - 1)x^2 + nx,
\end{align*}
and hence
\begin{align*}
    \sum^n_{k=0} (k - nx)^2 r_k(x) & = \sum^n_{k=0} k^2 r_k(x) - 2nx \sum^n_{k=0} k r_k(x) + n^2x^2 \sum^n_{k=0} k r_k(x) \\
    & = n(n - 1)x^2 + nx - 2n^2 x^2 + n^2 x^2 = nx(1 - x).
\end{align*}

Then we have
\begin{align*}
    \left|f(x) - p_n(x)\right| & = \left|f(x) \sum^n_{k=0} r_k(x) - \sum^n_{k=0} f\left(\frac{k}{n}\right) r_k(x)\right| \\
    & = \left|\sum^n_{k=0} \left(f(x) - f\left(\frac{k}{n}\right)\right) r_k(x)\right| \leq \sum^n_{k=0} \left|f(x) - f\left(\frac{k}{n}\right)\right| r_k(x).
\end{align*}
By continuity of $f$ on compact set $[0,1]$, $\left|f\right|$ is bounded on $[0,1]$, say by $M$ and $f$ is uniformly continuous on $[0,1]$. Then for any $\varepsilon > 0$, there is a $\delta > 0$ such that $\left|f(x) - f(y)\right| < \varepsilon$ if $\left|x - y\right| < \delta$ for all $x,y \in [0,1]$. For this $\varepsilon$, if we choose $N > M/(\delta^2 \varepsilon)$, then for all $n \geq M$, we have
\begin{align*}
    \sum^n_{k=0} \left|f(x) - f\left(\frac{k}{n}\right)\right| r_k(x) & = \sum_{k:\left|x-k/n\right|<\delta} \left|f(x) - f\left(\frac{k}{n}\right)\right| r_k(x) + \sum_{k:\left|x-k/n\right|\geq \delta} \left|f(x) - f\left(\frac{k}{n}\right)\right| r_k(x) \\
    & \leq \frac{\varepsilon}{2} + 2M \sum_{k:\left|x-k/n\right|\geq \delta} r_k(x) 
    \leq \frac{\varepsilon}{2} + \frac{2M}{n^2\delta^2} \sum_{k:\left|x-k/n\right|\geq \delta} (k - nx)^2 r_k(x) \\
    & \leq \frac{\varepsilon}{2} + \frac{2M}{n^2\delta^2} n \underbrace{x(1 - x)}_{\leq\, 1/4} \leq \frac{\varepsilon}{2} + \frac{M}{2n\delta^2} < \frac{\varepsilon}{2} + \frac{\varepsilon}{2} = \varepsilon,
\end{align*}
and the theorem follows.
\end{proof}

\medskip

Next, we present the original proof in Rudin's book.

\medskip

\begin{proof}[Second Proof of Theorem \ref{th_619}]
Without loss of generality, we assume that $[a,b] = [0,1]$ and even more, we could assume $f(0) = f(1) = 0$. Since if the theorem is proved in this case, consider
\begin{align*}
    g(x) = f(x) - f(0) - x \left(f(1) - f(0)\right),
\end{align*}
for $0 \leq x \leq 1$. Clearly, $g(0) = g(1) = 0$. If $g$ is the limit of a uniformly convergent sequence of polynomials, the same it true for $f$, since $f - g$ is a polynomial.

Furthermore, we define $f(x) = 0$ for $x \notin [0,1]$, then $f$ is uniformly continuous on $\mathbb{R}$. Let $Q_n(x) = c_n (1 - x^2)^n, n = 1,2,3,\cdots$, where $c_n$ is chosen such that
\begin{align*}
    \int^1_{-1} Q_n(x) \,dx = 1, \,\, n = 1,2,3,\cdots.
\end{align*}
For the order of magnitude of $c_n$, we have
\begin{align*}
    \int^1_{-1} (1 - x^2)^n \,dx & = 2 \int^1_0 (1 - x^2)^n \,dx \geq 2 \int^{1/\sqrt{n}}_0 (1 - x^2) \,dx \geq 2 \int^{1/\sqrt{n}}_0 (1 - nx^2)^n \,dx > \frac{1}{\sqrt{n}},
\end{align*}
and $c_n$ need to satisfy $c_n < \sqrt{n}$. 

For any $\delta > 0$, since $c_n < \sqrt{n}$, $Q_n(x) \leq \sqrt{n} (1 - \delta^2)^n$ for $\delta \leq \left|x\right| \leq 1$, and hence $Q_n(x) \to 0$ uniformly in $\delta \leq \left|x\right| \leq 1$. Now let
\begin{align*}
    P_n(t) = \int^1_{-1} f(x+t) Q_n(t) \,dt,
\end{align*}
for $0 \leq x \leq 1$. By the change of variable and $f = 0$ outside $[0,1]$, we have
\begin{align*}
    P_n(x) = \int^{1-x}_{-x} f(x+t) Q_n(t) \,dt = \int^1_0 f(t) Q_n(t - x) \,dt,
\end{align*}
and the right hand side is a polynomial in $x$. 

Given any $\varepsilon > 0$, choose $\delta > 0$ such that $\left|f(x) - f(y)\right| < \varepsilon/2$ if $\left|x - y\right| < \delta$. Let $M = \sup \left|f(x)\right|$, then for any $x \in [0,1]$, we have
\begin{align*}
    \left|P_n(x) - f(x)\right| & = \left|\int^1_0 \left(f(x+t) - f(x)\right) Q_n(t) \,dt \right| \\
    & \leq \int^1_0 \left|f(x+t) - f(x)\right| Q_n(t) \,dt \\
    & \leq 2M \int^{-\delta}_{-1} Q_n(t) \,dt + \frac{\varepsilon}{2} \int^{\delta}_{-\delta} Q_n(t) \,dt + 2M \int^1_{\delta} Q_n(t) \,dt \\
    & \leq 4M \sqrt{n} (1 - \delta^2)^n + \frac{\varepsilon}{2} < \varepsilon,
\end{align*}
for $n$ large enough.
\end{proof}

\medskip

\begin{definition}\label{def_68}
A family $\mathscr{A}$ of complex functions defined on a set $E$ is said to be an algebra if for all $f, g \in \mathscr{A}$ and all complex constant $c$,
\begin{enumerate}[label=(\alph*)]
    \item $f + g \in \mathscr{A}$, \label{def_68_a}
    
    \item $fg \in \mathscr{A}$, \label{def_68_b}
    
    \item $cf \in \mathscr{A}$, \label{def_68_c}
\end{enumerate}
that is, $\mathscr{A}$ is closed under addition, multiplication and scalar multiplication. Also, for algebra of real functions, \ref{def_68_c} is only required to hold for real number $c$.

If $\mathscr{A}$ has the property that $f \in \mathscr{A}$ whenever $f_n \in \mathscr{A}$ and $f_n \to f$ uniformly on $E$, then $\mathscr{A}$ is said to be uniformly closed.

Let $\mathscr{B}$ be the set of all functions which are limits of uniformly convergent sequences of $\mathscr{A}$. Then $\mathscr{B}$ is called the uniform closure of $\mathscr{A}$.
\end{definition}

\medskip

\begin{remark}
The set of all polynomials is an algebra, and then the Weiestrass theorem \ref{th_619} can be stated as the set of continuous functions on $[a,b]$ is the uniform closure of the set of polynomials on $[a,b]$.
\end{remark}

\medskip

\begin{theorem}
Let $\mathscr{B}$ be the uniform closure of an algebra $\mathscr{A}$ of bounded functions, then $\mathscr{B}$ is a uniformly closed algebra.
\end{theorem}
\begin{proof}
If $f,g \in \mathscr{B}$, then there exist uniformly convergent sequences $\{f_n\}, \{g_n\}$ in $\mathscr{A}$ such that $f_n \to f$ and $g_n \to g$. Since $\mathscr{A}$ is bounded, it is clear that $f_n + g_n \to f + g$, $f_ng_n \to fg$ and $cf_n \to cf$ where the convergence is uniform in each case. Hence $f + g, fg, cf \in \mathscr{B}$, which implies $\mathscr{B}$ is an algebra. By Theorem \ref{th_112}, $\mathscr{B}$ is uniformly closed.
\end{proof}

\medskip

\begin{definition}
Let $\mathscr{A}$ be a family of functions on a set $E$. Then $\mathscr{A}$ is said to separate points on $E$ if for every $x_1, x_2 \in E$ and $x_1 \notin x_2$, there corresponds a function $f \in \mathscr{A}$ such that $f(x_1) = f(x_2)$.

If to each $x \in E$ there corresponds a function $g \in \mathscr{A}$ such that $g(x) \neq 0$, we say that $\mathscr{A}$ vanishes at no point of $E$.
\end{definition}

\medskip

\begin{theorem}\label{th_622}
Suppose $\mathscr{A}$ is an algebra of functions on a set $E$, $\mathscr{A}$ separates points on $E$ and $\mathscr{A}$ vanishes at no point of $E$. Suppose $x_1, x_2$ are distinct points of $E$, and $c_1, c_2$ are constants. Then $\mathscr{A}$ contains a function $f$ such that $f(x_1) = c_1$ and $f(x_2) = c_2$.
\end{theorem}
\begin{proof}
Since $\mathscr{A}$ is an algebra, then $\mathscr{A}$ contains functions $g,h$ and $k$ such that
\begin{align*}
    g(x_1) \neq g(x_2), \quad h(x_1) \neq 0, \quad k(x_2) \neq 0.
\end{align*}
Let $u = gk - g(x_1)k$ and $v = gh - g(x_2)h$. Clearly, $u,v \in \mathscr{A}$ and $u(x_1) = v(x_2) = 0$, $u(x_2) \neq 0$, $v(x_2) \neq 0$. Then the function defined as
\begin{align*}
    f = \frac{c_1v}{v(x_1)} + \frac{c_2u}{u(x_2)}
\end{align*}
has the desired properties.
\end{proof}

\medskip

Now we can talk about the Stone's generalization of the Weierstrass theorem.

\medskip

\begin{theorem}[Stone-Weiestrass]\label{th_623}
Let $\mathscr{A}$ be an algebra of real functions on a compact set $K$. If $\mathscr{A}$ separates points on $K$ and if $\mathscr{A}$ vanishes on point of $K$, then the uniform closure $\mathscr{B}$ of $\mathscr{A}$ consists of all real functions on $K$.
\end{theorem}
\begin{remark}
The theorem can also be stated as: if $\mathscr{A} \subset C(K,\mathbb{R})$ satisfies the above assumptions, then $\mathscr{A}$ is a dense subset of $C(K,\mathbb{R})$, i.e. $\overline{\mathscr{A}} = C(K,\mathbb{R})$.

Note that the set $\mathscr{A} = \{f: [a,b] \to \mathbb{R} \,:\, f \,\, \text{is continuous}\}$ satisfies all above properties, and hence Weiestrass theorem \ref{th_619} follows from this theorem.
\end{remark}
\begin{proof}
We will divide the proof into four steps.
\begin{enumerate}[label=(\Roman*)]
    \item If $f \in \mathscr{B}$, then $\left|f\right| \in \mathscr{B}$. Let $a = \sup \left|f(x)\right|, x \in K$. By Theorem \ref{th_619}, for every $\varepsilon > 0$, there is a polynomial $P_n$ such that
    \begin{align*}
        \left|P_n(y) - \left|y\right|\right| < \varepsilon, \quad -a \leq y \leq a.
    \end{align*}
    Since $\mathscr{B}$ is an algebra, $P_n(f(x)) \in \mathscr{B}$. Hence we have 
    \begin{align*}
        \left|P_n(f(x)) - \left|f(x)\right|\right| < \varepsilon, \quad x \in K.
    \end{align*}
    Since $\mathscr{B}$ is uniformly closed, $\left|f\right| \in \mathscr{B}$.
    
    \item If $f \in \mathscr{B}$ and $g \in \mathscr{B}$, then $\max (f,g) \in \mathscr{B}$ and $\min (f,g) \in \mathscr{B}$. Indeed, 
    \begin{align*}
        \max(f,g) & = \frac{f + g}{2} + \frac{\left|f - g\right|}{2}, \\
        \min(f,g) & = \frac{f + g}{2} - \frac{\left|f - g\right|}{2}.
    \end{align*}
    
    \item Given a real continuous function $f$ on $K$, a point $x \in K$ and $\varepsilon > 0$, there is a function $g_x \in \mathscr{B}$ such that $g_x(x) = f(x)$ and
    \begin{align}\label{th_623_equ1}
        g_x(t) > f(t) - \varepsilon, \quad t \in K.
    \end{align}
    Since $\mathscr{A} \subset \mathscr{B}$ and by Theorem \ref{th_622}, fir every $y \in K$, there is a function $h_y \in \mathscr{B}$ such that
    \begin{align*}
        h_y(x) = f(x), \quad h_y(y) = f(y).
    \end{align*}
    By continuity of $h_y$, there is an open set $U_y$ of $y$ such that $h_y(t) > f(t) - \varepsilon$ for $t \in U_y$. Since $K$ is finite, there is a finite covering of points $y_1, \cdots, y_n$ such that 
    \begin{align*}
        K \subset \bigcup^n_{i=1} U_{y_i}.
    \end{align*}
    Let $g_x = \max \{h_{y_1}, \cdots, h_{y_n}\}$, and clearly $g_x \in \mathscr{B}$ and satisfies \eqref{th_623_equ1}. 
    \label{th_623_3}
    
    \item Given a real continuous function $f$ on $K$ and $\varepsilon > 0$, there is a function $h \in \mathscr{B}$ such that
    \begin{align}\label{th_623_equ2}
        \left|h(x) - f(x)\right| < \varepsilon, \quad x \in K.
    \end{align}
    Indeed, consider $g_x$ for each $x \in K$ in Step \ref{th_623_3}. By the continuity of $g_x$, there exists open sets $V_x$ of $x$ such that $g_x(t) < f(t) + \varepsilon$ for $t \in V_x$. Since $K$ is compact, there is a finite covering of points $x_1, \cdots, x_m$ such that
    \begin{align*}
        K \subset \bigcup^m_{i=1} V_{x_i}.
    \end{align*}
    Let $h = \min \{g_{x_1}, \cdots, g_{x_m}\}$, and clearly $h \in \mathscr{B}$ and satisfies \eqref{th_623_equ2}.
\end{enumerate}
Since $\mathscr{B}$ is uniformly closed, \eqref{th_623_equ2} is equivalent to the conclusion of the theorem.
\end{proof}

\medskip

\begin{remark}
This theorem does not hold for complex algebras. However, the theorem does hold for complex algebras if an extra condition is satisfied on $\mathscr{A}$, that $\mathscr{A}$ being self-adjoint, that is, for every $f \in \mathscr{A}$, its complex conjugate $\overline{f} \in \mathscr{A}$, where $\overline{f}(x) = \overline{f(x)}$. Then we have the following result.
\end{remark}

\medskip

\begin{theorem}[Stone-Weiestrass]\label{th_624}
Let $\mathscr{A}$ be a self-adjoint algebra of complex functions on a compact set $K$. If $\mathscr{A}$ separates points on $K$ and if $\mathscr{A}$ vanishes on point of $K$, then the uniform closure $\mathscr{B}$ of $\mathscr{A}$ consists of all complex functions on $K$.
\end{theorem}
\begin{proof}
Let $\mathscr{A}_R$ be the set of all real functions on $K$ which belong to $\mathscr{A}$. If $f \in \mathscr{A}$, then $f = u + iv$, with $u,v$ being real functions. Then $u = (f + \overline{f})/2$ and $u \in \mathscr{A}_R$. If $x_1 \neq x_2$, there exists $f \in \mathscr{A}$ such that $f(x_1) = 1, f(x_2) = 0$ and hence $0 = u(x_1) \neq u(x_2) = 1$, which implies that $\mathscr{A}_R$ separates points on $K$. If $x \in K$, then $g(x) \neq 0$ for some $g \in \mathscr{A}$ and there is a complex number $\lambda$ such that $\lambda g(x) > 0$. If $f = \lambda g$ and $f = u + iv$, then $u(x) > 0$, and hence $\mathscr{A}_R$ vanishes at no point of $K$.

Hence $\mathscr{A}_R$ satisfies the properties of Theorem \ref{th_623} and it follows that all real continuous functions on $K$ belong to the uniform closure of $\mathscr{A}_R$, and hence in $\mathscr{B}$. If $f$ is a complex continuous function on $K$ and $f = u + iv$, then $u,v \in \mathscr{B}$, hence $f \in \mathscr{B}$.
\end{proof}



\medskip

\section{Power Series}

\begin{definition}
Power series of the form
\begin{align*}
    f(x) = \sum^\infty_{n=0} c_n x^n,
\end{align*}
is defined in Definition \ref{def_212}, more generally, it can also be defined as
\begin{align*}
    f(x) = \sum^\infty_{n=0} c_n (x - a)^n,
\end{align*}
which is called analytic function.
\end{definition}

\medskip

\begin{theorem}\label{th_625}
Suppose the series
\begin{align}\label{th_625_equ1}
    \sum^\infty_{n=0} c_n x^n
\end{align}
converges for $\left|x\right| < R$, and define
\begin{align}\label{th_625_equ2}
    f(x) = \sum^\infty_{n=0} c_n x^n, \quad \left|x\right| < R.
\end{align}
Then \eqref{th_625_equ1} converges uniformly on $[-R + \varepsilon, R - \varepsilon]$ for any $\varepsilon > 0$. Then $f$ is continuous and differentiable in $(-R,R)$, and
\begin{align}\label{th_625_equ3}
    f'(x) = \sum^\infty_{n=1} n c_n x^{n-1}, \quad \left|x\right| < R.
\end{align}
\end{theorem}
\begin{proof}
For any $\varepsilon > 0$, and $\left|x\right| \leq R - \varepsilon$, we have $\left|c_nx^n\right| \leq \left|c_n(R - \varepsilon)^n\right|$, and since
\begin{align*}
    \sum^\infty_{n=0} c_n(R - \varepsilon)^n
\end{align*}
converges absolutely, the series \eqref{th_625_equ2} converges uniformly on $[-R + \varepsilon, R - \varepsilon]$ by $M$-test (Theorem \ref{th_62}). Since $\sqrt[n]{n} \to 1$ as $n \to \infty$, we have
\begin{align*}
    \limsup_{n\to\infty} \sqrt[n]{n\left|c_n\right|} = \limsup_{n\to\infty} \sqrt[n]{\left|c_n\right|},
\end{align*}
and hence \eqref{th_625_equ2} and \eqref{th_625_equ3} have the same radius of convergence $R$, defined as in Theorem \ref{th_232}.\footnote{The radius of convergence for \eqref{th_625_equ1} and \eqref{th_625_equ2} are
\begin{align*}
    R = \frac{1}{\limsup_{n\to\infty} \sqrt[n]{\left|c_n\right|}}, \quad \overline{R} = \frac{1}{\limsup_{n\to\infty} \sqrt[n]{n\left|c_n\right|}}.
\end{align*}
In order to prove that $R = \overline{R}$, we need to show that $\left|x\right| < R$ implies \eqref{th_625_equ3} converges and $\left|x\right| > R$ implies \eqref{th_625_equ3} diverges. If $\left|x\right| < R$, there is $r$ such that $\left|x\right| < r < R$ and then $\left|n c_n x^{n-1}\right| = n \left|c_n r^{n-1}\right| \left|x/r\right|^{n-1}$. Since $\sum^\infty_{n=0} c_n r^n$ converges by assumption, $c_n r^{n-1} = c_nr^n/r$ converges to zero as $n \to \infty$ by Theorem \ref{th_218} and hence form a bounded sequence, that is $\left|c_n r^{n-1}\right| \leq M$ for all $n$. Therefore, $\sum^\infty_{n=0} nM \left|x/r\right|^{n-1}$ converges by Ratio Test since $\left|x/r\right| < 1$, and the convergence of \eqref{th_625_equ3} follows. If $\left|x\right| > R$, the divergence is obvious.}Then \eqref{th_625_equ3} converges uniformly on $[-R + \varepsilon, R - \varepsilon]$ for every $\varepsilon > 0$ and by Theorem \ref{th_610}, \eqref{th_625_equ3} holds for $\left|x\right| \leq R - \varepsilon$. Since $\varepsilon$ is arbitrary, \eqref{th_625_equ3} holds for $\left|x\right| \leq R$. 

Thus the continuity of $f$ exists by the existence of $f'$.
\end{proof}

\medskip

\begin{proof}[Second Proof of Theorem \ref{th_625}]
We discuss another approach to prove the term by term differentiability of the series $f(x)$. Fix $x_0 \in (-R,R)$. For $x \neq x_0$, we have
\begin{align*}
    \frac{f(x) - f(x_0)}{x - x_0} = \sum^\infty_{n=1} c_n \frac{x^n - x_0^n}{x - x_0} = \sum^\infty_{n=1} c_n \left(x^{n-1} + x_0 x^{n-2} + \cdots + x x_0^{n-2} + x_0^{n-1}\right).
\end{align*}
Let $G(x)$ denote the right hand side and let $r$ be such that $\left|x_0\right| < r < R$. We need to prove that $G(x)$ converges uniformly on $[-r,r]$. Observe that
\begin{align*}
    \left|c_n \left(x^{n-1} + x_0 x^{n-2} + \cdots + x x_0^{n-2} + x_0^{n-1}\right)\right| \leq n \left|c_n\right| r^{n-1}.
\end{align*}
Since $\sum^\infty_{n=1} n c_n r^{n-1}$ converges absolutely on $[-r,r]$, $G(x)$ converges uniformly on $[-r,r]$ by $M$-test. Therefore $G(x)$ is continuous on $[-r,r]$ and in particular, continuous at $x_0$, and hence
\begin{align*}
    f'(x_0) = \lim_{x\to x_0} \frac{f(x) - f(x_0)}{x - x_0} = \lim_{x \to x_0} G(x) = G(x_0) = \sum^\infty_{n=1} n c_n x_0^{n-1}.
\end{align*}
\end{proof}

\medskip

\begin{corollary}\label{coro_6251}
If $f(x) = \sum^\infty_{n=0} c_n x^n$ has the radius of convergence $R > 0$, then the function $f(x)$ is of class $C^\infty(-R,R)$ and
\begin{align*}
    f(x) = \sum^\infty_{n=0} \frac{f^{(n)}}{n!} x^n,
\end{align*}
for all $x \in (-R,R)$.
\end{corollary}
\begin{proof}
We can apply Theorem \ref{th_625} to the series of derivatives
\begin{align*}
    \left(\sum^\infty_{n=1} n c_n x^{n-1} \right)' = \sum^\infty_{n=2} n (n-1) c_n x^{n-2},
\end{align*}
and continue this process for all $k$-th derivatives. Also, we have
\begin{align*}
    f^{(k)}(x) = \sum^\infty_{n=k} n (n-1) \cdots (n-k+1) c_n x^{n-k},
\end{align*}
taking $x = 0$ implies $f^{(k)}(0) = k! c_k$, which proves the corollary.
\end{proof}

\medskip

\begin{theorem}[Abel]\label{th_626}
If $0 < R < \infty$ the radius of convergence of the series
\begin{align*}
    f(x) = \sum^\infty_{n=0} c_n x^n,
\end{align*}
and the series converges at $x = R$ (or at $x = -R$), then the function $f:(-R,R] \to \mathbb{R}$ is left continuous at $x = R$ (or $f:[-R,R) \to \mathbb{R}$ is right continuous at $x = -R$). In other words,
\begin{align*}
    \sum^\infty_{n=0} c_n R^n = \lim_{x \to R^-} c_n x^n, \quad \text{or} \quad \sum^\infty_{n=0} c_n (-R)^n = \lim_{x \to -R^+} c_n x^n.
\end{align*}
\end{theorem}
\begin{proof}
If the series converges at $x = -R$, then the series $f(-x) = \sum^\infty_{n=0} c_n (-1)^n x^n$ converges at $x = R$, so it suffices to prove the series converges at $x = R$ only. Also, if the series has the radius of convergence $0 < R < \infty$, then the series $f(Rx) = \sum^\infty_{n=0} c_n R^n x^n$ has the radius of convergence $1$. The without loss of generality, we may assume $R = 1$ and the series converges at $x = 1$. Then we need to prove that
\begin{align}\label{th_626_equ1}
    \lim_{x \to 1^-} \sum^\infty_{n=0} c_n x^n = \sum^\infty_{n=0} c_n.
\end{align}
Let $s_n = c_0 + c_1 + \cdots + c_n, s_{-1} = 0$ and $s = \sum^\infty_{n=0} c_n$. For $\left|x\right| < 1$, we have
\begin{align*}
    \sum^m_{n=0} c_n x^n & = \sum^m_{n=0} \left(s_n - s_{n-1}\right) x^n \\
    & = \left(s_0 + s_1 x + \cdots + s_m x^m\right) - \left(s_{-1} + s_0 x + \cdots + s_{m-1}x^m\right) \\
    & = (1 - x) s_0 + (1 - x)xs_1 + \cdots + (1 - x)x^{m-1} + s_m x^m \\
    & = (1 - x) \sum^{m-1}_{n=0} s_n x^n + s_m x^m,
\end{align*}
and hence
\begin{align*}
    - s_m x^m + \sum^m_{n=0} c_n x^n = (1 - x) \sum^{m-1}_{n=0} s_n x^n.
\end{align*}
For $\left|x\right| < 1$, the left hand side converges as $m \to \infty$, since the series converges to $f(x)$ and $s_m x^m \to 0$. Hence the right hand side converges too, which yields
\begin{align*}
    f(x) = (1 - x) \sum^{\infty}_{n=0} s_n x^n, \quad \left|x\right| < 1.
\end{align*}
Since $(1 - x) \sum^\infty_{n=0} x^n = 1$ for $\left|x\right| < 1$, we have
\begin{align*}
    s = (1 - x) \sum^\infty_{n=0} s x^n,
\end{align*}
and hence
\begin{align*}
    \left|f(x) - s\right| = \left|(1 - x) \sum^{\infty}_{n=0} (s_n - s) x^n\right|.
\end{align*}

For any $\varepsilon > 0$, there is an integer $N > 0$ such that $\left|s_n - s\right| < \varepsilon/2$ for all $n \geq N$. Let
\begin{align*}
    0 < \delta < \frac{\varepsilon}{2 \sum^N_{n=0}\left|s_n - s\right|},
\end{align*}
then for any $x \in (1 - \delta,1)$, we have
\begin{align*}
    \left|f(x) - s\right| & \left|(1 - x) \sum^{N}_{n=0} (s_n - s) x^n\right| + \left|(1 - x) \sum^{\infty}_{n=N+1} (s_n - s) x^n\right| \\
    & \leq \left|1 - x\right| \sum^N_{n=0} \left|s_n - s\right| \left|x\right|^n + \frac{\varepsilon}{2} \left|(1 - x) \sum^{\infty}_{n=N+1} x^n\right| \\
    & \leq \delta \sum^N_{n=0} \left|s_n - s\right| + \frac{\varepsilon}{2} \cdot 1 < \varepsilon,
\end{align*}
which proves \eqref{th_626_equ1}.
\end{proof}

\medskip



\section{Taylor's Theorems}

In Corollary \ref{coro_6251}, we proved that if a power series $f(x) = \sum^\infty_{n=0} c_n x^n$ has radius of convergence $R > 0$, then $f$ is infinitely differentiable on $(-R,R)$ and
\begin{align*}
    f(x) = \sum^\infty_{k=0} \frac{f^{(k)}(0)}{k!} x^k,
\end{align*}
for all $x \in (-R,R)$. Therefore, if $f: I \to \mathbb{R}$ is defined on an open interval $I$ and $n$-times differentiable at $0 \in I$, it is natural to consider the {\em Maclaurin polynomial} defined by
\begin{align*}
    \sum^n_{k=0} \frac{f^{(k)}(0)}{k!} x^k = f(0) + f'(0)x + \frac{f''(0)}{2!}x^2 + \cdots + \frac{f^{(n)}(0)}{n!}x^n.
\end{align*}
The difference between the Maclaurin polynomial and $f$ is called the {\em remainder term} $R_n(x)$, which is defined by the equality
\begin{align}\label{sec_68_equ1}
    f(x) = \sum^n_{k=0} \frac{f^{(k)}(0)}{k!} x^k + R_n(x),
\end{align}
for all $x \in I$. 

\medskip

\begin{lemma}\label{lemma_63}
If $f$ is $(n-1)$-times differentiable in an open interval $I$ and $n$-times differentiable at $x = 0 \in I$, then the remainder $R_n(x)$ defined in \eqref{sec_68_equ1} is $n$-times differentiable at $0$ and $R_n(0) = R_n'(0) = \cdots = R_n^{(n)}(0) = 0$.
\end{lemma}
\begin{proof}
Since $f$ is $n$-times differentiable at $0$ and the Maclaurin polynomial is infinitely differentiable, $R_n(x)$ is $n$-times differentiable. 

Now we have
\begin{align*}
    R_n(x) = f(x) - \sum^n_{k=0} \frac{f^{(k)}(0)}{k!} x^k,
\end{align*}
and then $R_n(0) = f(0) - f(0) = 0$. Taking the derivative on the both sides yields
\begin{align*}
    R_n'(x) = f'(x) - \sum^n_{k=1} \frac{f^{(k)}(0)}{(k-1)!} x^{k-1},
\end{align*}
and then $R_n'(0) = f'(0) - f'(0) = 0$. We can continue this process and the lemma follows from the induction.
\end{proof}

\medskip

In \eqref{sec_68_equ1}, all derivatives of $f$ are computed at $x = 0$. We may consider the formula when the derivatives are computed at other point $x \in I$. Then if $f$ is $n$-times differentiable at $x \in I$, then the {\em Taylor polynomial} and the corresponding remainder term are defined as
\begin{align}\label{sec_68_equ2}
    f(x + h) = \sum^n_{k=0} \frac{f^{(k)}(x)}{k!} h^k + R_n(h).
\end{align}
Taylor's polynomial is defined for all $h$ and the remainder $R_n(h)$ is only defined for $h$ for which $x + h \in I$. And Lemma \ref{lemma_63} implies the following result.

\medskip

\begin{lemma}\label{lemma_64}
If $f$ is $(n-1)$-times differentiable in an open interval $I$ and $n$-times differentiable at $x \in I$, then the remainder $R_n(h)$ defined in \eqref{sec_68_equ2} is $n$-times differentiable at $0$ and $R_n(0) = R_n'(0) = \cdots = R_n^{(n)}(0) = 0$.
\end{lemma}

\medskip

Recall that in Theorem \ref{th_42}, if $f$ is differentiable at $x \in I$, then there is a function $\psi$ that is continuous at $0$, $\psi(0) = 0$ and
\begin{align*}
    f(x + h) = f(x) + f'(x) h + h \psi(h) = f(x) + f'(x) h + o(h),
\end{align*}
and in this case, $f(x) + f'(x) h$ is the Taylor polynomial of degree $n = 1$, and $R_1(h) = h \psi(h) = o(h)$ is the remainder term.

\medskip

\begin{theorem}[Taylor's theorem with Peano's remainder]\label{th_627}
If $f: I \to \mathbb{R}$ is $(n-1)$-times differentiable in an open interval $I$ and if $f$ is $n$-times differentiable at $x \in I$, then there is a function $\psi$, continuous at $0$, $\psi(0) = 0$ such that
\begin{align}\label{th_627_equ1}
    f(x + h) = f(x) + f'(x)h + \frac{f''(x)}{2!}h^2 + \cdots + \frac{f^{(n)}(x)}{n!}h^n + h^n \psi(h),
\end{align}
for $x + h \in I$. In particular, $h^n \psi(h) = o(h^n)$.
\end{theorem}
\begin{proof}
Let
\begin{align*}
    R_n(h) = f(x + h) - \sum^n_{k=0} \frac{f^{(k)}(x)}{k!} h^k,
\end{align*}
and define
\begin{align*}
    \psi(h) = \begin{cases}
        \frac{R_n(h)}{h^n}, & h \neq 0, \\
        0, & h = 0.
    \end{cases}
\end{align*}
Hence \eqref{th_627_equ1} is obvious and it remains to show that $R_n(h) = o(h^n)$. It suffices to show that $R_n(h)/h^n = \psi(h) \to 0$ as $h \to 0$. By Lemma \ref{lemma_64}, we have
\begin{align*}
    R_n(0) = R_n'(0) = \cdots = R_n^{(n)}(0) = 0.
\end{align*}
By L'Hospital's Rule (Theorem \ref{th_412}), we have
\begin{align*}
    \lim_{h \to 0} \frac{R_n(h)}{h^n} = \lim_{h \to 0} \frac{R_n'(h)}{nh^{n-1}} = \cdots = \lim_{h \to 0} \frac{R_n^{(n-1)}(h)}{(n-1)!h} \coloneqq M,
\end{align*}
and the above equality holds if the limit on the right hand side exists.\footnote{Note that we cannot appluy L'Hospital's Rule again, since by definition, $R_n^{(n-1)}$ is not necessarily differentiable expect $x = 0$, and this is because $f$ is only $(n-1)$-times differentiable at $x \neq 0$.}By the definition of the derivative, we have
\begin{align*}
    M = \lim_{h \to 0} \frac{f^{(n-1)}(x+h) - f^{(n-1)}(x) - f^{(n)}(x)h}{(n-1)!h} = 0,
\end{align*}
thus the theorem follows.
\end{proof}

\medskip

\begin{theorem}[Taylor's theorem with Lagrange and Cauchy remainders]\label{th_628}
If $f: I \to \mathbb{R}$ is $(n+1)$-times differentiable in an open interval $I$. If $x, x + h \in I$, then there are $\theta_1, \theta_2 \in (0,1)$ such that the remainder term $R_n(h)$ in
\begin{align*}
    f(x + h) = f(x) + f'(x)h + \frac{f''(x)}{2!}h^2 + \cdots + \frac{f^{(n)}(x)}{n!}h^n + R_n(h)
\end{align*}
equals
\begin{align*}
    R_n(h) = \frac{f^{(n+1)}(x+\theta_1 h)}{(n+1)!} h^{n+1} = \frac{f^{(n+1)}(x+\theta_2 h)}{(n+1)!} (1-\theta_2)^n h^{n+1}.
\end{align*}
The first remainder is called Lagrange remainder and the second one is Cauchy remainder.
\end{theorem}
\begin{proof}
Fix $x$ and $h$ and for $t \in [0,1]$, define a new function
\begin{align*}
    g(t) = f(x+th) + f'(x+th)h(1-t) + \frac{f''(x+th)}{2!} h^2 (1-t)^2 + \cdots + \frac{f^{(n)}(x+th)}{n!} h^n (1-t)^n.
\end{align*}
Clearly we have 
\begin{align*}
    g(1) - g(0) = f(x+h) - \left(f(x) + f'(x)h + \frac{f''(x)}{2!}h^2 + \cdots + \frac{f^{(n)}(x)}{n!}h^n\right) = R_n(h).
\end{align*}
By the Mean Value theorem \ref{th_48}, $g(1) - g(0) = g(\theta)$ for some $\theta \in (0,1)$. Note that
\begin{align*}
    g'(t) & = f'(x+th)h + \left(f''(x+th)h^2(1-t) - f'(x+th)h\right) \\
    & + \left(\frac{f'''(x+th)}{2!} h^3 (1-t)^2 - f''(x+th)h^2(1-t)\right) + \cdots \\
    & + \left(\frac{f^{(n)}(x+th)}{n!} h^{n+1} (1-t)^n - \frac{f^{(n)}(x+th)}{(n-1)!} h^n (1-t)^{n-1}\right) \\
    & = \frac{f^{(n)}(x+th)}{n!} h^{n+1} (1-t)^n,
\end{align*}
and hence we have
\begin{align*}
    R_n(h) = g(1) - g(0) = g'(\theta) = \frac{f^{(n)}(x+\theta h)}{n!} h^{n+1} (1-\theta)^n,
\end{align*}
for some $\theta \in (0,1)$. This proves the Cauchy remainder.

Now apply Cauchy Mean Value theorem \ref{th_47} to function $g(t)$ and $u(t) = -(1 - t)^{n+1}$, then there is $\widetilde{\theta} \in (0,1)$ such that
\begin{align*}
    R_n(h) = g(1) - g(0) = \frac{g(1) - g(0)}{u(1) - u(0)} = \frac{g'(\widetilde{\theta})}{u'(\widetilde{\theta})} = \frac{\frac{f^{(n)}(x+\widetilde{\theta} h)}{n!} h^{n+1} (1-\widetilde{\theta})^n}{(n+1)(1 - \widetilde{\theta})^n} = \frac{f^{(n)}(x+\widetilde{\theta} h)}{(n+1)!} h^{n+1},
\end{align*}
and this proves the Lagrange remainder.
\end{proof}






\chapter{Functions of Several Variables}

\section{Linear Transformations}

\begin{definition}
~\begin{enumerate}[label=(\alph*)]
    \item A nonempty set $X \subset \mathbb{R}^n$ is a vector space if $\mathbf{x} + \mathbf{y} \in X$ and $c\mathbf{x} \in X$ for any $\mathbf{x}, \mathbf{y} \in X$ and for all scalar $c$.
    
    \item If $\mathbf{x}_1, \cdots, \mathbf{x}_k \in \mathbb{R}^n$ and $c_1, \cdots, c_k$ are scalars, then the vector 
    \begin{align*}
        c_1 \mathbf{x}_1 + \cdots + c_k \mathbf{x}_k
    \end{align*}
    is called a linear combination of $\mathbf{x}_1, \cdots, \mathbf{x}_k$. If $S \subset \mathbb{R}^n$ and $E$ is the set of all linear combinations of elements of $S$, we say that $S$ spans $E$.
\end{enumerate}
\end{definition}

\begin{remark}
A normed vector space $X$ is a vector space over the real or complex numbers (field $K = \mathbb{R}$ or $\mathbb{C}$), on which a norm is defined. A norm is a real-valued function defined on the vector space that is commonly denoted by $\left\|\cdot\right\|: X \to \mathbb{R}$ that satisfies
\begin{enumerate}[label=(\roman*)]
    \item $\left\|x + y\right\| \leq \left\|x\right\| + \left\|y\right\|$ for all $x,y \in X$. \label{remark_71a} 
    
    \item $\left\|\alpha x\right\| = \left|\alpha\right| \left\|x\right\|$ for $x \in X$ and $\alpha$ is a scalar. \label{remark_71b}
    
    \item $\left\|x\right\| = 0$ if and only if $x = 0$. \label{remark_71c}
\end{enumerate}

Recall Definition \ref{def_19} of the metric space. By \ref{remark_71a}, the triangle inequality $\left\|x - y\right\| \leq \left\|x - z\right\| + \left\|z - y\right\|$ holds. Combined with \ref{remark_71b} and \ref{remark_71c}, we have that every normed vector space can be considered as a metric space, with distance between $x$ and $y$ being defined by $\left\|\cdot\right\|$ as above.
\end{remark}

\medskip

\begin{definition}
A normed vector space $X$ with norm $\left\|\cdot\right\|$ is a Banach space if it is a complete metric space with respect to the metric $d(x,y) = \left\|x - y\right\|$.
\end{definition}

\medskip

\begin{definition}
A mapping $A$ of a vector space $X$ into a vector space $Y$ is said to be linear transformation if 
\begin{align*}
    A(\mathbf{x}_1 + \mathbf{x}_2) = A \mathbf{x}_1 + A \mathbf{x}_2, \qquad A(c\mathbf{x}) = c A \mathbf{x},
\end{align*}
for all $\mathbf{x}, \mathbf{x}_1, \mathbf{x}_2 \in X$ and all scalars $c$. 
\end{definition}

\begin{remark}
Note that a linear transformation $A$ of $X$ into $Y$ is determined by its action on any basis: if $\mathbf{x}_1, \cdots, \mathbf{x}_n$ is a basis of $X$, then every $\mathbf{x} \in X$ can be uniquely represented as 
\begin{align*}
    \mathbf{x} = \sum^n_{i=1} c_i \mathbf{x}_i,
\end{align*}
and the linearity of $A$ allows to compute $A\mathbf{x}$ from the vectors $A\mathbf{x}_1, \cdots, A\mathbf{x}_n$ and the scalars $c_1, \cdots, c_n$ by
\begin{align*}
    A\mathbf{x} = \sum^n_{i=1} c_i A \mathbf{x}_i.
\end{align*}
Also observe that $A0 = 0$.
\end{remark}

\medskip

\begin{theorem}\label{th_71}
A linear transformation $A$ on a finite-dimensional vector space $X$ is one-to-one if and only if the range of $A$ is $X$.
\end{theorem}
\begin{proof}
Let $\mathbf{x}_1, \cdots, \mathbf{x}_n$ be a basis of $X$. The linearity of $A$ shows that the range $R_A$ is the span of $A\mathbf{x}_1, \cdots, A\mathbf{x}_n$. Then $R_A = X$ if and only if $Q$ is linearly independent. We need to prove that $A$ is one-to-one if and only if $A\mathbf{x}_1, \cdots, A\mathbf{x}_n$ are linearly independent.

Suppose $A$ is one-to-one, and $\sum^n_{i=1} c_i A \mathbf{x}_i = 0$. Then $A \left(\sum^n_{i=1} c_i \mathbf{x}_i\right) = 0$ and hence $\sum^n_{i=1} c_i \mathbf{x}_i = 0$. Clearly $c_1 = \cdots = c_n = 0$, which shows that $A\mathbf{x}_1, \cdots, A\mathbf{x}_n$ are linearly independent.

Conversely, suppose $A \left(\sum^n_{i=1} c_i \mathbf{x}_i\right) = 0$. Then $\sum^n_{i=1} c_i A \mathbf{x}_i = 0$, and hence $c_1 = \cdots = c_n = 0$. This implies $A\mathbf{x} = 0$ only if $\mathbf{x} = 0$. Now if $A\mathbf{x} = A\mathbf{y}$, then $A(\mathbf{x} - \mathbf{y}) = 0$ and hence $\mathbf{x} = \mathbf{y}$, thus $A$ is one-to-one.
\end{proof}

\medskip

\begin{definition}\label{def_74}
~\begin{enumerate}[label=(\alph*)]
    \item Let $L(X,Y)$ be the set of all linear transformation of the vector space $X$ into $Y$. When $X = Y$, instead of $L(X,X)$, we simply write $L(X)$. 
    
    \item If $X,Y,Z$ are vector spaces, and if $A \in L(X,Y)$ and $B \in L(Y,Z)$, we define the product $BA$ to be the composition of $A$ and :
    \begin{align*}
        (BA)\mathbf{x} = B(A\mathbf{x}), \quad \mathbf{x} \in X,
    \end{align*}
    then $BA \in L(X,Z)$. (Note that $BA$ need to to be the same as $AB$, even if $X = Y = Z$.)
    
    \item For $A \in L(\mathbb{R}^n,\mathbb{R}^m)$, define the operator norm $\left\|A\right\|$ of $A$ by
    \begin{align*}
        \left\|A\right\| = \sup_{\left|\mathbf{x}\right|=1} \left|A\mathbf{x}\right|.
    \end{align*}
    Note that the inequality $\left|A\mathbf{x}\right| \leq \left\|A\right\| \left|\mathbf{x}\right|$ holds for all $\mathbf{x} \in \mathbb{R}^n$. Also, if $\lambda$ is such that $\left|A\mathbf{x}\right| \leq \lambda \left|\mathbf{x}\right|$ for all $\mathbf{x} \in \mathbb{R}^n$, then $\left\|A\right\| \leq \lambda$.
\end{enumerate}
\end{definition}

\medskip

\begin{theorem}
~\begin{enumerate}[label=(\alph*)]
    \item If $A \in L(\mathbb{R}^n,\mathbb{R}^m)$, then $\left\|A\right\| < \infty$ and $A$ is a uniformly continuous mapping of $\mathbb{R}^n$ into $\mathbb{R}^m$.
    
    \item If $A, B \in L(\mathbb{R}^n,\mathbb{R}^m)$ and $c$ is a scalar, then
    \begin{align*}
        \left\|A + B\right\| \leq \left\|A\right\| + \left\|B\right\|, \quad \left\|cA\right\| = \left|c\right| \left\|A\right\|.
    \end{align*}
    
    \item If $A \in L(\mathbb{R}^n,\mathbb{R}^m)$ and $B \in L(\mathbb{R}^m,\mathbb{R}^k)$, then
    \begin{align*}
        \left\|BA\right\| \leq \left\|B\right\| \left\|A\right\|.
    \end{align*}
\end{enumerate}
\end{theorem}


\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item Let $\{e_1, \cdots, e_n\}$ be the standard basis of $\mathbb{R}^n$ and suppose $\mathbf{x} = \sum^n_{i=1} c_i e_i$ and $\left|\mathbf{x}\right| = 1$, then $\left|c_i\right| \leq 1$ for $i = 1,2,\cdots,n$. Then we have
    \begin{align*}
        \left|A\mathbf{x}\right| = \left|\sum^n_{i=1} c_i Ae_i\right| \leq \sum^n_{i=1} \left|c_i\right| \left|A e_i\right| \leq \sum^n_{i=1} \left|A e_i\right|,
    \end{align*}
    and hence $\left\|A\right\| \leq \sum \left|A e_i\right| < \infty$. 
    
    Since $\left|A\mathbf{x} - A\mathbf{y}\right| \leq \left\|A\right\| \left|\mathbf{x} - \mathbf{y}\right|$, $A$ is uniformly continuous.
    
    \item It follows from
    \begin{align*}
        \left|(A+B)\mathbf{x}\right| = \left|A\mathbf{x} + B\mathbf{x}\right| \leq \left|A\mathbf{x}\right| + \left|B\mathbf{x}\right| \leq \left(\left\|A\right\| + \left\|B\right\|\right) \left|\mathbf{x}\right|.
    \end{align*}
    
    \item Finally, it follows from
    \begin{align*}
        \left|(BA)\mathbf{x}\right| = \left|B(A\mathbf{x})\right| \leq \left\|B\right\| \left|A\mathbf{x}\right| \leq \left\|A\right\| \left\|B\right\| \left|\mathbf{x}\right|.
    \end{align*}
\end{enumerate}
\end{proof}

\begin{remark}
With the distance between $A$ and $B$ defined as $\left\|A - B\right\|$, $L(\mathbb{R}^n,\mathbb{R}^m)$ is a metric space. Indeed, if $A, B, C \in L(\mathbb{R}^n,\mathbb{R}^m)$, we have the triangle inequality
\begin{align*}
    \left\|A - C\right\| = \left\|(A - B) + (B - C)\right\| \leq \left\|A - B\right\| + \left\|B - C\right\|.
\end{align*}
Also, it is obvious that $\left\|A - B\right\|$ has the other properties of a metric.
\end{remark}

\medskip

\begin{definition}
Suppose $\{\mathbf{x}_1, \cdots, \mathbf{x}_n\}$ and $\{\mathbf{y}_1, \cdots, \mathbf{y}_m\}$ are bases of vector spaces $X$ and $Y$. Then every $A \in L(X,Y)$ determines a set of numbers $a_{ij}$ such that
\begin{align*}
    A\mathbf{x}_j = \sum^m_{i=1} a_{ij} \mathbf{y}_i, \quad 1 \leq j \leq n.
\end{align*}
We arrange these numbers in an array of $m$ rows and $n$ columns, called an $m$ by $n$ matrix:
\begin{align*}
    A = \begin{pmatrix}
        a_{11} & a_{12} & \cdots & a_{1n} \\
        a_{21} & a_{22} & \cdots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \cdots & a_{mn}
    \end{pmatrix}.
\end{align*}
\end{definition}

\medskip

If $\mathbf{x} = \sum^n_{j=1} c_j \mathbf{x}_j$, we have
\begin{align*}
    A\mathbf{x} = \sum^m_{i=1} \left(\sum^m_{i=1} a_{ij} c_j\right) \mathbf{y}_i.
\end{align*}
Also, the Cauchy-Schwarz inequality shows that
\begin{align*}
    \left|A\mathbf{x}\right|^2 = \sum^m_{i=1} \left(\sum^n_{j=1} a_{ij} c_j\right)^2 \leq \sum^m_{i=1} \left(\sum^n_{j=1} a_{ij}^2 \cdot \sum^n_{i=1} c_j\right) = \sum_{i,j} a_{ij}^2 \left|\mathbf{x}\right|^2,
\end{align*}
and hence
\begin{align*}
    \left\|A\right\| \leq \left(\sum_{i,j} a_{ij}^2\right)^{1/2}.
\end{align*}

\medskip


\section{Differentiation}

\begin{definition}\label{def_76}
Suppose $\Omega \subset \mathbb{R}^n$ is an open set and $f: \Omega \to \mathbb{R}^m$. If there exists a linear transformation $A \in L(\mathbb{R}^n,\mathbb{R}^m)$ such that
\begin{align}\label{def_76_equ1}
    \lim_{\mathbf{h}\to 0} \frac{\left|f(\mathbf{x} + \mathbf{h}) - f(\mathbf{x}) - A\mathbf{h}\right|}{\left|\mathbf{h}\right|} = 0,
\end{align}
then we say $f$ is differentiable at $\mathbf{x}$, and we write $A = Df(\mathbf{x})$. If $f$ is differentiable at every $\mathbf{x} \in \Omega$, we say that $f$ is differentiable in $\Omega$.
\end{definition}

\medskip

\begin{theorem}\label{th_73}
Suppose $\Omega$ and $f$ are defined as in Definition \ref{def_76}, $\mathbf{x} \in \Omega$, and \eqref{def_76_equ1} holds for $A_1$ and $A_2$, then $A_1 = A_2$. In other words, $Df(\mathbf{x})$ is uniquely defined.
\end{theorem}
\begin{proof}
Let $B = A_1 - A_2$, then we have
\begin{align*}
    \left|B\mathbf{h}\right| \leq \left|f(\mathbf{x} + \mathbf{h}) - f(\mathbf{x}) - A_1 \mathbf{h}\right| + \left|f(\mathbf{x} + \mathbf{h}) - f(\mathbf{x}) - A_2 \mathbf{h}\right|,
\end{align*}
and hence $\left|B\mathbf{h}\right|/\left|\mathbf{h}\right| \to 0$ as $\left|\mathbf{h}\right| \to 0$. For fixed $\mathbf{h} \neq 0$, we have
\begin{align}\label{th_73_equ1}
    \lim_{t\to 0} \frac{\left|B(t\mathbf{h}) \right|}{\left|t\mathbf{h} \right|} = 0,
\end{align}
and hence the linearity of $B$ shows that \eqref{th_73_equ1} is independent of $t$. Thus $B\mathbf{h} = 0$ for all $\mathbf{h} \in \mathbb{R}^n$, and hence $B = 0$.
\end{proof}

\medskip

\begin{proposition}\label{prop_71}
A mapping $f: \Omega \to \mathbb{R}^m$, where $\Omega \subset \mathbb{R}^n$ is open, is differentiable at $\mathbf{x}_0 \in \Omega$ if and only if there are $\varepsilon > 0$, a continuous function $\varphi: B^n(0,\varepsilon) \to \mathbb{R}^m$ such that $\varphi(0) = 0$, where $B^n(0,\varepsilon) = \left\{\mathbf{h} \in \mathbb{R}^n \,:\, \left|\mathbf{h}\right| < \varepsilon\right\}$ and a linear mapping $L: \mathbb{R}^n \to \mathbb{R}^m$ such that
\begin{align}\label{prop_71_equ1}
    f(\mathbf{x}_0 + \mathbf{h}) = f(\mathbf{x}_0) + L\mathbf{h} + \varphi(\mathbf{h}) \left|\mathbf{h}\right|,
\end{align}
for all $\mathbf{h}$ for which $\left|\mathbf{h}\right| < \varepsilon$. Then, $Df(\mathbf{x}_0) = L$.
\end{proposition}
\begin{remark}
Compare this theorem with Theorem \ref{th_42} \ref{th_42_c}.
\end{remark}
\begin{proof}
Suppose \ref{prop_71_equ1} holds, then
\begin{align*}
    \varphi(\mathbf{h}) = \frac{f(\mathbf{x}_0 + \mathbf{h}) - f(\mathbf{x}_0) - L\mathbf{h}}{\left|\mathbf{h}\right|},
\end{align*}
letting $\mathbf{h} = \mathbf{x} - \mathbf{x}_0$ implies
\begin{align*}
    \frac{f(\mathbf{x}) - f(\mathbf{x}_0) - L(\mathbf{x} - \mathbf{x}_0)}{\left|\mathbf{x} - \mathbf{x}_0\right|} = \varphi(\mathbf{x} - \mathbf{x}_0) \to 0,
\end{align*}
as $\mathbf{x} \to \mathbf{x}_0$. Hence by definition, $f$ is differentiable at $\mathbf{x}_0$ and $Df(\mathbf{x}_0) = L$.

Conversely, let $L = Df(\mathbf{x}_0)$ and define 
\begin{align*}
    \varphi(\mathbf{h}) = \begin{cases}
        \frac{f(\mathbf{x}_0 + \mathbf{h}) - f(\mathbf{x}_0) - L\mathbf{h}}{\left|\mathbf{h}\right|}, & \mathbf{h} \neq 0, \\
        0 \in \mathbb{R}^m, & \mathbf{h} = 0 \in \mathbb{R}^n.
    \end{cases}
\end{align*}
Let $\varepsilon > 0$ be such that $\left|\mathbf{h}\right| < \varepsilon$ implies $\mathbf{x}_0 + \mathbf{h} \in \Omega$, then $\varphi$ is well defined for all $\mathbf{h}$ with $\left|\mathbf{h}\right| < \varepsilon$. Clearly, $\varphi$ is continuous with $\varphi(0) = 0$ and
\begin{align*}
    f(\mathbf{x}_0 + \mathbf{h}) = f(\mathbf{x}_0) + L\mathbf{h} + \varphi(\mathbf{h}) \left|\mathbf{h}\right|.
\end{align*}
\end{proof}

\medskip

\begin{theorem}[Chain Rule]\label{th_74}
Let $\Omega \subset \mathbb{R}^n$ be open and $f: \Omega \to \mathbb{R}^m$ be differentiable at $\mathbf{x}_0 \in \Omega$. Let $G \subset \mathbb{R}^m$ be an open set containing $f(\Omega)$ and $g: G \to \mathbb{R}^k$ be differentiable at $f(\mathbf{x}_0)$. Then the mapping $g \circ f: \Omega \to \mathbb{R}^k$ is differentiable at $\mathbf{x}_0$ and
\begin{align*}
    D(g \circ f)(\mathbf{x}_0) = Dg(f(\mathbf{x}_0)) \cdot Df(\mathbf{x}_0).
\end{align*}
On the right hand side, we have the product of two linear transformation, as defined in Definition \ref{def_74}.
\end{theorem}
\begin{proof}
Let $D(g(f(\mathbf{x}_0)) = A$ and $Df(\mathbf{x}_0) = B$. By Proposition \ref{prop_71}, we have
\begin{align*}
    g(f(\mathbf{x}_0 + \mathbf{k})) & = g(f(\mathbf{x}_0)) + A \mathbf{k} + \psi(\mathbf{k}) \left|\mathbf{k}\right|, \\
    f(\mathbf{x}_0 + \mathbf{h}) & = f(\mathbf{x}_0) + B \mathbf{h} + \varphi(\mathbf{h}) \left|\mathbf{h}\right|.
\end{align*}
Then we have
\begin{align*}
    g(f(\mathbf{x}_0 + \mathbf{h})) & = g(f(\mathbf{x}_0) + B \mathbf{h} + \varphi(\mathbf{h}) \left|\mathbf{h}\right|) \\
    & = g(f(\mathbf{x}_0)) + A(B \mathbf{h} + \varphi(\mathbf{h}) \left|\mathbf{h}\right|) + \psi(B \mathbf{h} + \varphi(\mathbf{h}) \left|\mathbf{h}\right|) \left|B \mathbf{h} + \varphi(\mathbf{h}) \left|\mathbf{h}\right|\right| \\
    & = g(f(\mathbf{x}_0)) + AB \mathbf{h} + \eta(\mathbf{h}) \left|\mathbf{h}\right|,
\end{align*}
where $\eta(\mathbf{h}) = A \varphi(\mathbf{h}) + \psi(B \mathbf{h} + \varphi(\mathbf{h}) \left|\mathbf{h}\right|) \left|B \mathbf{h}/\left|\mathbf{h}\right| + \varphi(\mathbf{h})\right|$. Clearly, $\eta(\mathbf{h}) \to 0$ as $\mathbf{h} \to 0$, and letting $\eta(0) = 0$ gives continuity of $\eta$. Hence,
\begin{align*}
    g(f(\mathbf{x}_0 + \mathbf{h})) = g(f(\mathbf{x}_0)) + AB \mathbf{h} + \eta(\mathbf{h}) \left|\mathbf{h}\right|,
\end{align*}
and thus $g \circ f$ is differentiable at $\mathbf{x}_0$ and $D(g \circ f)(\mathbf{x}_0) = AB = Dg(f(\mathbf{x}_0)) \cdot Df(\mathbf{x}_0)$.
\end{proof}

\medskip

\begin{definition}
Let $f: \Omega \to \mathbb{R}^m$, where $\Omega \subset \mathbb{R}^n$ is open. Let $\{e_1, \cdots, e_n\}$ and $\{u_1, \cdots, u_m\}$ be the standard bases of $\mathbb{R}^n$ and $\mathbb{R}^m$. The components of $f$ are real functions $f_1, \cdots, f_m$ defined by
\begin{align*}
    f(\mathbf{x}) = \sum^m_{i=1} f_i(\mathbf{x}) u_i, \quad \mathbf{x} \in \Omega.
\end{align*}
We write it $f = (f_1, \cdots, f_m)$. For $\mathbf{x} \in \Omega$, $1 \leq i \leq m$ and $1 \leq j \leq n$, we define
\begin{align*}
    \left(D_jf_i\right)(\mathbf{x}) = \lim_{t\to 0} \frac{f_i(\mathbf{x} + te_j) - f_i(\mathbf{x})}{t},
\end{align*}
provided the limit exists. $\left(D_jf_i\right)(\mathbf{x})$ is the derivative of $f_i$ with respect to $x_j$, where $\mathbf{x} = (x_1, \cdots, x_n)$. Then the notation
\begin{align*}
    \frac{\partial f_i}{\partial x_j}
\end{align*}
is often used, and it is called a partial derivative.
\end{definition}

\medskip

\begin{theorem}\label{th_75}
Let $f = (f_1, \cdots, f_m): \Omega \to \mathbb{R}^m$, where $\Omega \subset \mathbb{R}^n$ is open, be differentiable at $\mathbf{x}_0 \in \Omega$, then
\begin{align}\label{th_75_equ1}
    Df(\mathbf{x}_0)e_j = \sum^m_{i=1} \frac{\partial f_i}{\partial x_j}(\mathbf{x}_0) u_i, \quad 1 \leq j \leq m.
\end{align}
In other words,
\begin{align*}
    Df(\mathbf{x}_0) = \begin{pmatrix}
        \frac{\partial f_1}{\partial x_1}(\mathbf{x}_0) & \frac{\partial f_1}{\partial x_2}(\mathbf{x}_0) & \cdots & \frac{\partial f_1}{\partial x_n}(\mathbf{x}_0) \\
        \frac{\partial f_2}{\partial x_1}(\mathbf{x}_0) & \frac{\partial f_2}{\partial x_2}(\mathbf{x}_0) & \cdots & \frac{\partial f_2}{\partial x_n}(\mathbf{x}_0) \\
        \vdots & \vdots & \ddots & \vdots \\
        \frac{\partial f_n}{\partial x_1}(\mathbf{x}_0) & \frac{\partial f_n}{\partial x_2}(\mathbf{x}_0) & \cdots & \frac{\partial f_n}{\partial x_n}(\mathbf{x}_0)
    \end{pmatrix},
\end{align*}
and this matrix is called Jacobian matrix of $f$ at $\mathbf{x}_0$.
\end{theorem}
\begin{proof}
Since
\begin{align*}
    \lim_{\mathbf{x} \to \mathbf{x}_0} \frac{\left|f(\mathbf{x}) - f(\mathbf{x}_0) - Df(\mathbf{x}_0) (\mathbf{x} - \mathbf{x}_0)\right|}{\left|\mathbf{x} - \mathbf{x}_0\right|} = 0,
\end{align*}
then in particular for $\mathbf{x} = \mathbf{x}_0 + he_j$, we have
\begin{align*}
    \lim_{h \to 0} \frac{f(x_1,\cdots,x_j+h, \cdots,x_n) - f(x_1,\cdots,x_n) - Df(\mathbf{x}_0)(he_j)}{h} = 0,
\end{align*}
and hence
\begin{align*}
    \lim_{h \to 0} \frac{f(x_1,\cdots,x_j+h, \cdots,x_n) - f(x_1,\cdots,x_n)}{h} = Df(\mathbf{x}_0)e_j.
\end{align*}
This is equivalent to
\begin{align*}
    \lim_{h \to 0} \begin{pmatrix}
        \frac{f_1(\mathbf{x}_0 + he_j) - f_1(\mathbf{x}_0)}{h} \\
        \vdots \\
        \frac{f_m(\mathbf{x}_0 + he_j) - f_m(\mathbf{x}_0)}{h}
    \end{pmatrix} = \begin{pmatrix}
        \frac{\partial f_1}{\partial x_j}(\mathbf{x}_0) \\
        \vdots \\
        \frac{\partial f_n}{\partial x_j}(\mathbf{x}_0)
    \end{pmatrix},
\end{align*}
which proves the theorem.
\end{proof}

\medskip

\begin{definition}
Let $f: \Omega \to \mathbb{R}^m$, where $\Omega \subset \mathbb{R}^n$ is open, be differentiable at $\mathbf{x}_0$. We define gradient as a vector whose components are partial derivatives of $f$ at $\mathbf{x}_0$:
\begin{align*}
    \nabla f(\mathbf{x}_0) = \left(\frac{\partial f}{\partial x_1}(\mathbf{x}_0), \cdots, \frac{\partial f}{\partial x_n}(\mathbf{x}_0)\right).
\end{align*}
\end{definition}

\medskip

\begin{proposition}
Let $f: \Omega \to \mathbb{R}^m$, where $\Omega \subset \mathbb{R}^n$ is open, be differentiable at $\mathbf{x}_0$, then
\begin{align*}
    Df(\mathbf{x}_0) \mathbf{v} = \nabla f(\mathbf{x}_0) \cdot \mathbf{v},
\end{align*}
for all $\mathbf{v} \in \mathbb{R}^n$. On the right hand side, we have the scalar product of vectors in $\mathbb{R}^n$.
\end{proposition}
\begin{proof}
Since
\begin{align*}
    Df(\mathbf{x}_0) = \left[\frac{\partial f}{\partial x_1}(\mathbf{x}_0), \cdots, \frac{\partial f}{\partial x_n}(\mathbf{x}_0)\right],
\end{align*}
then for all $\mathbf{v} = \left[v_1, \cdots, v_n\right]^T$, we have
\begin{align*}
    Df(\mathbf{x}_0) \mathbf{v} = \left[\frac{\partial f}{\partial x_1}(\mathbf{x}_0), \cdots, \frac{\partial f}{\partial x_n}(\mathbf{x}_0)\right] 
    \begin{bmatrix}
        v_1 \\
        \vdots \\
        v_n
    \end{bmatrix} = \frac{\partial f}{\partial x_1}(\mathbf{x}_0) v_1 + \cdots + \frac{\partial f}{\partial x_n}(\mathbf{x}_0) v_n = \nabla f(\mathbf{x}_0) \cdot \mathbf{v}.
\end{align*}
\end{proof}

\medskip

\begin{definition}
Let $f: \Omega \to \mathbb{R}^m$, where $\Omega \subset \mathbb{R}^n$ is open, and a unit vector $\mathbf{v} \in \mathbb{R}^n$, we define the directional derivative at $\mathbf{x}_0 \in \Omega$ by
\begin{align*}
    D_{\mathbf{v}} f(\mathbf{x}_0) = \lim_{t \to 0} \frac{f(\mathbf{x}_0 + t \mathbf{v}) - f(\mathbf{x}_0)}{t}, 
\end{align*}
provided the limit exists.
\end{definition}

\medskip

\begin{proposition}
Let $f: \Omega \to \mathbb{R}^m$, where $\Omega \subset \mathbb{R}^n$ is open, be differentiable at $\mathbf{x}_0$, then for every unit vector $\mathbf{v} \in \mathbb{R}^n$, we have
\begin{align*}
    D_{\mathbf{v}} f(\mathbf{x}_0) = Df(\mathbf{x}_0) \mathbf{v} = \nabla f(\mathbf{x}_0) \cdot \mathbf{v}.
\end{align*}
\end{proposition}
\begin{proof}
Since $f$ is differentiable at $\mathbf{x}_0$, we have
\begin{align*}
    \lim_{t\to 0} \frac{\left|f(\mathbf{x} + t\mathbf{v}) - f(\mathbf{x}_0) - Df(\mathbf{x})(t\mathbf{v})\right|}{\left|t\mathbf{v}\right|} = 0,
\end{align*}
and since $\left|\mathbf{v}\right| = 1$, we have
\begin{align*}
    \lim_{t\to 0} \frac{f(\mathbf{x} + t\mathbf{v}) - f(\mathbf{x}_0) - t \cdot Df(\mathbf{x})\mathbf{v}}{t} = 0,
\end{align*}
which is equivalent to
\begin{align*}
    \lim_{t\to 0} \frac{f(\mathbf{x} + t\mathbf{v}) - f(\mathbf{x}_0)\mathbf{v}}{t} = Df(\mathbf{x}) \mathbf{v},
\end{align*}
and the left side is exactly $D_{\mathbf{v}} f(\mathbf{v}_0)$.
\end{proof}

\medskip

\begin{theorem}
Let $f: \Omega \to \mathbb{R}^m$, where $\Omega \subset \mathbb{R}^n$ is open, be differentiable at $\mathbf{x}_0$, then $f$ is continuous at $\mathbf{x}_0$. In particular, there is a constant $M > 0$ and $\delta > 0$ such that $\left|\mathbf{x} - \mathbf{x}_0\right| < \delta$ implies 
\begin{align*}
    \left|f(\mathbf{x}) - f(\mathbf{x}_0)\right| \leq M \left|\mathbf{x} - \mathbf{x}_0\right|.
\end{align*}
In other words, $f$ satisfies a local Lipschitz continuity at $\mathbf{x}_0$.
\end{theorem}
\begin{proof}
Since $f$ is differentiable at $\mathbf{x}_0$, we have
\begin{align*}
    \lim_{\mathbf{x} \to \mathbf{x}_0} \frac{\left|f(\mathbf{x}) - f(\mathbf{x}_0) - Df(\mathbf{x}_0) (\mathbf{x} - \mathbf{x}_0)\right|}{\left|\mathbf{x} - \mathbf{x}_0\right|} = 0,
\end{align*}
then there is $\delta > 0$ such that $\left|\mathbf{x} - \mathbf{x}_0\right| < \delta$ implies
\begin{align*}
    \left|f(\mathbf{x}) - f(\mathbf{x}_0) - Df(\mathbf{x}_0) (\mathbf{x} - \mathbf{x}_0)\right| < \left|\mathbf{x} - \mathbf{x}_0\right|.
\end{align*}
By the triangle inequality, we have
\begin{align*}
    \left|f(\mathbf{x}) - f(\mathbf{x}_0)\right| & \leq \left|f(\mathbf{x}) - f(\mathbf{x}_0) - Df(\mathbf{x}_0) (\mathbf{x} - \mathbf{x}_0)\right| + \left|Df(\mathbf{x}_0) (\mathbf{x} - \mathbf{x}_0)\right| \\
    & \leq \left|\mathbf{x} - \mathbf{x}_0\right| + \left\|Df(\mathbf{x})\right\| \left|\mathbf{x} - \mathbf{x}_0\right| \\
    & = \left(\left\|Df(\mathbf{x})\right\| + 1\right) \left|\mathbf{x} - \mathbf{x}_0\right|,
\end{align*}
for $\left|\mathbf{x} - \mathbf{x}_0\right| < \delta$. Letting $M = \left\|Df(\mathbf{x})\right\| + 1$ yields the theorem.
\end{proof}

\medskip

\begin{theorem}\label{th_77}
Let $f: \Omega \to \mathbb{R}^m$, where $\Omega \subset \mathbb{R}^n$ is convex and open, and $f$ is differentiable in $\Omega$. If there is a real number $M > 0$ such that $\left\|Df(\mathbf{x})\right\| \leq M$ for every $\mathbf{x} \in \Omega$, then 
\begin{align*}
    \left|f(\mathbf{x}) - f(\mathbf{y})\right| \leq M \left|\mathbf{x} - \mathbf{y}\right|,
\end{align*}
for all $\mathbf{x}, \mathbf{y} \in \Omega$.
\end{theorem}
\begin{proof}
Fix $\mathbf{x}, \mathbf{y} \in \Omega$, let $\gamma(t) = (1 - t)\mathbf{x} + t\mathbf{y}$ for $t \in \mathbb{R}$ such that $\gamma(t) \in \Omega$. Since $\Omega$ is convex, $\gamma(t) \in \Omega$ for $0 \leq t \leq 1$. Let $g(t) = f(\gamma(t))$, and by the Chain Rule, we have $Dg(t) = Df(\gamma(t)) \cdot (\mathbf{y} - \mathbf{x})$, and hence
\begin{align*}
    \left|Dg(t)\right| \leq \left\|Df(\gamma(t))\right\| \left|\mathbf{y} - \mathbf{x}\right| \leq M \left|\mathbf{y} - \mathbf{x}\right|,
\end{align*}
for all $t \in [0,1]$. By Theorem \ref{th_414}, we have
\begin{align*}
    \left|f(\mathbf{x}) - f(\mathbf{y})\right| = \left|g(0) - g(1)\right| \leq M \left|\mathbf{y} - \mathbf{x}\right|.
\end{align*}
\end{proof}

\medskip

\begin{corollary}
If $Df(\mathbf{x}) = 0$ for all $\mathbf{x} \in \Omega$, then $f$ is constant.
\end{corollary}
\begin{proof}
Note the above theorem holds now for $M = 0$.
\end{proof}

\medskip

\begin{definition}
A differentiable mapping $f: \Omega \to \mathbb{R}^m$, where $\Omega \subset \mathbb{R}^n$ is open, is said to be continuously differentiable in $\Omega$ if $Df$ is a continuous mapping of $\Omega$ into $\mathbb{R}^m$. And we say that $f$ is a $C^1$-mapping, or $f \in C^1(\Omega)$.
\end{definition}

\medskip

\begin{theorem}
Let $f: \Omega \to \mathbb{R}^m$, where $\Omega \subset \mathbb{R}^n$ is open. Then $f \in C^1(\Omega)$ if and only if the partial derivative $\partial f_i/\partial x_j$ exists and continuous on $\Omega$ for all $1 \leq i \leq m$ and $1 \leq i \leq n$.
\end{theorem}
\begin{proof}
Suppose $f \in C^1(\Omega)$ and by \eqref{th_75_equ1}, we have
\begin{align*}
    \frac{\partial f_i}{\partial x_j}(\mathbf{x}) = \left(Df(\mathbf{x}) e_j\right) \cdot u_i,
\end{align*}
for all $i,j$ and all $\mathbf{x} \in \Omega$. Hence 
\begin{align*}
    \frac{\partial f_i}{\partial x_j}(\mathbf{x}) - \frac{\partial f_i}{\partial x_j}(\mathbf{y}) = \left\{[Df(\mathbf{x}) - Df(\mathbf{y})] e_j\right\} \cdot u_i,
\end{align*}
and since $\left|e_j\right| = \left|u_i\right| = 1$, we have
\begin{align*}
    \left|\frac{\partial f_i}{\partial x_j}(\mathbf{x}) - \frac{\partial f_i}{\partial x_j}(\mathbf{y})\right| \leq \left|[Df(\mathbf{x}) - Df(\mathbf{y})] e_j\right| \leq \left|Df(\mathbf{x}) - Df(\mathbf{y})\right|,
\end{align*}
and the continuity of $\partial f_i/\partial x_j$ follows.

Conversely, it suffices to prove it when $m = 1$. Fix $\mathbf{x} \in \Omega$ and $\varepsilon > 0$. Since $\Omega$ is open and there is an open ball $B(\mathbf{x}, r) \subset \Omega$ such that
\begin{align*}
    \left|\frac{\partial f_i}{\partial x_j}(\mathbf{y}) - \frac{\partial f_i}{\partial x_j}(\mathbf{x})\right| < \frac{\varepsilon}{n},
\end{align*}
for $\mathbf{y} \in B$ and $1 \leq j \leq n$. Suppose $\mathbf{h} = \sum^n_{i=1} h_i e_i$ and $\left|\mathbf{h}\right| < r$. Let $\mathbf{v}_0 = 0$ and $\mathbf{v}_k = \sum^k_{i=1} h_i e_i$ for $1 \leq k \leq n$, then
\begin{align*}
    f(\mathbf{x} + \mathbf{h}) - f(\mathbf{x}) = \sum^n_{i=1} \left(f(\mathbf{x} + \mathbf{v}_j) - f(\mathbf{x} + \mathbf{v}_{j-1})\right).
\end{align*}
Since $\left|\mathbf{v}_k\right| < r$ for $1 \leq k \leq n$ and $S$ is convex, the segments connecting $\mathbf{x} + \mathbf{v}_j$ and $\mathbf{x} + \mathbf{v}_{j-1}$ lie in $S$. Since $\mathbf{v}_j = \mathbf{v}_{j-1} + h_j e_j$, by the Mean Value theorem \ref{th_48}, we have
\begin{align*}
    f(\mathbf{x} + \mathbf{v}_j) - f(\mathbf{x} + \mathbf{v}_{j-1}) = h_j\frac{\partial f}{\partial x_j}(\mathbf{x} + \mathbf{v}_{j-1} + \theta_j h_j e_j), \quad j = 1,\cdots,n,
\end{align*}
for some $\theta_j \in (0,1)$. Then we have
\begin{align*}
    \left|f(\mathbf{x} + \mathbf{h}) - f(\mathbf{x}) - \sum^n_{i=1} h_i \frac{\partial f}{\partial x_j}(\mathbf{x}) \right| & = \left|\sum^n_{i=1} h_i \left( \frac{\partial f}{\partial x_j}(\mathbf{x} + \mathbf{v}_{j-1} + \theta_j h_j e_j) - \frac{\partial f}{\partial x_j}(\mathbf{x}) \right) \right| < \sum^n_{i=1} h_i \frac{\varepsilon}{n} < \left|\mathbf{h}\right| \varepsilon,
\end{align*}
for all $\mathbf{h}$ such that $\left|\mathbf{h}\right| < r$. This implies that $f$ is differentiable at $\mathbf{x}$ and $Df$ is the linear function which assigns $\sum^n_{i=1} h_i (\partial f/\partial x_j)(\mathbf{x})$ to the vector $\mathbf{h} = \sum^n_{i=1} h_i e_i$ and thus the theorem follows.
\end{proof}

\medskip

Now we discuss another Mean Value theorem.

\medskip

\begin{theorem}[Mean Value Theorem]\label{th_79}
Let $f: \Omega \to \mathbb{R}^m$ be differentiable on an open set $\Omega \subset \mathbb{R}^n$. Then for any $\mathbf{x}, \mathbf{y} \in \Omega$ such that the segment connecting $\mathbf{x}$ and $\mathbf{y}$ lies in $\Omega$, then there is a point $\mathbf{c}$ on this segment such that
\begin{align*}
    f(\mathbf{y}) - f(\mathbf{x}) = Df(\mathbf{c}) \cdot (\mathbf{y} - \mathbf{x}).
\end{align*}
\end{theorem}
\begin{proof}
Let $h(t) = f((1 - t)\mathbf{x} + t\mathbf{y}), t \in [0,1]$. Clearly, $h$ is differentiable and by the Chain Rule, we have
\begin{align*}
    h'(t) = Df((1 - t)\mathbf{x} + t\mathbf{y}) \cdot (\mathbf{y} - \mathbf{x}).
\end{align*}
Now by the Mean Value theorem of one variable (see Theorem \ref{th_48}), for some $t \in (0,1)$, we have
\begin{align*}
    f(\mathbf{y}) - f(\mathbf{x}) = h(1) - h(0) = h'(t) = Df((1 - t)\mathbf{x} + t\mathbf{y}) \cdot (\mathbf{y} - \mathbf{x}),
\end{align*}
and letting $\mathbf{c} = (1 - t)\mathbf{x} + t\mathbf{y}$ proves the theorem.
\end{proof}

\medskip

\begin{proposition}
Let $f: \Omega \to \mathbb{R}^m$, where $\Omega \subset \mathbb{R}^n$ is open and $f \in C^1(\Omega)$. Then for any $\mathbf{x}, \mathbf{y} \in \Omega$ such that the segment connecting $\mathbf{x}$ and $\mathbf{y}$ lies in $\Omega$, we have
\begin{align*}
    f(\mathbf{y}) - f(\mathbf{x}) = \int^1_0 Df(\mathbf{x} + t(\mathbf{y} - \mathbf{x})) \cdot (\mathbf{y} - \mathbf{x}) \,dt.
\end{align*}
\end{proposition}
\begin{proof}
Let $h(t) = f(\mathbf{x} + t(\mathbf{y} - \mathbf{x}), t \in [0,1]$, And as before, we have
\begin{align*}
    h'(t) = Df((1 - t)\mathbf{x} + t\mathbf{y}) \cdot (\mathbf{y} - \mathbf{x}).
\end{align*}
Continuity of the partial derivatives of $f$ implies the continuity of $h'$ and hence
\begin{align*}
    f(\mathbf{y}) - f(\mathbf{x}) = h(1) - h(0) = \int^1_0 h'(t) \,dt = \int^1_0 Df(\mathbf{x} + t(\mathbf{y} - \mathbf{x})) \cdot (\mathbf{y} - \mathbf{x}) \,dt.
\end{align*}
\end{proof}

\medskip




\section{The Banach Contraction Principle}

Now we need to talk about a fixed point theorem which holds in any complete metric spaces. It will be used in the Inverse Function theorem.

\medskip

\begin{definition}
Let $X$ be a metric space, with the metric $d$. Let $f: X \to X$ and if there is a number $c < 1$ such that 
\begin{align}\label{def_711_equ1}
    d(f(x),f(y)) \leq c d(x,y),
\end{align}
for all $x,y \in X$, then $f$ is said to be a contraction of $X$ into $X$. 
\end{definition}

\medskip

\begin{theorem}[The Banach Contraction Principle]\label{th_710}
If $X$ is a complete metric space, and if $f$ is a contraction of $X$ into $X$, then there exists one and only one fixed point $x \in X$ such that $f(x) = x$.
\end{theorem}

\begin{remark}
In other words, $f$ has a unique fixed point. The uniqueness is trivial since if $f(x) = x$ and $f(y) = y$, then $d(x,y) = d(f(x),f(y)) \leq c d(x,y)$ for $c < 1$, which it is only possible when $d(x,y) = 0$.
\end{remark}

\begin{proof}
Pick $x_0 \in X$ be arbitrary, and define $\{x_n\}$ by 
\begin{align*}
    x_{n+1} = f(x_n), \quad n = 0,1,2,\cdots.
\end{align*}
Let $c < 1$ such that \eqref{def_711_equ1} holds. For $n \geq 1$, we have
\begin{align*}
    d(x_n,x_{n+1}) = d(f(x_{n-1}),f(x_n)) \leq c d(x_{n-1},x_n).
\end{align*}
Hence, the induction gives
\begin{align*}
    d(x_n,x_{n+1}) \leq c^n d(x_1, x_0), \quad n = 0,1,2,\cdots.
\end{align*}
If $n < m$, it follows that
\begin{align*}
    d(x_n,x_m) & \leq \sum^m_{i=n+1} d(x_i,x_{i-1}) \\
    & \leq \left(c^n + c^{n+1} + \cdots + c^{m-1}\right) d(x_1,x_0) \\
    & = c^n \left(1 + c^1 + \cdots + c^{m-n-1}\right) d(x_1,x_0) \\
    & \leq c^n \left(\sum^\infty_{i=0} c^i \right) d(x_1,x_0)  \\
    & = \frac{c^n}{1 - c} d(x_1,x_0).
\end{align*}
Thus $\{x_n\}$ is a Cauchy sequence. Since $X$ is complete, then $x_n \to x \in X$. Since $f$ is a contraction, then $f$ is continuous (uniformly continuous on $X$), and hence we have
\begin{align*}
    f(x) = \lim_{n\to\infty} x_n = x.
\end{align*}
\end{proof}

\medskip


\section{The Inverse Function Theorem}

The Inverse Function theorem states that a continuously differentiable mapping $f$ is invertible in a neighborhood of any point $\mathbf{x}$ at which the linear transformation $Df(\mathbf{x})$ is invertible.

\medskip

\begin{lemma}\label{lemma_71}
Let $\operatorname{GL}(\mathbb{R}^n)$ be the set of all invertible linear operators on $\mathbb{R}^n$. If $A \in \operatorname{GL}(\mathbb{R}^n)$ and $B \in L(\mathbb{R}^n)$ and 
\begin{align*}
    \left\|B - A\right\| \cdot \left\|A^{-1}\right\| < 1,
\end{align*}
then $B \in \operatorname{GL}(\mathbb{R}^n)$.
\end{lemma}
\begin{proof}
Let $\left\|A^{-1}\right\| = 1/\alpha$ and $\left\|B - A\right\| = \beta$, then $\beta < \alpha$. For every $\mathbf{x} \in \mathbb{R}^n$, we have
\begin{align*}
    \alpha \left|\mathbf{x}\right| & = \alpha \left|A^{-1} A \mathbf{x}\right| \leq \alpha \left\|A^{-1}\right\| \cdot \left|A\mathbf{x}\right| = \left|A\mathbf{x}\right| \\
    & \leq \left|(A - B)\mathbf{x}\right| + \left|B\mathbf{x}\right| \leq \beta \left|\mathbf{x}\right| + \left|B\mathbf{x}\right|,
\end{align*}
and hence
\begin{align}\label{lemma_71_equ1}
    (\alpha - \beta)\left|\mathbf{x} \right| \leq \left|B\mathbf{x}\right|, \quad \mathbf{x} \in \mathbb{R}^n.
\end{align}
Since $\alpha - \beta > 0$, we have $B\mathbf{x} \neq 0$ if $\mathbf{x} \neq 0$, hence $B$ is one-to-one. Thus $B \in \operatorname{GL}(\mathbb{R}^n)$. This holds for all $B$ with $\left\|B - A\right\| < \alpha$, and hence $\operatorname{GL}(\mathbb{R}^n)$ is open.
\end{proof}

\medskip

\begin{lemma}\label{lemma_72}
The set $\operatorname{GL}(\mathbb{R}^n)$ is an open subset of $L(\mathbb{R}^n)$, and the mapping $A \mapsto A^{-1}$ for $A \in \operatorname{GL}(\mathbb{R}^n)$ is continuous on $\operatorname{GL}(\mathbb{R}^n)$. (This mapping is also obviously a one-to-one mapping of $\operatorname{GL}(\mathbb{R}^n)$ onto $\operatorname{GL}(\mathbb{R}^n)$, which is its own inverse.)
\end{lemma}
\begin{proof}
Replacing $\mathbf{x}$ by $B^{-1} \mathbf{y}$ in \eqref{lemma_71_equ1} yields
\begin{align*}
    (\alpha - \beta)\left|B^{-1} \mathbf{y}\right| \leq \left|B B^{-1} \mathbf{y}\right| = \left|\mathbf{y}\right|, \quad \mathbf{y} \in \mathbb{R}^n,
\end{align*}
which implies
\begin{align*}
    \left\|B^{-1}\right\| \leq \frac{1}{\alpha - \beta}.
\end{align*} 
Then the identity $B^{-1} - A^{-1} = B^{-1} (A - B) A^{-1}$ implies that
\begin{align*}
    \left\|B^{-1} - A^{-1}\right\| \leq \left\|B^{-1}\right\| \left\|A - B\right\| \left\|A^{-1}\right\| \leq \frac{\beta}{\alpha(\alpha - \beta)} = \frac{1}{\alpha^2/\beta - \alpha},
\end{align*}
and the right hand side tends to $0$ as $B \to A$, which proves the continuity of the mapping $A \mapsto A^{-1}$.
\end{proof}
    
\medskip

\begin{definition}
Let $f: \Omega \to \mathbb{R}^n$, where $\Omega \subset \mathbb{R}^n$ is open. Suppose $f$ is differentiable at $\mathbf{x} \in  \Omega$, we define the Jacobian determinant $Jf(\mathbf{x})$ at $\mathbf{x}$ by
\begin{align*}
    Jf(\mathbf{x}) = \det Df(\mathbf{x}).
\end{align*}
\end{definition}

\medskip

\begin{theorem}[Inverse Function Theorem]\label{th_711}
Let $f: \Omega \to \mathbb{R}^n$, where $\Omega \subset \mathbb{R}^n$ is open and $f \in C^1(\Omega)$. Suppose $Df(\mathbf{x}_0)$ is invertible for some $\mathbf{x}_0 \in \Omega$ and $\mathbf{y}_0 = f(\mathbf{x}_0)$. Then
\begin{enumerate}[label=(\alph*)]
    \item there exist open neighborhood $U, V \in \mathbb{R}^n$ such that $\mathbf{x}_0 \in U$, $\mathbf{y}_0 \in V$ such that $f$ is one-to-one on $U$ and $f(U) = V$; \label{th_711_a}
    
    \item if $f^{-1}: V \to U$ is the inverse of $f$ (which exists, by \ref{th_711_a}), defined on $V$ by
    \begin{align*}
        f^{-1}(f(\mathbf{x})) = x, \quad \mathbf{x} \in U,
    \end{align*}
    then $f^{-1} \in C^1(V)$. Also, we have
    \begin{align*}
        Df^{-1}(\mathbf{y}) = \frac{1}{Df(\mathbf{x})},
    \end{align*}
    for $\mathbf{y} \in V$ and $\mathbf{x} = f^{-1}(\mathbf{y}) \in U$. \label{th_711_b}
\end{enumerate}
\end{theorem}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item Let $Df(\mathbf{x}_0) = A$, and choose $\lambda$ such that $2 \lambda \left\|A^{-1}\right\| = 1$. Since $Df$ is differentiable at $\mathbf{x}_0$, there is an open ball $U \subset \Omega$ of $\mathbf{x}_0$ such that 
    \begin{align}\label{th_711_equ1}
        \left\|Df(\mathbf{x}) - A\right\| < \lambda, \quad \mathbf{x} \in U.
    \end{align}
    
    For each $\mathbf{y} \in \mathbb{R}^n$ we define a function $\varphi$ by
    \begin{align*}
        \varphi^{\mathbf{y}}(\mathbf{x}) = \mathbf{x} + A^{-1} \left(\mathbf{y} - f(\mathbf{x})\right), \quad \mathbf{x} \in \Omega.
    \end{align*}
    
    {\em Note that $f(\mathbf{x}) = \mathbf{y}$ if and only if $\mathbf{x}$ is a fixed point of $\varphi^{\mathbf{y}}(\mathbf{x})$.} 
    
    Since $D\varphi^{\mathbf{y}}(\mathbf{x}) = I - A^{-1} Df(\mathbf{x}) = A^{-1}\left(A - Df(\mathbf{x})\right)$, if $\mathbf{x} \in U$, by \ref{th_711_equ1}, we have
    \begin{align*}
        \left\|D\varphi^{\mathbf{y}}(\mathbf{x})\right\| \leq \left\|A^{-1}\right\| \left\|Df(\mathbf{x}) - A\right\| < \lambda \left\|A^{-1}\right\| < \frac{1}{2}.
    \end{align*}
    Hence by Theorem \ref{th_77} and $U$ is convex, we have
    \begin{align}\label{th_711_equ2}
        \left|\varphi^{\mathbf{y}}(\mathbf{x}_1) - \varphi^{\mathbf{y}}(\mathbf{x}_2)\right| \leq \frac{1}{2} \left|\mathbf{x}_1 - \mathbf{x}_2\right|,
    \end{align}
    for $\mathbf{x}_1, \mathbf{x}_2 \in U$. It follows that $\varphi^{\mathbf{y}}$ has at most one fixed point in $U$, and hence $f(\mathbf{x}) = \mathbf{y}$ for at most one $\mathbf{x} \in U$. Thus $f$ is one-to-one in $U$.
    
    \medskip
    
    Next, let $V = f(U)$, and let $\mathbf{y}_0 \in V$. Then $\mathbf{y}_0 = f(\mathbf{x}_0)$ for some $\mathbf{x}_0 \in U$. Let $B(\mathbf{x}_0,r)$ be an open ball such that $\overline{B} \subset U$. We will show that $\mathbf{y} \in V$ whenever $\left|\mathbf{y} - \mathbf{y}_0\right| < \lambda r$, which proves that $V$ is open.
    
    Fix $\mathbf{y}$ such that $\left|\mathbf{y} - \mathbf{y}_0\right| < \lambda r$. Then we have
    \begin{align*}
        \left|\varphi^{\mathbf{y}}(\mathbf{x}_0) - \mathbf{x}_0\right| = \left|A^{-1}\left(\mathbf{y} - f(\mathbf{x}_0)\right)\right| = \left|A^{-1}\left(\mathbf{y} - \mathbf{y}_0\right)\right| < \lambda r \left\|A^{-1}\right\| = \frac{r}{2}.
    \end{align*}
    If $\mathbf{x} \in \overline{B}$, then by \eqref{th_711_equ2}, we have
    \begin{align*}
        \left|\varphi^{\mathbf{y}}(\mathbf{x}) - \mathbf{x}_0\right| \leq \left|\varphi^{\mathbf{y}}(\mathbf{x}) - \varphi^{\mathbf{y}}(\mathbf{x}_0)\right| + \left|\varphi^{\mathbf{y}}(\mathbf{x}_0) - \mathbf{x}_0\right| < \frac{1}{2} \left|\mathbf{x} - \mathbf{x}_0\right| + \frac{r}{2} = r,
    \end{align*}
    hence $\varphi^{\mathbf{y}}(\mathbf{x}) \in \overline{B}$. Thus $\varphi^{\mathbf{y}}$ is a contraction of $\overline{B}$ into $\overline{B}$, and since $\overline{B}$ is complete, by Theorem \ref{th_710}, $\varphi^{\mathbf{y}}$ has a fixed point $\mathbf{x} \in \overline{B}$. For this $\mathbf{x}$, $f(\mathbf{x}) = \mathbf{y}$. Thus, $\mathbf{y} \in \overline{B} \subset f(U) = V$. 
    
    \item Let $\mathbf{y} \in V$ and $\mathbf{y} + \mathbf{k} \in V$. Then there exist $\mathbf{x}, \mathbf{x} + \mathbf{h} \in U$ such that $\mathbf{y} = f(\mathbf{x})$ and $\mathbf{y} + \mathbf{k} = f(\mathbf{x} + \mathbf{h})$. Then we have
    \begin{align*}
        \varphi^{\mathbf{y}}(\mathbf{x} + \mathbf{h}) - \varphi^{\mathbf{y}}(\mathbf{x}) = \mathbf{h} + A^{-1} \left(f(\mathbf{x}) - f(\mathbf{x} + \mathbf{h})\right) = \mathbf{h} + A^{-1} \mathbf{k}.
    \end{align*}
    By \eqref{th_711_equ2}, we have $\left|\mathbf{h} + A^{-1} \mathbf{k}\right| \leq 1/2 \left|\mathbf{h}\right|$, and hence $\left|A^{-1} \mathbf{k}\right| \geq 1/2 \left|\mathbf{h}\right|$. Hence, 
    \begin{align}\label{th_711_equ3}
        \left|\mathbf{h}\right| \leq 2 \left\|A^{-1}\right\| \left|\mathbf{k}\right| = \lambda^{-1} \left|\mathbf{k}\right|.
    \end{align}
    
    By Lemma \ref{lemma_71} and \eqref{th_711_equ1}, we have
    \begin{align*}
        \left\|Df(\mathbf{x}) - A\right\| \cdot \left\|A^{-1}\right\| < \lambda \frac{1}{2\lambda} < 1,
    \end{align*}
    and hence $Df(\mathbf{x})$ has an inverse, denoted by $T$. Since 
    \begin{align*}
        f^{-1}(\mathbf{y} + \mathbf{k}) - f^{-1}(\mathbf{y}) - T \mathbf{k} = \mathbf{h} - T \mathbf{k} = -T \left(f(\mathbf{x} + \mathbf{h}) - f(\mathbf{x}) - Df(\mathbf{x}) \mathbf{h}\right),
    \end{align*}
    \eqref{th_711_equ3} implies that
    \begin{align*}
        \frac{\left|f^{-1}(\mathbf{y} + \mathbf{k}) - f^{-1}(\mathbf{y}) - T \mathbf{k}\right|}{\left|\mathbf{k}\right|} \leq \frac{\left\|T\right\|}{\lambda} \cdot \frac{\left|f(\mathbf{x} + \mathbf{h}) - f(\mathbf{x}) - Df(\mathbf{x}) \mathbf{h}\right|}{\left|\mathbf{h}\right|}.
    \end{align*}
    As $\mathbf{k} \to 0$, \eqref{th_711_equ3} implies that $\mathbf{kh} \to 0$ and hence the right hand side tends to $0$, so do the left hand side. Hence we proved that $Df^{-1}(\mathbf{y}) = T$, thus
    \begin{align*}
        Df^{-1}(\mathbf{y}) = \frac{1}{Df(\mathbf{x})}, \quad \mathbf{y} \in V,
    \end{align*}
    where $\mathbf{y} = f(\mathbf{x})$.
    
    Note that $f^{-1}$ is a continuous mapping from $V$ into $U$ (since $f^{-1}$ is differentibale), and $Df$ is a continuous mapping of $U$ into the set $\operatorname{GL}(\mathbb{R}^n)$, and that $Df \mapsto \left(Df\right)^{-1}$ is a continuous mapping of $\operatorname{GL}(\mathbb{R}^n)$ into $\operatorname{GL}(\mathbb{R}^n)$ by Lemma \ref{lemma_72}, thus $f^{-1} \in C^1(V)$.
\end{enumerate}
\end{proof}

\medskip

The following result is an immediate consequence of part \ref{th_711_a} of the Inverse Function theorem.

\medskip

\begin{theorem}
Let $f: \Omega \to \mathbb{R}^n$, where $\Omega \subset \mathbb{R}^n$ is open and $f \in C^1(\Omega)$. If $Df(\mathbf{x})$ is invertible for every $\mathbf{x} \in \Omega$, then $f(U)$ is an open subset of $\mathbb{R}^n$ for every open set $U \subset \Omega$.
\end{theorem}

\medskip

\begin{remark}
If we assume $f \in C^k(\Omega), k \geq 1$ in Theorem \ref{th_711}, then $f^{-1} \in C^k(\Omega)$ as well. Here, $C^k(\Omega)$ is the set of all functions defined on $\Omega$ whose all partial derivatives of orders less than or equal to $k$ are continuous. (We will talk about the derivatives of higher order later). 
\end{remark}
\begin{proof}
We claim that the mapping $T: A \mapsto A^{-1}$ is of class $C^\infty$. Indeed, if $A = \left(a_{ij}\right)^n_{i,j}$, then the entries of the matrix $A^{-1}$ are of the form
\begin{align}\label{remark_76_equ1}
    \frac{P(a_{11}, \cdots, a_{nn})}{Q(a_{11}, \cdots, a_{nn})},
\end{align}
where $P, Q$ are polynomials and $Q \neq 0$\footnote{Indeed, from linear algebra, we have $A^{-1} = \operatorname{adj}(A)/\det A$. Here, $\operatorname{adj}(A)$ is the classical adjoint matrix of $A$ and $\operatorname{adj}(A)_{ij} = (-1)^{i+j} A_{ji}$, where $(-1)^{i+j} A_{ji} = C_{ji}$ is the cofactor of $A$, defined by $(-1)^{i+j}$ times the determinant of $(n-1) \times (n-1)$ matrix by deleting the $j$th row and $i$th column of $A$.}. In fact, $Q(a_{11}, \cdots, a_{nn}) = \det A$, and clearly \eqref{remark_76_equ1} is a $C^\infty$ function of $a_{11}, \cdots, a_{nn}$. 

We will prove that $Df^{-1} \in C^k$ using the so called {\em bootstrap} argument. Since $f^{-1}$ is continuous, then $Df \in C^{k-1}$, and since the mapping $T: A \to A^{-1}$ is of class $C^\infty$, we have that $Df^{-1} = T \circ Df$ is continuous. Thus $f^{-1} \in C^1$.  

Now we have $f^{-1} \in C^1$, $Df \in C^{k-1}$, and $T \in C^\infty$, then $Df^{-1} \in C^1$ and hence $f^{-1} \in C^2$. Continue this process and we finally have $f^{-1} \in C^{k-1}$, $Df \in C^{k-1}$, and $T \in C^\infty$, then $Df^{-1} \in C^{k-1}$ and hence $f^{-1} \in C^k$.
\end{proof}

\medskip

\begin{definition}
If $U, V \subset \mathbb{R}^n$ are open, $f: U \to V$ is of class $C^k, k \geq 1$, one-to-one and onto, and $f^{-1}: V \to U$ is of class $C^k$, then we say $f$ is a $C^k$ diffeomorphism of $U$ and $V$.
\end{definition}

\begin{remark}
By the Inverse Function theorem, if $f: \Omega \to \mathbb{R}^n, \Omega \subset \mathbb{R}^n$ is of class $C^k$ and $Jf(\mathbf{x}) \neq 0$, then $f$ is a $C^k$ diffeomorphism in a neighborhood of $\mathbf{x}$.
\end{remark}

\medskip

\begin{proposition}\label{prop_75}
Let $U,V \subset \mathbb{R}^n$ be open. A one-to-one and onto mapping $f: U \to V$ of class $C^k, k \geq 1$ is a $C^k$ diffeomorphism if and only if $Jf(\mathbf{x}) \neq 0$ for every $\mathbf{x} \in U$.
\end{proposition}
\begin{proof}
Suppose $Jf(\mathbf{x}) \neq 0$ for every $\mathbf{x} \in U$, then the result is obvious. Conversely, we have $f^{-1} \circ f(\mathbf{x}) = \mathbf{x}$, and hence $I = D(f^{-1} \circ f)(\mathbf{x}) = Df^{-1}(f(\mathbf{x})) \cdot Df(\mathbf{x})$. Clearly, $Df(\mathbf{x})$ is invertible and hence $Jf(\mathbf{x}) \neq 0$.
\end{proof}

\medskip




\section{Implicit Function Theorem}

If $f$ is continuously differentiable in the plane, then the equation $f(x,y) = 0$ can be solved for $y$ in terms of $x$ in a neighborhood of any point $(a,b)$ at which $f(a,b) = 0$ and $\partial f/\partial y \neq 0$. Similarly, one can solve $x$ in terms of $y$ near $(a,b)$ is $\partial f/\partial x \neq 0$ at $(a,b)$.

\medskip

\begin{definition}
If $\mathbf{x} = (x_1,\cdots,x_n) \in \mathbb{R}^n$ and $\mathbf{y} = (y_1,\cdots,y_m) \in \mathbb{R}^m$, let us write $(\mathbf{x},\mathbf{y})$ for the point (or vector)
\begin{align*}
    (x_1,\cdots,x_n, y_1,\cdots,y_m) \in \mathbb{R}^{n+m}.
\end{align*}
Every $A \in L(\mathbb{R}^{n+m}, \mathbb{R}^n)$ can be split into two linear transformations $A_x$ and $A_y$, defined by
\begin{align*}
    A_x \mathbf{h} = A(\mathbf{h},0), \quad A_y \mathbf{h} = A(0,\mathbf{k}),
\end{align*}
for $\mathbf{h} \in \mathbb{R}^n, \mathbf{k} \in \mathbb{R}^m$. Then $A_x \in L(\mathbb{R}^n)$, $A_y \in L(\mathbb{R}^m,\mathbb{R}^n)$, and
\begin{align}\label{def_714_equ1}
    A(\mathbf{h},\mathbf{k}) = A_x \mathbf{h} + A_y \mathbf{k}.
\end{align}
\end{definition}

\medskip

Now the linear version of the Implicit Function theorem is almost obvious.

\medskip

\begin{theorem}\label{th_713}
If $A \in L(\mathbb{R}^{n+m}, \mathbb{R}^n)$ and if $A_{\mathbf{x}}$ is invertible, then for every $\mathbf{k} \in \mathbb{R}^m$, there is a unique $\mathbf{h} \in \mathbb{R}^n$ such that $A(\mathbf{h},\mathbf{k}) = 0$. This $\mathbf{h}$ can be computed from $\mathbf{k}$ by the formula
\begin{align}\label{th_713_equ1}
    \mathbf{h} = - (A_x)^{-1} A_y \mathbf{k}.
\end{align}
\end{theorem}
\begin{proof}
By \eqref{def_714_equ1}, $A(\mathbf{h},\mathbf{k}) = 0$ if and only if $A_x \mathbf{h} + A_y \mathbf{k} = 0$, which is the same as \eqref{th_713_equ1} if $A_x$ is invertible.
\end{proof}

\medskip

\begin{theorem}[Implicit Function Theorem]\label{th_714}
Let $f$ be a $C^1$ mapping from an open set $\Omega \subset \mathbb{R}^{n+m}$ into $\mathbb{R}^n$ such that $f(\mathbf{x}_0,\mathbf{y}_0) = 0$ for some $(\mathbf{x}_0,\mathbf{y}_0) \in \Omega$. Let $A = Df(\mathbf{x}_0,\mathbf{y}_0)$ and assume that $A_x$ is invertible. Then there exist open sets $U \subset \mathbb{R}^{n+m}$ and $V \subset \mathbb{R}^m$, with $(\mathbf{x}_0,\mathbf{y}_0) \in U$ and $\mathbf{y}_0 \in V$, satisfying the following property:

For every $\mathbf{y} \in V$, there is a unique $\mathbf{x}$ such that $(\mathbf{x},\mathbf{y}) \in U$ and $f(\mathbf{x},\mathbf{y}) = 0$. If this $\mathbf{x}$ is defined to be $g(\mathbf{y})$, then $g$ is a $C^1$ mapping of $V$ into $\mathbb{R}^n$, $g(\mathbf{y}_0) = \mathbf{x}_0$,
\begin{align}\label{th_714_equ1}
    f(g(\mathbf{y}),\mathbf{y}) = 0, \quad \mathbf{y} \in V,
\end{align}
and
\begin{align}\label{th_714_equ2}
    Dg(\mathbf{y}_0) = -(A_x)^{-1} A_y.
\end{align}
\end{theorem}

\begin{remark}
The equation $f(\mathbf{x},\mathbf{y}) = 0$ can be written as a system of $n$ equations in $n + m$ variables:
\begin{align*}
    f_1(x_1, \cdots, x_n, & y_1, \cdots, y_m) = 0, \\
    & \cdots \\
    f_n(x_1, \cdots, x_n, & y_1, \cdots, y_m) = 0.
\end{align*}
And the assumption that $A_x$ is invertible means that:
\begin{align*}
    \det \begin{pmatrix}
        \frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} \\
        \vdots & \ddots & \vdots \\
        \frac{\partial f_n}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n}
    \end{pmatrix}(\mathbf{x}_0,\mathbf{y}_0) \neq 0.
\end{align*}
If \eqref{th_714_equ1} holds for $\mathbf{x} = \mathbf{x}_0$ and $\mathbf{y} = \mathbf{y}_0$, then \eqref{th_714_equ1} can be solved for $x_1, \cdots, x_n$ in terms of $y_1, \cdots, y_m$ for every $\mathbf{y}$ near $\mathbf{y}_0$, and that these solutions are continuously differentiable functions of $\mathbf{y}$.
\end{remark}

\begin{proof}
Define $F(\mathbf{x},\mathbf{y}) = \left(f(\mathbf{x},\mathbf{y}), \mathbf{y}\right)$ for $(\mathbf{x},\mathbf{y}) \in \Omega$. Then $F$ is a $C^1$ mapping of $\Omega$ into $\mathbb{R}^{n+m}$ and we have
\begin{align*}
    DF(\mathbf{x}_0,\mathbf{y}_0) = 
    \begin{pmatrix}
        \begin{matrix}
            \frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} \\
            \vdots & \ddots & \vdots \\
            \frac{\partial f_n}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n}
        \end{matrix} & 0 \\
        0 & \begin{matrix}
            1 & \cdots & 0 \\
            \vdots & \ddots & \vdots \\
            0 & \cdots & 1
        \end{matrix}
    \end{pmatrix}(\mathbf{x}_0,\mathbf{y}_0).
\end{align*}
Hence, $JF(\mathbf{x}_0,\mathbf{y}_0) \neq 0$. By the Inverse Function theorem \ref{th_711} and Proposition \ref{prop_75}, $F$ is a diffeomorphism on a neighborhood of $(\mathbf{x}_0,\mathbf{y}_0)$, and hence there is a neighborhood of $(\mathbf{x}_0,\mathbf{y}_0)$ of the form $\widetilde{U} \times \widetilde{V}$, where $\widetilde{U} \subset \mathbb{R}^n$ is a neighborhood of $\mathbf{x}_0$ and $\widetilde{V} \subset \mathbb{R}^m$ is a neighborhood of $\mathbf{y}_0$ such that $F|_{\widetilde{U} \times \widetilde{V}}: \widetilde{U} \times \widetilde{V} \to F(\widetilde{U} \times \widetilde{V})$ is a $C^1$ diffeomorphism. 

Let $\widetilde{W} = F(\widetilde{U} \times \widetilde{V})$ and $\widetilde{W}$ is open. Note that $F(\mathbf{x}_0,\mathbf{y}_0) = (0,\mathbf{y}_0) \in \widetilde{W}$. Since $\widetilde{W}$ is open, there is a neighborhood $V \subset \widetilde{V}$ of $\mathbf{y}_0$ such that 
\begin{align*}
    \{0\} \times V \subset \widetilde{W}.
\end{align*}
Let $U = \widetilde{U} \times V$. Then the mapping
\begin{align*}
    F|_{U}: U \to F(U) \coloneqq W
\end{align*}
is also a diffeomorphism and $\{0\} \times V \subset W$. The inverse diffeomorphism is of the form
\begin{align*}
    \left(F|_{U}\right)^{-1}(\mathbf{x}, \mathbf{y}) = (h(\mathbf{x}, \mathbf{y}), \mathbf{y}).
\end{align*}

Now let $S \coloneqq \{(\mathbf{x},\mathbf{y}) \,:\, f(\mathbf{x}, \mathbf{y}) = 0\}$. If $(\mathbf{x}, \mathbf{y}) \in S \cap U$, then $(F|_{U})(\mathbf{x}, \mathbf{y}) = (0,\mathbf{y})$, which implies $(\mathbf{x}, \mathbf{y}) = \left(F|_{U}\right)^{-1}(0,\mathbf{y})$. Hence, $(\mathbf{x}, \mathbf{y}) = (h(0, \mathbf{y}), \mathbf{y})$ and then we have $\mathbf{x} = h(0, \mathbf{y})$. Therefore, $S \cap U$ is a graph of a $C^1$ mapping $g: V \to \mathbb{R}^n$, where $g$ is defined as $g(\mathbf{y}) = h(0, \mathbf{y})$. This proves \eqref{th_714_equ1}.

\medskip

Now let $G(\mathbf{y}) = (g(\mathbf{y}), \mathbf{y})$. Then we have $(f \circ G)(\mathbf{y}) = f(g(\mathbf{y}), \mathbf{y}) = 0$, and hence
\begin{align*}
    D(f \circ G)(\mathbf{y}) = Df(G(\mathbf{y})) \cdot DG(\mathbf{y}) = 0.
\end{align*}
Since
\begin{align*}
    Df(g(\mathbf{y})) = \begin{pmatrix}
        \frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} &  \frac{\partial f_1}{\partial y_1} & \cdots & \frac{\partial f_1}{\partial y_m}\\
        \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
        \frac{\partial f_n}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} & \frac{\partial f_n}{\partial y_1} & \cdots & \frac{\partial f_n}{\partial y_m}
    \end{pmatrix},
\end{align*}
and
\begin{align*}
    DG(\mathbf{y}) = \begin{pmatrix}
        \frac{\partial g_1}{\partial y_1} & \cdots & \frac{\partial g_1}{\partial y_m} \\
        \vdots & \ddots & \vdots \\
        \frac{\partial g_n}{\partial y_1} & \cdots & \frac{\partial g_n}{\partial y_m} \\
        1 & \cdots & 0 \\
        \vdots & \ddots & \vdots \\
        0 & \cdots & 1
    \end{pmatrix},
\end{align*}
we have
\begin{align*}
    \begin{pmatrix}
        \frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} \\
        \vdots & \ddots & \vdots \\
        \frac{\partial f_n}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n}
    \end{pmatrix} 
    \begin{pmatrix}
        \frac{\partial g_1}{\partial y_1} & \cdots & \frac{\partial g_1}{\partial y_m} \\
        \vdots & \ddots & \vdots \\
        \frac{\partial g_n}{\partial y_1} & \cdots & \frac{\partial g_n}{\partial y_m}
    \end{pmatrix} + 
    \begin{pmatrix}
        \frac{\partial f_1}{\partial y_1} & \cdots & \frac{\partial f_1}{\partial y_m} \\
        \vdots & \ddots & \vdots \\
        \frac{\partial f_n}{\partial y_1} & \cdots & \frac{\partial f_n}{\partial y_m}
    \end{pmatrix} = 0,
\end{align*}
and hence
\begin{align*}
    Dg(\mathbf{y}) = \begin{pmatrix}
        \frac{\partial g_1}{\partial y_1} & \cdots & \frac{\partial g_1}{\partial y_m} \\
        \vdots & \ddots & \vdots \\
        \frac{\partial g_n}{\partial y_1} & \cdots & \frac{\partial g_n}{\partial y_m}
    \end{pmatrix} = - \begin{pmatrix}
        \frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} \\
        \vdots & \ddots & \vdots \\
        \frac{\partial f_n}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n}
    \end{pmatrix}^{-1} \begin{pmatrix}
        \frac{\partial f_1}{\partial y_1} & \cdots & \frac{\partial f_1}{\partial y_m} \\
        \vdots & \ddots & \vdots \\
        \frac{\partial f_n}{\partial y_1} & \cdots & \frac{\partial f_n}{\partial y_m}
    \end{pmatrix}
\end{align*}
holds in the neighborhood of $\mathbf{y}_0$, which is a generalization of \eqref{th_714_equ2}.
\end{proof}




\medskip


\section{Derivatives of Higher Order}

\begin{definition}
Suppose $f$ is a real function defined in an open set $\Omega \subset \mathbb{R}^n$ with partial derivatives $\frac{\partial f}{\partial x_1}, \cdots, \frac{\partial f}{\partial x_n}$. If the function $\frac{\partial f}{\partial x_j}$ are differentiable, then  the second-order partial derivatives of $f$ are defined by
\begin{align*}
    \frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{\partial}{\partial x_i} \left(\frac{\partial f}{\partial x_j}\right).
\end{align*}
If all these functions are continuous in $\Omega$, we say $f$ is of class $C^2$ in $\Omega$ or $f \in C^2(\Omega)$. A mapping $f$ of $\Omega$ into $\mathbb{R}^m$ is said to be of class $C^2$ if each component of $f$ is of class $C^2$.
\end{definition}

\medskip

We will show that
\begin{align*}
    \frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{\partial^2 f}{\partial x_j \partial x_x}
\end{align*}
whenever these derivatives are continuous.

\medskip

\begin{theorem}\label{th_715}
Suppose $f$ is defined in an open set $\Omega \subset \mathbb{R}^2$, and $\frac{\partial f}{\partial x_1}$ and $\frac{\partial^2 f}{\partial x_2 \partial x_1}$ exist at every point of $\Omega$. Suppose $Q \subset \Omega$ is a closed rectangle with sides parallel to the coordinate axes, having $(x,y)$ and $(x+h,y+k)$ as opposite vertices ($h,k \neq 0$). Let
\begin{align*}
    \Delta (f,Q) = f(x+h,y+k) - f(x+h,y) - f(x,y+k) + f(x,y).
\end{align*}
Then there is a point $(a,b)$ in the interior of $Q$ such that
\begin{align}\label{th_715_equ1}
    \Delta (f,Q) = hk \frac{\partial^2 f}{\partial x_2 \partial x_1}(a,b).
\end{align}
\end{theorem}
\begin{proof}
Let $u(t) = f(t,y+k) - f(t,y)$. By the Mean Value theorem \ref{th_48}, there is $a \in (x,x+h)$ and $b \in (y,y+k)$ such that
\begin{align*}
    \Delta (f,Q) = u(x+h) - u(x) = h u'(a)  = h \left(\frac{\partial f}{\partial x_1}(a,y+k) - \frac{\partial f}{\partial x_1}(a,y)\right) = hk \frac{\partial^2 f}{\partial x_2 \partial x_1}(a,b).
\end{align*}
\end{proof}

\medskip

\begin{theorem}
Suppose $f$ is defined in an open set $\Omega \subset \mathbb{R}^2$ and suppose that $\frac{\partial f}{\partial x_1}$, $\frac{\partial f}{\partial x_2}$ and $\frac{\partial^2 f}{\partial x_2 \partial x_1}$  exist at every point of $\Omega$, and $\frac{\partial^2 f}{\partial x_2 \partial x_1}$ is continuous at some point $(x,y) \in \Omega$. Then $\frac{\partial^2 f}{\partial x_1 \partial x_2}$ exists at $(x,y)$ and 
\begin{align*}
    \frac{\partial^2 f}{\partial x_1 \partial x_2}(x,y) = \frac{\partial^2 f}{\partial x_2 \partial x_1}(x,y).
\end{align*}
\end{theorem}
\begin{proof}
Let $A = \frac{\partial^2 f}{\partial x_2 \partial x_1}(x,y)$. Let $\varepsilon > 0$, and $Q$ be a rectangle as in Theorem \ref{th_715}, if $h,k$ are small enough, we have
\begin{align*}
    \left|A - \frac{\partial^2 f}{\partial x_2 \partial x_1}(a,b)\right| < \varepsilon,
\end{align*}
for $(a,b) \in Q$. Then by \eqref{th_715_equ1}, we have
\begin{align*}
    \left|\frac{\Delta (f,Q)}{hk} - A\right| < \varepsilon.
\end{align*}
Fix $h$ and letting $k \to 0$, since $\frac{\partial f}{\partial x_2}$ exists in $\Omega$, we have
\begin{align*}
    \left|\frac{1}{h}\left(\frac{\partial f}{\partial x_2}(x+h,y) - \frac{\partial f}{\partial x_2}(x,y)\right) - A\right| < \varepsilon,
\end{align*}
and since this inequality holds for all sufficiently small $h \neq 0$, it follows that $\frac{\partial^2 f}{\partial x_1 \partial x_2}(x,y) = A$.
\end{proof}

\medskip

\begin{theorem}
Suppose $f$ is defined in an open set $\Omega \subset \mathbb{R}^2$. If $f \in C^2(\Omega)$, then
\begin{align*}
    \frac{\partial^2 f}{\partial x_1 \partial x_2}(x,y) = \frac{\partial^2 f}{\partial x_2 \partial x_1}(x,y),
\end{align*}
for all $(x,y) \in \Omega$.
\end{theorem}

\medskip

By induction, we obtain the symmetry of the higher order partial derivatives. For example, if $f: \mathbb{R}^5 \to \mathbb{R}$ has continuous fourth order partial derivatives, then
\begin{align*}
    \frac{\partial^4 f}{\partial x_1^2 \partial x_3 \partial x_5} = \frac{\partial^4 f}{\partial x_1 \partial x_5 \partial x_3 \partial x_1} = \frac{\partial^4 f}{\partial x_3 \partial x_1^2 \partial x_5} = \cdots
\end{align*}

\medskip

\begin{definition}
Let $\Omega \subset \mathbb{R}^n$ be an open set. We say that $f \in C^k(\Omega)$, where $k \geq 1$ is an integer, if all derivatives of order less than or equal to $k$ are continuous in $\Omega$.
\end{definition}

\medskip

To denote higher order partial derivatives, we introduce a convenient notation.

\medskip

\begin{definition}
For an integer $n$, A multiindex is any sequence of nonnegative integers $\alpha = (\alpha_1,\cdots,\alpha_n)$. For a multiindex $\alpha$, $x \in \mathbb{R}^n$ and a function $f: \Omega \to \mathbb{R}$, where $\Omega \subset \mathbb{R}^n$ is open, we define
\begin{align*}
    \left|\alpha\right| & = \alpha_1 + \alpha_2 + \cdots + \alpha_n, \\ \alpha! & = \alpha_1! \alpha_2! \cdots \alpha_n!, \\
    x^{\alpha} & = x_1^{\alpha_1} x_2^{\alpha_2} \cdots x_n^{\alpha_n},
\end{align*}
and
\begin{align*}
    D^{\alpha} f = \frac{\partial^{\left|\alpha\right|}f}{\partial x_1^{\alpha_1} \cdots \partial x_n^{\alpha_n}}.
\end{align*}
\end{definition}

\medskip

Recall the generalization of the binomial formula:
\begin{align*}
    (x_1 + \cdots + x_n)^k = \sum_{\alpha_1 + \cdots + \alpha_n = k} \frac{k!}{\alpha_1! \cdots \alpha_n!} x_1^{\alpha_1} \cdots x_n^{\alpha_n} = \sum_{\left|\alpha\right|=k} \frac{k!}{\alpha!} x^{\alpha},
\end{align*}
where in the last step we use the notation in the above definition.

\medskip

Now we can talk about the multi-variable version of the Taylor theorem. Suppose $f \in C^k(\Omega), \Omega \subset \mathbb{R}^n$. For $\mathbf{x} \in \Omega$ and $\mathbf{h} \in \mathbb{R}^n$ such that $\mathbf{x} + \mathbf{h} \in \Omega$, and for all $t \in [0,1]$, we have
\begin{align*}
    \dv{}{t}f(\mathbf{x} + t\mathbf{h}) & = \sum^n_{i=1} \frac{\partial f}{\partial x_i}(\mathbf{x} + t\mathbf{h}) h_i, \\
    \dv{^2}{t^2}f(\mathbf{x} + t\mathbf{h}) & = \sum^n_{i_1,i_2=1} \frac{\partial^2 f}{\partial x_{i_1} \partial x_{i_2}}(\mathbf{x} + t\mathbf{h}) h_{i_1} h_{i_2}, \\
    & \cdots \\
    \dv{^k}{t^k}f(\mathbf{x} + t\mathbf{h}) & = \sum^n_{i_1,\cdots,i_k=1} \frac{\partial^k f}{\partial x_{i_1} \cdots \partial x_{i_k}} (\mathbf{x} + t\mathbf{h}) h_{i_1} \cdots h_{i_2} \\
    & = \sum_{\left|\alpha\right|=k} \frac{k!}{\alpha!} D^{\alpha} f(\mathbf{x} + t\mathbf{h}) \mathbf{h}^{\alpha}.
\end{align*}

Denote $g(t) = f(\mathbf{x} + t\mathbf{h})$. From the Taylor's theorem \ref{th_628} for $g$, we have
\begin{align*}
    g(1) - g(0) = 
\end{align*}

















\newpage
\bibliographystyle{unsrt}
\bibliography{bibliography}

\end{document}